
📦 Project Structure of: E:\MyProjects\networksecurity

📁 .dvc/
    📄 .gitignore
    📁 cache/
        📁 files/
            📁 md5/
                📁 68/
                    📄 770e0efceb9cd3777502f505ecbec4
                📁 70/
                    📄 3506878baa6bf0360b2b42782e9677
                📁 83/
                    📄 de4e3a0dbe7d93403a8c15753377f7
                📁 86/
                    📄 4e5c8474fcd1b15bc434932036628d
                📁 87/
                    📄 f220430be73d16609bfd637b1644af
                📁 8d/
                    📄 e71e17a20f71bece7659a34bae7027
                📁 bf/
                    📄 fa4350e8aa16caaa3cbb92ec8e048c
                📁 e1/
                    📄 a8ae356f072436dd3acc1bcbafbd89
    📄 config
    📁 tmp/
        📄 btime
        📄 dag.md
        📁 exps/
            📁 cache/
                📁 3e/
                    📄 a84c7b28b0242174e45334d5d6a9d3026f546d
                📁 48/
                    📄 faf510268751ce441d9178a563f5e2c4577d88
                📁 4a/
                    📄 640ce33b51ffeb8c1aa85e520ce78dfe105123
                📁 61/
                    📄 a81d71c590a7a662176a82aa5f00a7532a3e16
                📁 8a/
                    📄 ed08439cb37bbee9fee4ad00d82ea30435606b
                📁 a8/
                    📄 2c1cb5bbc048ca32cb78061a4b8aaf3735233f
                📁 b0/
                    📄 63569211a95f11f23c4f61f6b9518bbf3b2cf3
                📁 c4/
                    📄 9033287ca54d77ad6ab9c7603c06825bbac4e6
                📁 c7/
                    📄 31a093cb23b7a2bef65922d39d9d20ac56e6f3
                📁 e3/
                    📄 5de325a2125360b5ae289c080fc748a0140cdc
                📁 f9/
                    📄 21cb7aa3662a13f16a67e07a71781cec9b1a91
            📁 celery/
                📁 broker/
                    📁 control/
                    📁 in/
                    📁 processed/
                📁 result/
        📄 lock
        📄 rwlock
        📄 rwlock.lock
📄 .dvcignore
📄 .env
📄 .gitignore
📄 Dockerfile
📄 LICENSE
📄 README.md
📄 app.py
📁 artifacts/
    📁 2025_04_23T19_43_09Z/
        📁 data_ingestion/
            📁 featurestore/
                📄 raw_data.csv
            📁 ingested/
                📄 ingested_data.csv
        📁 data_transformation/
            📁 preprocessor/
                📄 x_preprocessor.joblib
                📄 y_preprocessor.joblib
            📁 transformed/
                📁 test/
                    📄 x_test.npy
                    📄 y_test.npy
                📁 train/
                    📄 x_train.npy
                    📄 y_train.npy
                📁 val/
                    📄 x_val.npy
                    📄 y_val.npy
        📁 data_validation/
            📁 reports/
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.json
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
        📁 model_trainer/
            📄 model.joblib
            📄 training_report.yaml
    📁 2025_04_23T19_48_15Z/
        📁 data_ingestion/
            📁 featurestore/
                📄 raw_data.csv
            📁 ingested/
                📄 ingested_data.csv
        📁 data_transformation/
            📁 preprocessor/
                📄 x_preprocessor.joblib
                📄 y_preprocessor.joblib
            📁 transformed/
                📁 test/
                    📄 x_test.npy
                    📄 y_test.npy
                📁 train/
                    📄 x_train.npy
                    📄 y_train.npy
                📁 val/
                    📄 x_val.npy
                    📄 y_val.npy
        📁 data_validation/
            📁 reports/
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.json
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
        📁 model_trainer/
            📄 model.joblib
            📄 training_report.yaml
    📁 2025_04_23T19_54_45Z/
        📁 data_ingestion/
            📁 featurestore/
                📄 raw_data.csv
            📁 ingested/
                📄 ingested_data.csv
        📁 data_transformation/
            📁 preprocessor/
                📄 x_preprocessor.joblib
                📄 y_preprocessor.joblib
            📁 transformed/
                📁 test/
                    📄 x_test.npy
                    📄 y_test.npy
                📁 train/
                    📄 x_train.npy
                    📄 y_train.npy
                📁 val/
                    📄 x_val.npy
                    📄 y_val.npy
        📁 data_validation/
            📁 reports/
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.json
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
        📁 model_trainer/
            📄 model.joblib
            📄 training_report.yaml
📁 config/
    📄 config.yaml
    📄 params.yaml
    📄 schema.yaml
    📄 templates.yaml
📁 data/
    📁 raw/
        📄 raw_data.csv
        📄 raw_data.csv.dvc
    📁 transformed/
        📁 test/
            📄 x_test.npy
            📄 x_test.npy.dvc
            📄 y_test.npy
            📄 y_test.npy.dvc
        📁 train/
            📄 x_train.npy
            📄 x_train.npy.dvc
            📄 y_train.npy
            📄 y_train.npy.dvc
        📁 val/
            📄 x_val.npy
            📄 x_val.npy.dvc
            📄 y_val.npy
            📄 y_val.npy.dvc
    📁 validated/
        📄 validated_data.csv
        📄 validated_data.csv.dvc
📄 debug.py
📄 debug_pipeline.py
📁 logs/
    📁 2025_04_23T19_43_09Z/
        📄 2025_04_23T19_43_09Z.log
    📁 2025_04_23T19_48_15Z/
        📄 2025_04_23T19_48_15Z.log
    📁 2025_04_23T19_54_45Z/
        📄 2025_04_23T19_54_45Z.log
📄 main.py
📁 network_data/
    📁 input_csv/
        📄 phisingData.csv
📄 notes.txt
📄 print_structure.py
📄 project_code_dump.txt
📄 project_dump.py
📄 project_template.py
📄 prompt.txt
📄 requirements.txt
📁 research/
    📁 config/
        📄 schema.yaml
    📄 ingested_data.csv
    📁 logs/
        📁 2025_04_10T16_40_06/
            📄 2025_04_10T16_40_06.log
        📁 2025_04_10T17_30_35/
            📄 2025_04_10T17_30_35.log
        📁 2025_04_10T17_38_55/
            📄 2025_04_10T17_38_55.log
    📄 research.ipynb
📄 setup.py
📁 src/
    📁 networksecurity/
        📄 __init__.py
        📁 cloud/
            📄 __init__.py
        📁 components/
            📄 __init__.py
            📄 data_ingestion.py
            📄 data_transformation.py
            📄 data_validation.py
            📄 model_trainer.py
        📁 config/
            📄 __init__.py
            📄 configuration.py
        📁 constants/
            📄 __init__.py
            📄 constants.py
        📁 data_processors/
            📄 encoder_factory.py
            📄 imputer_factory.py
            📄 label_mapper.py
            📄 preprocessor_builder.py
            📄 scaler_factory.py
        📁 dbhandler/
            📄 __init__.py
            📄 base_handler.py
            📄 mongodb_handler.py
        📁 entity/
            📄 __init__.py
            📄 artifact_entity.py
            📄 config_entity.py
        📁 exception/
            📄 __init__.py
            📄 exception.py
        📁 inference/
            📄 estimator.py
        📁 logging/
            📄 __init__.py
            📄 logger.py
        📁 pipeline/
            📄 __init__.py
            📄 data_ingestion_pipeline.py
            📄 data_transformation_pipeline.py
            📄 data_validation_pipeline.py
            📄 model_trainer_pipeline.py
        📁 utils/
            📄 __init__.py
            📄 core.py
            📄 timestamp.py
📁 templates/
    📄 index.html

--- PYTHON CODE DUMP ---


================================================================================
# FILE: app.py
================================================================================



================================================================================
# FILE: debug.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.dbhandler.mongodb_handler import MongoDBHandler
from src.networksecurity.components.data_ingestion import DataIngestion
from dotenv import load_dotenv
load_dotenv()


configmanager = ConfigurationManager()

mongohandler_config = configmanager.get_mongo_handler_config()

mongohandler = MongoDBHandler(mongohandler_config)

with mongohandler:
    mongohandler.insert_csv_to_collection(mongohandler_config.input_data_path)

================================================================================
# FILE: debug_pipeline.py
================================================================================

# FILE: debug_pipeline.py

from dotenv import load_dotenv
from src.networksecurity.pipeline.data_ingestion_pipeline import DataIngestionPipeline
from src.networksecurity.pipeline.data_validation_pipeline import DataValidationPipeline
from src.networksecurity.pipeline.data_transformation_pipeline import DataTransformationPipeline
from src.networksecurity.pipeline.model_trainer_pipeline import ModelTrainerPipeline
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError

# Load environment variables (e.g. MONGO_URI, MLFLOW_TRACKING_URI, etc.)
load_dotenv()

if __name__ == "__main__":
    try:
        # ────────────────
        # Data Ingestion
        # ────────────────
        logger.info("========== Launching Data Ingestion Pipeline ==========")
        ingestion_pipeline = DataIngestionPipeline()
        ingestion_artifact = ingestion_pipeline.run()
        logger.info("========== Data Ingestion Pipeline Finished ==========")
        logger.info("======================================================")

        # ────────────────
        # Data Validation
        # ────────────────
        logger.info("========== Launching Data Validation Pipeline ==========")
        validation_pipeline = DataValidationPipeline(
            ingestion_artifact=ingestion_artifact
        )
        validation_artifact = validation_pipeline.run()
        logger.info("========== Data Validation Pipeline Finished ==========")
        logger.info("======================================================")

        # ────────────────
        # Data Transformation
        # ────────────────
        if validation_artifact.validation_status:
            logger.info("========== Launching Data Transformation Pipeline ==========")
            transformation_pipeline = DataTransformationPipeline(
                validation_artifact=validation_artifact
            )
            transformation_artifact = transformation_pipeline.run()
            logger.info("========== Data Transformation Pipeline Finished ==========")
            logger.info("======================================================")
        else:
            logger.warning("⚠️ Skipping Data Transformation: Validation failed.")
            raise RuntimeError("Cannot proceed to transformation without valid data.")

        # ────────────────
        # Model Training
        # ────────────────
        logger.info("========== Launching Model Trainer Pipeline ==========")
        trainer_pipeline = ModelTrainerPipeline(
            transformation_artifact=transformation_artifact
        )
        trainer_artifact = trainer_pipeline.run()
        logger.info("========== Model Trainer Pipeline Finished ==========")
        logger.info("======================================================")

    except NetworkSecurityError:
        logger.exception("❌ Pipeline failed due to a known exception.")
    except Exception:
        logger.exception("❌ Pipeline failed due to an unexpected exception.")

================================================================================
# FILE: main.py
================================================================================



================================================================================
# FILE: print_structure.py
================================================================================

import os


def print_directory_tree(start_path: str, indent: str = "", exclude_dirs=None) -> None:
    """
    Recursively prints the directory structure starting from `start_path`,
    excluding directories listed in `exclude_dirs`.

    Args:
        start_path (str): The root folder path to start from.
        indent (str): Used for indentation in recursive calls.
        exclude_dirs (set): Directory names to ignore.
    """
    if exclude_dirs is None:
        exclude_dirs = {'.venv', 'venv', '__pycache__', '.github', '.git'}

    try:
        items = sorted(os.listdir(start_path))
    except PermissionError:
        return  # Skip directories/files we can't access

    for item in items:
        item_path = os.path.join(start_path, item)

        if os.path.isdir(item_path):
            if item in exclude_dirs:
                continue
            print(f"{indent}📁 {item}/")
            print_directory_tree(item_path, indent + "    ", exclude_dirs)
        else:
            print(f"{indent}📄 {item}")


if __name__ == "__main__":
    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
    print(f"\n📦 Project Structure of: {ROOT_DIR}\n")
    print_directory_tree(ROOT_DIR)

================================================================================
# FILE: project_dump.py
================================================================================

import os

EXCLUDE_DIRS = {'.venv', 'venv', '__pycache__', '.github', '.git', '.idea', '.vscode', 'build', 'dist', '.mypy_cache'}
OUTPUT_FILE = "project_code_dump.txt"
INCLUDE_YAML_FILES = {'config.yaml', 'params.yaml', 'schema.yaml'}


def is_valid_directory(dirname):
    return not any(part in EXCLUDE_DIRS for part in dirname.split(os.sep))


def print_directory_tree(start_path: str, indent: str = "", exclude_dirs=None, out_lines=None) -> list:
    """
    Recursively builds the directory structure starting from `start_path`.
    """
    if exclude_dirs is None:
        exclude_dirs = EXCLUDE_DIRS
    if out_lines is None:
        out_lines = []

    try:
        items = sorted(os.listdir(start_path))
    except PermissionError:
        return

    for item in items:
        item_path = os.path.join(start_path, item)
        if os.path.isdir(item_path):
            if item in exclude_dirs:
                continue
            out_lines.append(f"{indent}📁 {item}/")
            print_directory_tree(item_path, indent + "    ", exclude_dirs, out_lines)
        else:
            out_lines.append(f"{indent}📄 {item}")

    return out_lines


def list_target_files(root_dir):
    py_files = []
    yaml_files = []

    for dirpath, dirnames, filenames in os.walk(root_dir):
        dirnames[:] = [d for d in dirnames if is_valid_directory(os.path.join(dirpath, d))]

        for filename in filenames:
            full_path = os.path.join(dirpath, filename)
            rel_path = os.path.relpath(full_path, root_dir)

            if filename.endswith('.py'):
                py_files.append((rel_path, full_path))
            elif filename in INCLUDE_YAML_FILES:
                yaml_files.append((rel_path, full_path))

    return sorted(py_files), sorted(yaml_files)


def dump_project_code_to_file(root_dir='.'):
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out_file:
        out_file.write(f"\n📦 Project Structure of: {os.path.abspath(root_dir)}\n\n")

        # Directory structure
        tree_lines = print_directory_tree(root_dir, out_lines=[])
        out_file.write("\n".join(tree_lines))
        out_file.write("\n\n--- PYTHON CODE DUMP ---\n\n")

        # File lists
        py_files, yaml_files = list_target_files(root_dir)

        # Dump .py files
        for rel_path, full_path in py_files:
            out_file.write(f"\n{'=' * 80}\n")
            out_file.write(f"# FILE: {rel_path}\n")
            out_file.write(f"{'=' * 80}\n\n")
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    code = f.read()
                    out_file.write(code.strip() + "\n")
            except Exception as e:
                out_file.write(f"Error reading {rel_path}: {e}\n")

        # Dump YAML files
        out_file.write("\n\n--- YAML CONFIG FILES DUMP ---\n\n")
        for rel_path, full_path in yaml_files:
            out_file.write(f"\n{'=' * 80}\n")
            out_file.write(f"# YAML FILE: {rel_path}\n")
            out_file.write(f"{'=' * 80}\n\n")
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    out_file.write(content.strip() + "\n")
            except Exception as e:
                out_file.write(f"Error reading {rel_path}: {e}\n")

    print(f"\n✅ Full project dump saved to: {os.path.abspath(OUTPUT_FILE)}")


if __name__ == "__main__":
    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
    dump_project_code_to_file(ROOT_DIR)

================================================================================
# FILE: project_template.py
================================================================================

import os
from pathlib import Path
import logging


# ==============================
# 🔹 LOGGING SETUP
# ==============================

# ✅ Define the directory where logs will be stored
log_dir = "logs"

# ✅ Define the log file name and full path
log_filepath = os.path.join(log_dir, 'directorygen_logs.log')

# ✅ Define the format for log messages
log_format = '[%(asctime)s] - %(levelname)s - %(module)s - %(message)s'


def setup_logging():
    """
    Sets up a custom logger:
    - Creates the `logs/` directory if it doesn't exist.
    - Configures log messages to be written to both a file and the console.
    - Uses append mode (`"a"`) so logs persist across multiple runs.
    - Ensures handlers are not added multiple times.
    - Logger name: `directory_builder` (used for all logging in this script).

    Returns:
        logging.Logger: Custom logger instance.
    """

    # ✅ Ensure the log directory exists before creating the log file
    os.makedirs(log_dir, exist_ok=True)

    # ✅ Create a custom logger (separate from the root logger)
    logger = logging.getLogger('directory_builder')

    # ✅ Set the logger level to DEBUG (captures all log levels)
    logger.setLevel(logging.DEBUG)

    # ✅ Prevent adding duplicate handlers
    if not logger.hasHandlers():
        formatter = logging.Formatter(log_format)  # ✅ Define the log message format

        # ✅ Create a File Handler (logs INFO and above)
        file_handler = logging.FileHandler(log_filepath, mode='a')  # Append mode ("a")
        file_handler.setFormatter(formatter)  # Apply the log format

        # ✅ Create a Stream Handler (logs DEBUG and above to console)
        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(formatter)  # Apply the log format

        # ✅ Add handlers to the logger
        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)

    return logger  # ✅ Return the configured logger


# ==============================
# 🔹 PROJECT SETUP
# ==============================

# ✅ Define the project name (used in file paths)
project_name = input("Please enter the project name: ")

# ✅ List of files and directories to be created in the project structure
list_of_files = [
    # 🔹 GitHub workflows (for CI/CD setup)
    ".github/workflows/.gitkeep",
    
    # 🔹 Source Code Structure
    f"src/{project_name}/__init__.py",  # Main package initializer
    f"src/{project_name}/components/__init__.py",  # Components submodule initializer
    f"src/{project_name}/utils/__init__.py",  # Utilities submodule initializer
    f"src/{project_name}/utils/common.py",  # Common utility functions
    f"src/{project_name}/config/__init__.py",  # Configuration submodule
    f"src/{project_name}/config/configuration.py",  # Configuration handling script
    f"src/{project_name}/pipeline/__init__.py",  # Pipeline processing module
    f"src/{project_name}/entity/__init__.py",  # Entity-related module
    f"src/{project_name}/entity/config_entity.py",  # Configuration entity class
    f"src/{project_name}/constants/__init__.py",  # Constants module

    # 🔹 Configuration and Parameter Files
    "config/config.yaml",  # YAML file for configuration settings
    "params.yaml",  # YAML file for parameter tuning
    "schema.yaml",  # YAML file for data schema definition

    # 🔹 Project Execution and Deployment
    "main.py",  # Main entry point of the project
    "Dockerfile",  # Dockerfile for containerization
    "setup.py",  # Setup script for packaging
    "requirements.txt",  # Requirements file for Python dependencies

    # 🔹 Research and Web Components
    "research/research.ipynb",  # Jupyter notebook for exploratory research
    "templates/index.html",  # HTML template file (for a web component)

    # 🔹 Backend API
    "app.py"  # Flask or FastAPI backend application script
]


# ==============================
# 🔹 DIRECTORY & FILE CREATION
# ==============================

def create_file_structure(file_list):
    """
    Creates directories and files based on the given list.
    
    - If a directory does not exist, it is created.
    - If a file does not exist or is empty, it is created.
    - Logs every operation to track what is being created.

    Parameters:
        file_list (list): List of file paths to be created.
    """

    for filepath in file_list:
        filepath = Path(filepath)  # ✅ Convert string path to a `Path` object
        filedir, filename = os.path.split(filepath)  # ✅ Extract directory and filename separately

        # ✅ Ensure the parent directory exists before creating the file
        if filedir:
            os.makedirs(filedir, exist_ok=True)  # ✅ Create directory if it does not exist
            logger.info(f"Creating the directory '{filedir}' for file: '{filename}'")

        # ✅ Check if the file does not exist or is empty, then create it
        if not filepath.exists() or filepath.stat().st_size == 0:
            with open(filepath, 'w'):  # ✅ Create an empty file
                pass  # No content is added, just initializing the file
            logger.info(f"Creating empty file: '{filepath}'")  # ✅ Log file creation
        else:
            logger.info(f"'{filepath}' already exists")  # ✅ Log if the file already exists


# ✅ Initialize the logger
logger = setup_logging()

# ✅ Run the file creation function
create_file_structure(list_of_files)

================================================================================
# FILE: setup.py
================================================================================

from setuptools import find_packages, setup
from typing import List


def get_requirements() -> List[str]:
    requirement_lst: List[str] = []
    try:
        with open("requirements.txt", "r") as file:
            lines = file.readlines()
            for line in lines:
                requirement = line.strip()
                if requirement and requirement != "-e .":
                    requirement_lst.append(requirement)
    except FileExistsError:
        print("'requirements.txt' file not found")

    return requirement_lst


setup(
    name="NetworkSecurity",
    version="0.0.1",
    author="Gokul Krishna",
    author_email="iamgokul93@gmail.com",
    packages=find_packages(),
    install_requires=get_requirements(),
)

================================================================================
# FILE: src\networksecurity\__init__.py
================================================================================



================================================================================
# FILE: src\networksecurity\cloud\__init__.py
================================================================================



================================================================================
# FILE: src\networksecurity\components\__init__.py
================================================================================



================================================================================
# FILE: src\networksecurity\components\data_ingestion.py
================================================================================

import numpy as np
import pandas as pd

from src.networksecurity.entity.config_entity import DataIngestionConfig
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact
from src.networksecurity.dbhandler.base_handler import DBHandler
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from src.networksecurity.utils.core import save_to_csv, read_csv


class DataIngestion:
    def __init__(
        self,
        config: DataIngestionConfig,
        db_handler: DBHandler,
    ):
        try:
            self.config = config
            self.db_handler = db_handler
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def __fetch_raw_data(self) -> pd.DataFrame:
        try:
            with self.db_handler as handler:
                df = handler.load_from_source()
            logger.info(f"Fetched {len(df)} raw rows from data source.")
            return df
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def __clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        try:
            df_cleaned = df.drop(columns=["_id"], errors="ignore").copy()
            df_cleaned.replace({"na": np.nan}, inplace=True)
            logger.info("Raw DataFrame cleaned successfully.")
            return df_cleaned
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run_ingestion(self) -> DataIngestionArtifact:
        try:
            logger.info("========== Starting Data Ingestion ==========")

            # Step 1: Fetch raw data
            raw_df = self.__fetch_raw_data()

            # Step 2: Save raw data
            save_to_csv(
                raw_df,
                self.config.raw_data_filepath,
                self.config.raw_dvc_path,
                label="Raw Data"
            )

            # Step 3: Clean raw data
            cleaned_df = self.__clean_dataframe(raw_df)

            # Step 4: Save cleaned (ingested) data
            save_to_csv(
                cleaned_df,
                self.config.ingested_data_filepath,
                label="Cleaned (Ingested) Data"
            )

            logger.info("========== Data Ingestion Completed ==========")

            return DataIngestionArtifact(
                raw_artifact_path=self.config.raw_data_filepath,
                raw_dvc_path=self.config.raw_dvc_path,
                ingested_data_filepath=self.config.ingested_data_filepath,
            )

        except Exception as e:
            logger.error("Data ingestion failed.")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\components\data_transformation.py
================================================================================

# FILE: src/networksecurity/components/data_transformation.py

from pathlib import Path
from typing import Tuple
import pandas as pd
from sklearn.model_selection import train_test_split

from src.networksecurity.entity.config_entity import DataTransformationConfig
from src.networksecurity.entity.artifact_entity import DataValidationArtifact, DataTransformationArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.utils.core import read_csv, save_to_csv, save_object, save_array
from src.networksecurity.data_processors.preprocessor_builder import PreprocessorBuilder
from src.networksecurity.constants.constants import (
    X_TRAIN_LABEL, Y_TRAIN_LABEL,
    X_VAL_LABEL, Y_VAL_LABEL,
    X_TEST_LABEL, Y_TEST_LABEL
)


class DataTransformation:
    def __init__(self, config: DataTransformationConfig, validation_artifact: DataValidationArtifact):
        try:
            self.config = config
            self.validation_artifact = validation_artifact
            self.df = read_csv(validation_artifact.validated_filepath, label="Validated Data")
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _split_features_and_target(self) -> Tuple[pd.DataFrame, pd.Series]:
        try:
            df = self.df.copy()
            X = df.drop(columns=[self.config.target_column])
            y = df[self.config.target_column]
            return X, y
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _split_data(self, X: pd.DataFrame, y: pd.Series) -> Tuple:
        try:
            split_params = self.config.transformation_params.data_split
            stratify = y if split_params.stratify else None

            X_train, X_temp, y_train, y_temp = train_test_split(
                X, y,
                train_size=split_params.train_size,
                stratify=stratify,
                random_state=split_params.random_state
            )

            relative_test_size = split_params.test_size / (split_params.test_size + split_params.val_size)
            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp,
                test_size=relative_test_size,
                stratify=y_temp if split_params.stratify else None,
                random_state=split_params.random_state
            )

            return X_train, X_val, X_test, y_train, y_val, y_test
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _save_datasets(self, X_train, X_val, X_test, y_train, y_val, y_test):
        try:
            save_array(X_train, self.config.x_train_filepath, self.config.x_train_dvc_filepath, label=X_TRAIN_LABEL)

            save_array(y_train, self.config.y_train_filepath, self.config.y_train_dvc_filepath, label=Y_TRAIN_LABEL)
            save_array(X_val, self.config.x_val_filepath, self.config.x_val_dvc_filepath, label=X_VAL_LABEL)
            save_array(y_val, self.config.y_val_filepath, self.config.y_val_dvc_filepath, label=Y_VAL_LABEL)
            save_array(X_test, self.config.x_test_filepath, self.config.x_test_dvc_filepath, label=X_TEST_LABEL)
            save_array(y_test, self.config.y_test_filepath, self.config.y_test_dvc_filepath, label=Y_TEST_LABEL)
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run_transformation(self) -> DataTransformationArtifact:
        try:
            logger.info("========== Starting Data Transformation ==========")

            # Step 1: Separate features and target
            X, y = self._split_features_and_target()

            # Step 2: Split data into train/val/test
            X_train, X_val, X_test, y_train, y_val, y_test = self._split_data(X, y)

            # Step 3: Build preprocessor pipelines for X and Y
            builder = PreprocessorBuilder(
                steps=self.config.transformation_params.steps,
                methods=self.config.transformation_params.methods,
            )
            x_processor, y_processor = builder.build()

            # Step 4: Fit and transform data
            X_train = x_processor.fit_transform(X_train)
            X_val = x_processor.transform(X_val)
            X_test = x_processor.transform(X_test)

            y_train = y_processor.fit_transform(y_train)
            y_val = y_processor.transform(y_val)
            y_test = y_processor.transform(y_test)

            # Step 5: Save X and Y processors
            save_object(x_processor, self.config.x_preprocessor_filepath, label="X Preprocessor Pipeline")
            save_object(y_processor, self.config.y_preprocessor_filepath, label="Y Preprocessor Pipeline")

            # Step 6: Save transformed datasets
            self._save_datasets(X_train, X_val, X_test, y_train, y_val, y_test)

            logger.info("========== Data Transformation Completed ==========")

            return DataTransformationArtifact(
                x_train_filepath=self.config.x_train_filepath,
                y_train_filepath=self.config.y_train_filepath,
                x_val_filepath=self.config.x_val_filepath,
                y_val_filepath=self.config.y_val_filepath,
                x_test_filepath=self.config.x_test_filepath,
                y_test_filepath=self.config.y_test_filepath
            )

        except Exception as e:
            logger.error("Data transformation failed.")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\components\data_validation.py
================================================================================

import hashlib
import pandas as pd
from pathlib import Path
from box import ConfigBox
from datetime import timezone
from scipy.stats import ks_2samp

from src.networksecurity.entity.config_entity import DataValidationConfig
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact, DataValidationArtifact
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from src.networksecurity.utils.core import save_to_yaml, save_to_csv, save_to_json, read_csv
from src.networksecurity.utils.timestamp import get_shared_utc_timestamp


class DataValidation:
    def __init__(self, config: DataValidationConfig, ingestion_artifact: DataIngestionArtifact):
        try:
            self.config = config
            self.schema = config.schema
            self.params = config.validation_params
            self.df = read_csv(ingestion_artifact.ingested_data_filepath, "Ingested Data")

            self.base_df = None
            self.drift_check_performed = False
            if self.params.drift_detection.enabled and config.validated_dvc_path.exists():
                self.base_df = read_csv(config.validated_dvc_path, "Validated Base Data")
                self.drift_check_performed = True

            self.validated_filepath = None
            self.timestamp = get_shared_utc_timestamp()

            self.report = ConfigBox(config.val_report_template.copy())
            self.critical_checks = ConfigBox({k: False for k in self.report.check_results.critical_checks.keys()})
            self.non_critical_checks = ConfigBox({k: False for k in self.report.check_results.non_critical_checks.keys()})
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _check_schema_hash(self):
        try:
            expected_schema = self.schema.columns
            expected_str = "|".join(f"{col}:{dtype}" for col, dtype in sorted(expected_schema.items()))
            expected_hash = hashlib.md5(expected_str.encode()).hexdigest()

            current_str = "|".join(f"{col}:{self.df[col].dtype}" for col in sorted(self.df.columns))
            current_hash = hashlib.md5(current_str.encode()).hexdigest()

            self.critical_checks.schema_is_match = (current_hash == expected_hash)
            logger.info("Schema hash check passed." if self.critical_checks.schema_is_match else "Schema hash mismatch.")
        except Exception as e:
            self.critical_checks.schema_is_match = False
            raise NetworkSecurityError(e, logger) from e

    def _check_structure_schema(self):
        try:
            expected = set(self.schema.columns.keys()) | {self.schema.target_column}
            actual = set(self.df.columns)
            self.critical_checks.schema_is_match = (expected == actual)
            if not self.critical_checks.schema_is_match:
                logger.error(f"Schema structure mismatch: expected={expected}, actual={actual}")
        except Exception as e:
            self.critical_checks.schema_is_match = False
            raise NetworkSecurityError(e, logger) from e

    def _check_missing_values(self):
        try:
            missing = self.df.isnull().sum().to_dict()
            missing["timestamp"] = self.timestamp
            save_to_yaml(missing, self.config.missing_report_filepath, label="Missing Value Report")
            self.non_critical_checks.no_missing_values = not any(v > 0 for v in missing.values() if isinstance(v, (int, float)))
        except Exception as e:
            self.non_critical_checks.no_missing_values = False
            raise NetworkSecurityError(e, logger) from e

    def _check_duplicates(self):
        try:
            before = len(self.df)
            self.df = self.df.drop_duplicates()
            after = len(self.df)
            duplicates_removed = before - after

            result = {
                "duplicate_rows_removed": duplicates_removed,
                "timestamp": self.timestamp
            }

            save_to_json(result, self.config.duplicates_report_filepath, label="Duplicates Report")
            self.non_critical_checks.no_duplicate_rows = (duplicates_removed == 0)
        except Exception as e:
            self.non_critical_checks.no_duplicate_rows = False
            raise NetworkSecurityError(e, logger) from e

    def _check_drift(self):
        try:
            if self.base_df is None:
                logger.info("Base dataset not found. Skipping drift check.")
                return

            drift_results = {}
            drift_detected = False

            for col in self.df.columns:
                if col not in self.base_df.columns:
                    continue
                _, p = ks_2samp(self.base_df[col], self.df[col])
                drift = bool(p < self.params.drift_detection.p_value_threshold)
                drift_results[col] = {"p_value": float(p), "drift": drift}
                if drift:
                    drift_detected = True

            drift_results["drift_detected"] = drift_detected
            drift_results["timestamp"] = self.timestamp

            save_to_yaml(drift_results, self.config.drift_report_filepath, label="Drift Result")
            self.critical_checks.no_data_drift = not drift_detected
        except Exception as e:
            self.critical_checks.no_data_drift = False
            raise NetworkSecurityError(e, logger) from e

    def _generate_report(self) -> dict:
        try:
            validation_status = all(self.critical_checks.values())
            non_critical_passed = all(self.non_critical_checks.values())

            self.report.timestamp = self.timestamp
            self.report.validation_status = validation_status
            self.report.critical_passed = validation_status
            self.report.non_critical_passed = non_critical_passed
            self.report.schema_check_type = self.params.schema_check.method

            if self.drift_check_performed:
                self.report.drift_check_method = self.params.drift_detection.method
            else:
                del self.report["drift_check_method"]

            self.report.check_results.critical_checks = self.critical_checks.to_dict()
            self.report.check_results.non_critical_checks = self.non_critical_checks.to_dict()

            return self.report.to_dict()
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run_validation(self) -> DataValidationArtifact:
        try:
            logger.info("Running data validation...")

            if self.params.schema_check.method == "hash":
                self._check_schema_hash()
            else:
                self._check_structure_schema()

            self._check_missing_values()
            self._check_duplicates()

            if self.params.drift_detection.enabled:
                self._check_drift()

            report_dict = self._generate_report()
            save_to_yaml(report_dict, self.config.validation_report_filepath, label="Validation Report")

            is_valid = all(self.critical_checks.values())
            validated_filepath = self.config.validated_filepath if is_valid else None

            if is_valid:
                save_to_csv(self.df, validated_filepath, self.config.validated_dvc_path, label="Validated Data")
            else:
                logger.warning("Validation failed. Validated data not saved.")

            return DataValidationArtifact(
                validated_filepath=validated_filepath,
                validation_status=is_valid
            )

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\components\model_trainer.py
================================================================================

import os
import importlib
from pathlib import Path
from datetime import datetime, timezone

import dagshub
import mlflow
import optuna
import numpy as np
import joblib

from sklearn.model_selection import cross_val_score
from sklearn.metrics import get_scorer
from mlflow import sklearn as mlflow_sklearn

from src.networksecurity.entity.config_entity import ModelTrainerConfig
from src.networksecurity.entity.artifact_entity import (
    DataTransformationArtifact,
    ModelTrainerArtifact,
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.utils.core import save_to_yaml, save_object


class ModelTrainer:
    def __init__(
        self,
        config: ModelTrainerConfig,
        transformation_artifact: DataTransformationArtifact,
    ):
        """
        Args:
            config: ModelTrainerConfig (paths, optuna, mlflow, etc.)
            transformation_artifact: DataTransformationArtifact (x/y train & val filepaths)
        """
        try:
            self.config = config
            self.transformation_artifact = transformation_artifact

            logger.info(f"Initializing ModelTrainer with root_dir={config.root_dir}")
            config.root_dir.mkdir(parents=True, exist_ok=True)

            if config.tracking.mlflow.enabled:
                logger.info("MLflow tracking is enabled. Initializing DagsHub + MLflow...")
                dagshub.init(
                    repo_owner=os.getenv("DAGSHUB_REPO_OWNER"),
                    repo_name=os.getenv("DAGSHUB_REPO_NAME"),
                    mlflow=True,
                )
                mlflow.set_tracking_uri(config.tracking.tracking_uri)
                mlflow.set_experiment(config.tracking.mlflow.experiment_name)
                logger.info(f"MLflow experiment set to '{config.tracking.mlflow.experiment_name}' "
                            f"@ {config.tracking.tracking_uri}")

        except Exception as e:
            logger.error("Failed to initialize ModelTrainer", exc_info=True)
            raise NetworkSecurityError(e, logger) from e

    def _load_data(self):
        """Load NumPy arrays directly from the transformation artifact."""
        try:
            logger.info("Loading transformed data arrays from disk...")
            self.X_train = np.load(self.transformation_artifact.x_train_filepath)
            self.y_train = np.load(self.transformation_artifact.y_train_filepath)
            self.X_val   = np.load(self.transformation_artifact.x_val_filepath)
            self.y_val   = np.load(self.transformation_artifact.y_val_filepath)
            logger.info(f"Loaded X_train {self.X_train.shape}, y_train {self.y_train.shape}; "
                        f"X_val {self.X_val.shape}, y_val {self.y_val.shape}")
        except Exception as e:
            logger.error("Error loading data arrays", exc_info=True)
            raise NetworkSecurityError(e, logger) from e

    def _instantiate(self, fullname: str, params: dict):
        """Dynamically import & instantiate a sklearn estimator."""
        module, cls = fullname.rsplit(".", 1)
        logger.info(f"Instantiating model {fullname} with params={params}")
        return getattr(importlib.import_module(module), cls)(**(params or {}))

    def _optimize_one(self, model_spec: dict):
        """
        Run Optuna for a single candidate model spec.
        Supports categorical, int, float (with optional log scale).
        """
        search_space = model_spec.get("search_space", {})
        n_trials = self.config.optimization.n_trials
        cv_folds = self.config.optimization.cv_folds
        direction = self.config.optimization.direction

        model_name = model_spec["name"]
        logger.info(f"Starting hyperparameter search for {model_name}: "
                    f"{n_trials} trials, cv={cv_folds}, direction={direction}")
        logger.info(f"Search space: {search_space}")

        def objective(trial):
            sampled = {}
            for name, space in search_space.items():
                if "choices" in space:
                    sampled[name] = trial.suggest_categorical(name, space["choices"])
                else:
                    low, high = space["low"], space["high"]
                    step = space.get("step") or 1
                    log = space.get("log", False)
                    if isinstance(low, int) and isinstance(high, int):
                        sampled[name] = trial.suggest_int(name, low, high, step=step)
                    else:
                        sampled[name] = trial.suggest_float(name, float(low), float(high), log=log)

            logger.info(f"Trial params for {model_name}: {sampled}")
            clf = self._instantiate(model_name, sampled)
            scores = cross_val_score(
                clf, self.X_train, self.y_train,
                cv=cv_folds,
                scoring=self.config.optimization.scoring,
                n_jobs=-1,
            )
            mean_score, std_score = float(scores.mean()), float(scores.std())

            if self.config.tracking.mlflow.log_trials:
                with mlflow.start_run(nested=True):
                    mlflow.log_params(sampled)
                    mlflow.log_metric(f"mean_{self.config.optimization.scoring}", mean_score)
                    mlflow.log_metric(f"std_{self.config.optimization.scoring}", std_score)

            return mean_score

        study = optuna.create_study(direction=direction, sampler=optuna.samplers.TPESampler())
        study.optimize(objective, n_trials=n_trials)
        logger.info(f"Completed search for {model_name}: best value={study.best_value:.4f}, "
                    f"params={study.best_trial.params}")
        return study.best_trial, study

    def _select_and_tune(self):
        """
        Across all candidate specs:
         - if optimization.enabled: run Optuna → pick best trial
         - else: simple CV mean
        Returns a dict with keys: spec, score, trial, study
        """
        logger.info("Selecting best model among candidates...")
        best = {"score": -float("inf"), "spec": None, "trial": None, "study": None}

        for spec in self.config.models:
            model_name = spec["name"]
            if self.config.optimization.enabled:
                trial, study = self._optimize_one(spec)
                score = trial.value
            else:
                clf = self._instantiate(model_name, spec.get("params", {}))
                cv_scores = cross_val_score(
                    clf, self.X_train, self.y_train,
                    cv=self.config.optimization.cv_folds,
                    scoring=self.config.optimization.scoring,
                    n_jobs=-1,
                )
                score, trial, study = float(cv_scores.mean()), None, None
                logger.info(f"Candidate {model_name} CV mean={score:.4f}")

            if score > best["score"]:
                best.update(spec=spec, score=score, trial=trial, study=study)

        logger.info(f"Best model: {best['spec']['name']} with score={best['score']:.4f}")
        return best

    def _train_and_eval(self, spec: dict, params: dict):
        """
        Fit the champion on the train set, then compute train & val metrics.
        """
        model_name = spec["name"]
        logger.info(f"Training final model {model_name} on full training set...")
        clf = self._instantiate(model_name, params)
        clf.fit(self.X_train, self.y_train)

        train_metrics = {
            m: float(get_scorer(m)(clf, self.X_train, self.y_train))
            for m in self.config.tracking.mlflow.metrics_to_log
        }
        val_metrics = {
            m: float(get_scorer(m)(clf, self.X_val, self.y_val))
            for m in self.config.tracking.mlflow.metrics_to_log
        }

        logger.info(f"Final eval for {model_name} -> train: {train_metrics}, val: {val_metrics}")
        return clf, train_metrics, val_metrics

    def _generate_report(self, best, train_m, val_m):
        t = datetime.now(timezone.utc).isoformat()
        report = {
            "timestamp":         t,
            "best_model":        best["spec"]["name"].split(".")[-1],
            "best_model_params": (best["trial"].params if best["trial"] else best["spec"].get("params")),
            "train_metrics":     train_m,
            "val_metrics":       val_m,
            "optimization": {
                "method":     self.config.optimization.method,
                "best_trial": (best["trial"].number if best["trial"] else None),
                "n_trials":   self.config.optimization.n_trials,
                "direction":  self.config.optimization.direction,
                "cv": {
                    "folds":      self.config.optimization.cv_folds,
                    "mean_score": best["score"],
                    "std_score":  float(best["study"].best_trial.user_attrs.get("std", 0.0))
                                  if best["study"] else None
                }
            }
        }
        logger.info(f"Generated training report: {report}")
        return report

    def run_training(self) -> ModelTrainerArtifact:
        try:
            logger.info("========== Starting Model Training Pipeline ==========")

            # 1) load data
            self._load_data()

            # 2) hyperparameter search + final fit in a single MLflow run
            with mlflow.start_run():
                best = self._select_and_tune()
                clf, tr_m, val_m = self._train_and_eval(
                    best["spec"],
                    (best["trial"].params if best["trial"] else best["spec"].get("params")),
                )

                mlflow.log_params(best["trial"].params if best["trial"] else best["spec"].get("params"))
                for metric_name, metric_val in tr_m.items():
                    mlflow.log_metric(f"train_{metric_name}", metric_val)
                for metric_name, metric_val in val_m.items():
                    mlflow.log_metric(f"val_{metric_name}", metric_val)

                mlflow_sklearn.log_model(
                    sk_model=clf,
                    artifact_path="model",
                    registered_model_name=self.config.tracking.mlflow.registry_model_name
                )
                logger.info("Registered model to MLflow registry "
                            f"'{self.config.tracking.mlflow.registry_model_name}'")

            # 3) persist local artifacts
            report = self._generate_report(best, tr_m, val_m)
            save_to_yaml(report, self.config.training_report_filepath, label="Training Report")
            save_object(clf, self.config.trained_model_filepath, "Best Model")

            logger.info("========== Model Training Pipeline Completed ==========")
            return ModelTrainerArtifact(
                trained_model_filepath=self.config.trained_model_filepath,
                training_report_filepath=self.config.training_report_filepath,
            )

        except Exception as e:
            logger.error("Model training pipeline failed", exc_info=True)
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\config\__init__.py
================================================================================



================================================================================
# FILE: src\networksecurity\config\configuration.py
================================================================================

from pathlib import Path
import os

from src.networksecurity.constants.constants import (
    CONFIG_FILE_PATH,
    PARAMS_FILE_PATH,
    SCHEMA_FILE_PATH,
    TEMPLATES_FILE_PATH,
    MONGO_HANDLER_SUBDIR,
    MONGO_JSON_SUBDIR,
    DATA_INGESTION_SUBDIR,
    FEATURESTORE_SUBDIR,
    INGESTED_SUBDIR,
    DATA_VALIDATION_SUBDIR,
    VALIDATED_SUBDIR,
    REPORTS_SUBDIR,
    DATA_TRANSFORMATION_SUBDIR,
    TRANSFORMED_DATA_SUBDIR,
    DATA_TRAIN_SUBDIR,
    DATA_VAL_SUBDIR,
    DATA_TEST_SUBDIR,
    TRANSFORMED_OBJECT_SUBDIR,
    LOGS_ROOT,
    MODEL_TRAINER_SUBDIR
)

from src.networksecurity.entity.config_entity import (
    MongoHandlerConfig,
    DataIngestionConfig,
    DataValidationConfig,
    DataTransformationConfig,
    ModelTrainerConfig
)

from src.networksecurity.utils.core import (
    read_yaml,
    replace_username_password_in_uri,
)
from src.networksecurity.utils.timestamp import get_shared_utc_timestamp
from src.networksecurity.logging import logger


class ConfigurationManager:
    """
    Loads YAML-based configuration and generates paths for all pipeline stages.
    Dynamically sets timestamped artifact directories per run.
    """
    _global_timestamp: str = None

    def __init__(
        self,
        config_filepath: Path = CONFIG_FILE_PATH,
        params_filepath: Path = PARAMS_FILE_PATH,
        schema_filepath: Path = SCHEMA_FILE_PATH,
        templates_filepath: Path = TEMPLATES_FILE_PATH,
    ) -> None:
        self._load_configs(config_filepath, params_filepath, schema_filepath, templates_filepath)
        self._initialize_paths()

    def _load_configs(self, config_filepath: Path, params_filepath: Path, schema_filepath: Path, templates_filepath: Path):
        self.config = read_yaml(config_filepath)
        self.params = read_yaml(params_filepath)
        self.schema = read_yaml(schema_filepath)
        self.templates = read_yaml(templates_filepath)

    def _initialize_paths(self) -> None:
        if ConfigurationManager._global_timestamp is None:
            ConfigurationManager._global_timestamp = get_shared_utc_timestamp()

        timestamp = ConfigurationManager._global_timestamp
        base_artifact_root = Path(self.config.project.artifacts_root)
        self.artifacts_root = base_artifact_root / timestamp

        self.logs_root = Path(LOGS_ROOT) / timestamp

        
        

    def get_logs_dir(self) -> Path:
        return self.logs_root

    def get_artifact_root(self) -> Path:
        return self.artifacts_root

    def get_mongo_handler_config(self) -> MongoHandlerConfig:
        mongo_cfg = self.config.mongo_handler
        root_dir = self.artifacts_root / MONGO_HANDLER_SUBDIR
        json_data_dir = root_dir / MONGO_JSON_SUBDIR

        mongodb_uri = replace_username_password_in_uri(
            base_uri=os.getenv("MONGODB_URI_BASE"),
            username=os.getenv("MONGODB_USERNAME"),
            password=os.getenv("MONGODB_PASSWORD"),
        )

        return MongoHandlerConfig(
            root_dir=root_dir,
            input_data_path=Path(mongo_cfg.input_data_path),
            json_data_filename=mongo_cfg.json_data_filename,
            json_data_dir=json_data_dir,
            mongodb_uri=mongodb_uri,
            database_name=mongo_cfg.database_name,
            collection_name=mongo_cfg.collection_name,
        )

    def get_data_ingestion_config(self) -> DataIngestionConfig:
        ingestion_cfg = self.config.data_ingestion
        root_dir = self.artifacts_root / DATA_INGESTION_SUBDIR
        featurestore_dir = root_dir / FEATURESTORE_SUBDIR
        ingested_data_dir = root_dir / INGESTED_SUBDIR

        raw_dvc_path = Path(self.config.data_paths.raw_data_dvc_filepath)

        return DataIngestionConfig(
            root_dir=root_dir,
            featurestore_dir=featurestore_dir,
            raw_data_filename=ingestion_cfg.raw_data_filename,
            ingested_data_dir=ingested_data_dir,
            ingested_data_filename=ingestion_cfg.ingested_data_filename,
            raw_dvc_path=raw_dvc_path,
        )

    def get_data_validation_config(self) -> DataValidationConfig:
        validation_cfg = self.config.data_validation
        root_dir = self.artifacts_root / DATA_VALIDATION_SUBDIR
        validated_dir = root_dir / VALIDATED_SUBDIR
        report_dir = root_dir / REPORTS_SUBDIR

        validated_dvc_path = Path(self.config.data_paths.validated_dvc_filepath)

        return DataValidationConfig(
            root_dir=root_dir,
            validated_dir=validated_dir,
            validated_filename=validation_cfg.validated_filename,
            report_dir=report_dir,
            missing_report_filename=validation_cfg.missing_report_filename,
            duplicates_report_filename=validation_cfg.duplicates_report_filename,
            drift_report_filename=validation_cfg.drift_report_filename,
            validation_report_filename=validation_cfg.validation_report_filename,
            schema=self.schema,
            validated_dvc_path=validated_dvc_path,
            validation_params=self.params.validation_params,
            val_report_template=self.templates.validation_report
        )

    def get_data_transformation_config(self) -> DataTransformationConfig:
        transformation_cfg = self.config.data_transformation
        transformation_params = self.params.transformation_params
        target_column = self.schema.target_column

        root_dir = self.artifacts_root / DATA_TRANSFORMATION_SUBDIR
        train_dir = root_dir / TRANSFORMED_DATA_SUBDIR / DATA_TRAIN_SUBDIR
        val_dir = root_dir / TRANSFORMED_DATA_SUBDIR / DATA_VAL_SUBDIR
        test_dir = root_dir / TRANSFORMED_DATA_SUBDIR / DATA_TEST_SUBDIR
        preprocessor_dir = root_dir / TRANSFORMED_OBJECT_SUBDIR

        train_dvc_dir = Path(self.config.data_paths.train_dvc_dir)
        val_dvc_dir = Path(self.config.data_paths.val_dvc_dir)
        test_dvc_dir = Path(self.config.data_paths.test_dvc_dir)

        return DataTransformationConfig(
            root_dir=root_dir,
            transformation_params=transformation_params,
            train_dir=train_dir,
            val_dir=val_dir,
            test_dir=test_dir,
            target_column=target_column,
            x_train_filename=transformation_cfg.x_train_filename,
            y_train_filename=transformation_cfg.y_train_filename,
            x_val_filename=transformation_cfg.x_val_filename,
            y_val_filename=transformation_cfg.y_val_filename,
            x_test_filename=transformation_cfg.x_test_filename,
            y_test_filename=transformation_cfg.y_test_filename,
            preprocessor_dir=preprocessor_dir,
            x_preprocessor_filename=transformation_cfg.x_preprocessor_filename,
            y_preprocessor_filename=transformation_cfg.y_preprocessor_filename,
            train_dvc_dir=train_dvc_dir,
            val_dvc_dir=val_dvc_dir,
            test_dvc_dir=test_dvc_dir,
        )


    def get_model_trainer_config(self) -> ModelTrainerConfig:
        """
        Assemble and return the ModelTrainerConfig dataclass, incorporating:
        - artifact output paths under a timestamped MODEL_TRAINER_SUBDIR
        - filenames & report names from config.yaml
        - candidate models, optimization & tracking from params.yaml
        - MLflow URI injected from environment
        - DVC‐tracked input dirs & filenames for X/Y train/val/test
        """
        # static config.yaml entries
        yaml_cfg = self.config.model_trainer
        # dynamic params.yaml entries
        params_cfg = self.params.model_trainer

        # where to write models & reports
        root_dir = self.artifacts_root / MODEL_TRAINER_SUBDIR

        # DVC‐tracked data dirs (under /data/transformed)
        train_dir = Path(self.config.data_paths.train_dvc_dir)
        val_dir = Path(self.config.data_paths.val_dvc_dir)
        test_dir = Path(self.config.data_paths.test_dvc_dir)

        # pull MLflow sub‐box and inject the URI from env
        mlflow_cfg = params_cfg.tracking
        mlflow_cfg.tracking_uri = os.getenv("MLFLOW_TRACKING_URI")

        return ModelTrainerConfig(
            # where to write artifacts
            root_dir=root_dir,
            trained_model_filename=yaml_cfg.trained_model_filename,
            training_report_filename=yaml_cfg.training_report_filename,

            # what to train & how
            models=params_cfg.models,
            optimization=params_cfg.optimization,
            tracking=mlflow_cfg,

            # where to load transformed data from
            train_dir=train_dir,
            val_dir=val_dir,
            test_dir=test_dir,
        )

================================================================================
# FILE: src\networksecurity\constants\__init__.py
================================================================================

from pathlib import Path

CONFIG_FILE_PATH = Path("config/config.yaml")
PARAMS_FILE_PATH = Path("config/params.yaml")
SCHEMA_FILE_PATH = Path("config/schema.yaml")

================================================================================
# FILE: src\networksecurity\constants\constants.py
================================================================================

from pathlib import Path

# ---------------------------
# Configuration File Paths
# ---------------------------

CONFIG_DIR = Path("config")
CONFIG_FILE_PATH = CONFIG_DIR / "config.yaml"
PARAMS_FILE_PATH = CONFIG_DIR / "params.yaml"
SCHEMA_FILE_PATH = CONFIG_DIR / "schema.yaml"
TEMPLATES_FILE_PATH = CONFIG_DIR / "templates.yaml"

# ---------------------------
# Generic Constants
# ---------------------------

MISSING_VALUE_TOKEN = "na"

# ---------------------------
# MongoDB Connection Settings
# ---------------------------

MONGODB_CONNECT_TIMEOUT_MS = 40000
MONGODB_SOCKET_TIMEOUT_MS = 40000

# ---------------------------
# Root Directories
# ---------------------------

LOGS_ROOT = "logs"  # Central log directory (outside artifacts)
STABLE_DATA_DIR = Path("data")
RAW_DATA_SUBDIR = "raw"
VALIDATED_DATA_SUBDIR = "validated"
TRANSFORMED_DATA_SUBDIR = "transformed"
MODEL_DIR = "model"
EVALUATION_DIR = "evaluation"
PREDICTIONS_DIR = "predictions"

# ---------------------------
# Artifact Subdirectory Names (Dynamic Timestamped)
# ---------------------------

MONGO_HANDLER_SUBDIR = "mongo_handler"
MONGO_JSON_SUBDIR = "JSON_data"

DATA_INGESTION_SUBDIR = "data_ingestion"
FEATURESTORE_SUBDIR = "featurestore"
INGESTED_SUBDIR = "ingested"

DATA_VALIDATION_SUBDIR = "data_validation"
VALIDATED_SUBDIR = "validated"
REPORTS_SUBDIR = "reports"
SCHEMA_HASH_SUBDIR = "schema_hash"

DATA_TRANSFORMATION_SUBDIR = "data_transformation"
DATA_SUBDIR = "data"
DATA_TRAIN_SUBDIR = "train"
DATA_VAL_SUBDIR = "val"
DATA_TEST_SUBDIR = "test"

TRANSFORMED_OBJECT_SUBDIR = "preprocessor"

MODEL_TRAINER_SUBDIR = "model_trainer"
MODEL_EVALUATION_SUBDIR = "model_evaluation"
MODEL_PREDICTION_SUBDIR = "model_prediction"

# ---------------------------
# Default Filenames (used by config.yaml)
# ---------------------------

DEFAULT_SCHEMA_HASH_FILENAME = "schema_hash.json"
DEFAULT_VALIDATED_FILENAME = "validated_data.csv"
DEFAULT_MISSING_REPORT_FILENAME = "missing_values_report.json"
DEFAULT_DRIFT_REPORT_FILENAME = "drift_report.yaml"
DEFAULT_VALIDATION_REPORT_FILENAME = "validation_report.yaml"

# Logging and output labels
X_TRAIN_LABEL = "X_train"
Y_TRAIN_LABEL = "y_train"
X_VAL_LABEL = "X_val"
Y_VAL_LABEL = "y_val"
X_TEST_LABEL = "X_test"
Y_TEST_LABEL = "y_test"

================================================================================
# FILE: src\networksecurity\data_processors\encoder_factory.py
================================================================================

from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.pipeline import Pipeline

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class EncoderFactory:
    """
    Factory to build encoding pipelines for categorical features.
    Supports: onehot, ordinal.
    Easily extendable with new encoders.
    """
    _SUPPORTED_METHODS = {
        "onehot": OneHotEncoder,
        "ordinal": OrdinalEncoder
    }

    @staticmethod
    def get_encoder_pipeline(method: str, params: dict = None) -> Pipeline:
        try:
            if method in EncoderFactory._SUPPORTED_METHODS:
                encoder_class = EncoderFactory._SUPPORTED_METHODS[method]
                encoder = encoder_class(**(params or {}))
            else:
                raise ValueError(f"Unsupported encoding method: {method}")

            return Pipeline([("encoder", encoder)])

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\data_processors\imputer_factory.py
================================================================================

from sklearn.experimental import enable_iterative_imputer  # noqa: F401
from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer
from sklearn.pipeline import Pipeline

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class ImputerFactory:
    """
    Factory to generate sklearn-compatible imputer pipelines.
    Supports: knn, simple, iterative, and custom methods.
    Easily extendable with new methods.
    """
    _SUPPORTED_METHODS = {
        "knn": KNNImputer,
        "simple": SimpleImputer,
        "iterative": IterativeImputer,
    }

    @staticmethod
    def get_imputer_pipeline(method: str, params: dict) -> Pipeline:
        try:
            if method == "custom":
                if "custom_callable" not in params:
                    raise ValueError("Custom imputer requires a 'custom_callable' in params.")
                imputer = params["custom_callable"]()
            else:
                imputer_class = ImputerFactory._SUPPORTED_METHODS.get(method)
                if not imputer_class:
                    raise ValueError(f"Unsupported imputation method: {method}")
                imputer = imputer_class(**params)

            return Pipeline([("imputer", imputer)])

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\data_processors\label_mapper.py
================================================================================

from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd


class LabelMapper(BaseEstimator, TransformerMixin):
    """
    Custom transformer to map target labels from one value to another.
    For example, map -1 to 0.
    """
    def __init__(self, from_value, to_value):
        self.from_value = from_value
        self.to_value = to_value

    def fit(self, X, y=None):
        return self

    def transform(self, y):
        if isinstance(y, pd.Series):
            return y.replace(self.from_value, self.to_value)
        elif isinstance(y, pd.DataFrame):
            return y.apply(lambda col: col.replace(self.from_value, self.to_value))
        else:
            raise ValueError("Unsupported data type for label transformation.")

================================================================================
# FILE: src\networksecurity\data_processors\preprocessor_builder.py
================================================================================

from sklearn.pipeline import Pipeline
from sklearn.experimental import enable_iterative_imputer
from src.networksecurity.data_processors.imputer_factory import ImputerFactory
from src.networksecurity.data_processors.scaler_factory import ScalerFactory
from src.networksecurity.data_processors.encoder_factory import EncoderFactory
from src.networksecurity.data_processors.label_mapper import LabelMapper
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class PreprocessorBuilder:
    """
    Builds preprocessing pipelines for X and Y using dynamic configuration.
    Skips steps that are explicitly set to "none" or not provided at all.
    """

    STEP_BUILDERS = {
        "imputer": ImputerFactory.get_imputer_pipeline,
        "scaler": ScalerFactory.get_scaler_pipeline,
        "encoder": EncoderFactory.get_encoder_pipeline,
        "label_mapping": lambda method, params: LabelMapper(from_value=params["from"], to_value=params["to"])
    }

    def __init__(self, steps: dict, methods: dict):
        self.steps = steps or {}
        self.methods = methods or {}

    def _build_pipeline(self, section: str) -> Pipeline:
        try:
            pipeline_steps = []
            steps_to_build = self.steps.get(section, {})
            methods = self.methods.get(section, {})

            for step_name, method_name in steps_to_build.items():
                # Skip if explicitly marked as 'none' (case-insensitive)
                if method_name is None or str(method_name).lower() == "none":
                    logger.info(f"Skipping step '{step_name}' for '{section}' as it is set to 'none'.")
                    continue

                builder = self.STEP_BUILDERS.get(step_name)
                if not builder:
                    raise ValueError(f"Unsupported step: {step_name}")

                params = methods.get(step_name, {})
                pipeline_steps.append((step_name, builder(method_name, params)))

            return Pipeline(pipeline_steps)

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def build(self):
        try:
            x_pipeline = self._build_pipeline("x")
            y_pipeline = self._build_pipeline("y")
            return x_pipeline, y_pipeline
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\data_processors\scaler_factory.py
================================================================================

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.pipeline import Pipeline

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class ScalerFactory:
    """
    Factory to create scaling transformers.
    Supports: standard, minmax, robust.
    Easily extendable with new scalers.
    """
    _SUPPORTED_METHODS = {
        "standard": StandardScaler,
        "minmax": MinMaxScaler,
        "robust": RobustScaler
    }

    @staticmethod
    def get_scaler_pipeline(method: str, params: dict = None) -> Pipeline:
        try:
            if method in ScalerFactory._SUPPORTED_METHODS:
                scaler_class = ScalerFactory._SUPPORTED_METHODS[method]
                scaler = scaler_class(**(params or {}))
            else:
                raise ValueError(f"Unsupported scaler method: {method}")

            return Pipeline([("scaler", scaler)])

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\dbhandler\__init__.py
================================================================================



================================================================================
# FILE: src\networksecurity\dbhandler\base_handler.py
================================================================================

from abc import ABC, abstractmethod
from pathlib import Path
import pandas as pd

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class DBHandler(ABC):
    """
    Abstract base class for all database or storage handlers.
    Enables unified behavior across MongoDB, CSV, PostgreSQL, S3, etc.
    """

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    @abstractmethod
    def close(self) -> None:
        """
        Clean up resources like DB connections or sessions.
        """
        pass

    @abstractmethod
    def load_from_source(self) -> pd.DataFrame:
        """
        Load and return a DataFrame from the underlying data source.
        Example:
            - MongoDB: collection
            - PostgreSQL: table
            - CSVHandler: file
            - S3Handler: object
        """
        pass

    def load_from_csv(self, source: Path) -> pd.DataFrame:
        """
        Generic utility: load a DataFrame from a CSV file.
        Available to all subclasses.
        """
        try:
            df = pd.read_csv(source)
            logger.info(f"DataFrame loaded from CSV: {source}")
            return df
        except Exception as e:
            logger.error(f"Failed to load DataFrame from CSV: {source}")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\dbhandler\mongodb_handler.py
================================================================================

from pymongo import MongoClient
from pymongo.server_api import ServerApi
import pandas as pd
from pathlib import Path

from src.networksecurity.entity.config_entity import MongoHandlerConfig
from src.networksecurity.utils.core import csv_to_json_convertor, save_to_json
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from src.networksecurity.constants.constants import (
    MONGODB_CONNECT_TIMEOUT_MS,
    MONGODB_SOCKET_TIMEOUT_MS,
)

from src.networksecurity.dbhandler.base_handler import DBHandler


class MongoDBHandler(DBHandler):
    def __init__(self, config: MongoHandlerConfig):
        self.config = config
        self._client: MongoClient | None = None
        self._owns_client: bool = False

    def _get_client(self) -> MongoClient:
        if self._client is None:
            self._client = MongoClient(
                self.config.mongodb_uri,
                server_api=ServerApi("1"),
                connectTimeoutMS=MONGODB_CONNECT_TIMEOUT_MS,
                socketTimeoutMS=MONGODB_SOCKET_TIMEOUT_MS,
            )
            logger.info("MongoClient initialized.")
        return self._client

    def close(self) -> None:
        if self._client:
            self._client.close()
            logger.info("MongoClient connection closed.")
            self._client = None

    def ping_mongodb(self) -> None:
        try:
            self._get_client().admin.command("ping")
            logger.info("MongoDB ping successful.")
        except Exception as e:
            logger.error("MongoDB ping failed.")
            raise NetworkSecurityError(e, logger) from e

    def insert_csv_to_collection(self, csv_filepath: Path) -> int:
        try:
            records = csv_to_json_convertor(csv_filepath, self.config.json_data_filepath)

            # Save records as JSON using core utility
            save_to_json(records, self.config.json_data_filepath, label="Converted JSON Records")

            db = self._get_client()[self.config.database_name]
            collection = db[self.config.collection_name]
            result = collection.insert_many(records)
            logger.info(
                f"Inserted {len(result.inserted_ids)} records into "
                f"{self.config.database_name}.{self.config.collection_name}"
            )
            return len(result.inserted_ids)
        except Exception as e:
            logger.error("Failed to insert records into MongoDB.")
            raise NetworkSecurityError(e, logger) from e

    def load_from_source(self) -> pd.DataFrame:
        """
        Load data from the configured MongoDB collection as a pandas DataFrame.
        """
        try:
            db = self._get_client()[self.config.database_name]
            collection = db[self.config.collection_name]
            records = list(collection.find())
            df = pd.DataFrame(records)
            logger.info(
                f"Exported {len(df)} documents from "
                f"{self.config.database_name}.{self.config.collection_name} as DataFrame."
            )
            return df
        except Exception as e:
            logger.error("Failed to export data from MongoDB.")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\entity\__init__.py
================================================================================



================================================================================
# FILE: src\networksecurity\entity\artifact_entity.py
================================================================================

from dataclasses import dataclass
from pathlib import Path
from typing import Optional


@dataclass(frozen=True)
class DataIngestionArtifact:
    raw_artifact_path: Path
    ingested_data_filepath: Path
    raw_dvc_path: Path

    def __repr__(self) -> str:
        raw_artifact_str = self.raw_artifact_path.as_posix() if self.raw_artifact_path else "None"
        raw_dvc_str = self.raw_dvc_path.as_posix() if self.raw_dvc_path else "None"
        ingested_data_str = self.ingested_data_filepath.as_posix() if self.ingested_data_filepath else "None"

        return (
            "\nData Ingestion Artifact:\n"
            f"  - Raw Artifact:         '{raw_artifact_str}'\n"
            f"  - Raw DVC Path:         '{raw_dvc_str}'\n"
            f"  - Ingested Data Path:   '{ingested_data_str}'\n"
        )


@dataclass(frozen=True)
class DataValidationArtifact:
    validated_filepath: Path
    validation_status: bool

    def __repr__(self) -> str:
        validated_str = self.validated_filepath.as_posix() if self.validated_filepath else "None"

        return (
            "\nData Validation Artifact:\n"
            f"  - Validated Data Path: '{validated_str}'\n"
            f"  - Validation Status:   '{self.validation_status}'\n"
        )

@dataclass(frozen=True)
class DataTransformationArtifact:
    x_train_filepath: Path
    y_train_filepath: Path
    x_val_filepath: Path
    y_val_filepath: Path
    x_test_filepath: Path
    y_test_filepath: Path

    def __repr__(self) -> str:
        x_train_str = self.x_train_filepath.as_posix() if self.x_train_filepath else "None"
        y_train_str = self.y_train_filepath.as_posix() if self.y_train_filepath else "None"
        x_val_str = self.x_val_filepath.as_posix() if self.x_val_filepath else "None"
        y_val_str = self.y_val_filepath.as_posix() if self.y_val_filepath else "None"
        x_test_str = self.x_test_filepath.as_posix() if self.x_test_filepath else "None"
        y_test_str = self.y_test_filepath.as_posix() if self.y_test_filepath else "None"

        return (
            "\nData Transformation Artifact:\n"
            f"  - X-Train Data Path:    '{x_train_str}'\n"
            f"  - Y-Train Data Path:    '{y_train_str}'\n"
            f"  - X-Val Data Path:      '{x_val_str}'\n"
            f"  - Y-Val Data Path:      '{y_val_str}'\n"
            f"  - X-Test Data Path:     '{x_test_str}'\n"
            f"  - Y-Test Data Path:     '{y_test_str}'\n"
        )


@dataclass(frozen=True)
class ModelTrainerArtifact:
    trained_model_filepath: Path
    training_report_filepath: Path

    def __repr__(self) -> str:
        model_str  = self.trained_model_filepath.as_posix() if self.trained_model_filepath else "None"
        report_str = self.training_report_filepath.as_posix() if self.training_report_filepath else "None"

        return (
            "\nModel Trainer Artifact:\n"
            f"  - Trained Model Path:   '{model_str}'\n"
            f"  - Training Report Path: '{report_str}'\n"
        )

================================================================================
# FILE: src\networksecurity\entity\config_entity.py
================================================================================

from pathlib import Path
from dataclasses import dataclass
from box import ConfigBox
from typing import List


@dataclass
class MongoHandlerConfig:
    root_dir: Path
    input_data_path: Path
    json_data_filename: str
    json_data_dir: Path
    mongodb_uri: str
    database_name: str
    collection_name: str

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.input_data_path = Path(self.input_data_path)
        self.json_data_dir = Path(self.json_data_dir)

    @property
    def json_data_filepath(self) -> Path:
        return self.json_data_dir / self.json_data_filename


@dataclass
class DataIngestionConfig:
    root_dir: Path
    featurestore_dir: Path
    raw_data_filename: str
    ingested_data_dir: Path
    ingested_data_filename: str
    raw_dvc_path: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.featurestore_dir = Path(self.featurestore_dir)
        self.ingested_data_dir = Path(self.ingested_data_dir)
        self.raw_dvc_path = Path(self.raw_dvc_path)

    @property
    def raw_data_filepath(self) -> Path:
        return self.featurestore_dir / self.raw_data_filename

    @property
    def ingested_data_filepath(self) -> Path:
        return self.ingested_data_dir / self.ingested_data_filename


@dataclass
class DataValidationConfig:
    root_dir: Path
    validated_dir: Path
    validated_filename: str
    report_dir: Path
    missing_report_filename: str
    duplicates_report_filename: str
    drift_report_filename: str
    validation_report_filename: str
    schema: dict
    validation_params: dict
    validated_dvc_path: Path
    val_report_template: dict

    def __post_init__(self) -> Path:
        self.root_dir = Path(self.root_dir)
        self.validated_dir = Path(self.validated_dir)
        self.report_dir = Path(self.report_dir)
        self.validated_dvc_path = Path(self.validated_dvc_path)

    @property
    def validated_filepath(self) -> Path:
        return self.validated_dir / self.validated_filename

    @property
    def missing_report_filepath(self) -> Path:
        return self.report_dir / self.missing_report_filename

    @property
    def duplicates_report_filepath(self) -> Path:
        return self.report_dir / self.duplicates_report_filename

    @property
    def drift_report_filepath(self) -> Path:
        return self.report_dir / self.drift_report_filename

    @property
    def validation_report_filepath(self) -> Path:
        return self.report_dir / self.validation_report_filename


@dataclass
class DataTransformationConfig:
    root_dir: Path
    transformation_params: dict
    train_dir: Path
    val_dir: Path
    test_dir: Path
    x_train_filename: str
    y_train_filename: str
    x_val_filename: str
    y_val_filename: str
    x_test_filename: str
    y_test_filename: str
    preprocessor_dir: Path
    x_preprocessor_filename: str
    y_preprocessor_filename: str
    target_column: str
    train_dvc_dir: Path
    val_dvc_dir: Path
    test_dvc_dir: Path

    def __post_init__(self) -> Path:
        self.root_dir = Path(self.root_dir)
        self.train_dir = Path(self.train_dir)
        self.val_dir = Path(self.val_dir)
        self.test_dir = Path(self.test_dir)
        self.preprocessor_dir = Path(self.preprocessor_dir)

    @property
    def x_train_filepath(self) -> Path:
        return self.train_dir / self.x_train_filename

    @property
    def y_train_filepath(self) -> Path:
        return self.train_dir / self.y_train_filename

    @property
    def x_val_filepath(self) -> Path:
        return self.val_dir / self.x_val_filename

    @property
    def y_val_filepath(self) -> Path:
        return self.val_dir / self.y_val_filename

    @property
    def x_test_filepath(self) -> Path:
        return self.test_dir / self.x_test_filename

    @property
    def y_test_filepath(self) -> Path:
        return self.test_dir / self.y_test_filename

    @property
    def x_preprocessor_filepath(self) -> Path:
        return self.preprocessor_dir / self.x_preprocessor_filename

    @property
    def y_preprocessor_filepath(self) -> Path:
        return self.preprocessor_dir / self.y_preprocessor_filename

    @property
    def x_train_dvc_filepath(self) -> Path:
        return self.train_dvc_dir / self.x_train_filename

    @property
    def y_train_dvc_filepath(self) -> Path:
        return self.train_dvc_dir / self.y_train_filename

    @property
    def x_val_dvc_filepath(self) -> Path:
        return self.val_dvc_dir / self.x_val_filename

    @property
    def y_val_dvc_filepath(self) -> Path:
        return self.val_dvc_dir / self.y_val_filename

    @property
    def x_test_dvc_filepath(self) -> Path:
        return self.test_dvc_dir / self.x_test_filename

    @property
    def y_test_dvc_filepath(self) -> Path:
        return self.test_dvc_dir / self.y_test_filename



@dataclass
class ModelTrainerConfig:
    root_dir: Path
    trained_model_filename: str
    training_report_filename: str
    models: list[dict]
    optimization: dict
    tracking: dict
    models: List[dict]
    optimization: ConfigBox
    tracking: ConfigBox
    # where to load transformed arrays from (DVC-tracked)
    train_dir: Path
    val_dir:   Path
    test_dir:  Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)

    @property
    def trained_model_filepath(self) -> Path:
        return self.root_dir / self.trained_model_filename

    @property
    def training_report_filepath(self) -> Path:
        return self.root_dir / self.training_report_filename

================================================================================
# FILE: src\networksecurity\exception\__init__.py
================================================================================



================================================================================
# FILE: src\networksecurity\exception\exception.py
================================================================================

"""Custom exception and logger interface for the phishing detection web app.

This module defines:
- LoggerInterface: a structural protocol for logging
- NetworkSecurityError: a traceback-aware exception with logging support
"""


import sys
from typing import Protocol


class LoggerInterface(Protocol):
    """A structural interface that defines the logger's required methods.

    Any logger passed to NetworkSecurityException must implement this interface.
    """

    def error(self, message: str) -> None:
        """Log an error-level message."""
        ...


class NetworkSecurityError(Exception):
    """Custom exception class for the phishing detection web application.

    Captures traceback details and logs the error using the provided logger.
    """

    def __init__(self, error_message: Exception, logger: LoggerInterface) -> None:
        """Initialize the exception and log it using the injected logger.

        Args:
            error_message (Exception): The original caught exception.
            logger (LoggerInterface): A logger that supports an `error(str)` method.

        """
        super().__init__(str(error_message))
        self.error_message: str = str(error_message)

        # Get traceback info from sys
        _, _, exc_tb = sys.exc_info()
        self.lineno: int | None = exc_tb.tb_lineno if exc_tb else None
        self.file_name: str | None = (
            exc_tb.tb_frame.f_code.co_filename if exc_tb else "Unknown"
        )

        # Log the formatted error
        logger.error(str(self))

    def __str__(self) -> str:
        """Return a formatted error message with file name and line number."""
        return (
            f"Error occurred in file [{self.file_name}], "
            f"line [{self.lineno}], "
            f"message: [{self.error_message}]"
        )

================================================================================
# FILE: src\networksecurity\inference\estimator.py
================================================================================

import joblib
import numpy as np
import pandas as pd
from pathlib import Path

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class NetworkModel:
    def __init__(self, preprocessor, model, y_preprocessor=None):
        self.preprocessor = preprocessor
        self.model = model
        self.y_preprocessor = y_preprocessor

    @classmethod
    def from_artifacts(cls, model_path: Path, x_preprocessor_path: Path, y_preprocessor_path: Path = None):
        try:
            model = joblib.load(model_path)
            x_preprocessor = joblib.load(x_preprocessor_path)
            y_preprocessor = joblib.load(y_preprocessor_path) if y_preprocessor_path else None
            logger.info("Loaded NetworkModel components from disk.")
            return cls(x_preprocessor, model, y_preprocessor)
        except Exception as e:
            logger.error("Failed to load model artifacts", exc_info=True)
            raise NetworkSecurityError(e, logger) from e

    def predict(self, x: pd.DataFrame | dict) -> np.ndarray:
        try:
            if isinstance(x, dict):
                x = pd.DataFrame([x])
            elif not isinstance(x, pd.DataFrame):
                raise TypeError("Input must be a dict or pandas DataFrame.")

            logger.info(f"Input shape for prediction: {x.shape}")

            x_transformed = self.preprocessor.transform(x)
            predictions = self.model.predict(x_transformed)

            if self.y_preprocessor:
                predictions = self.y_preprocessor.inverse_transform(predictions)

            logger.info(f"Prediction output: {predictions}")
            return predictions

        except Exception as e:
            logger.error("Prediction failed", exc_info=True)
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\logging\__init__.py
================================================================================

"""
Initialize centralized logger for the `networksecurity.logging` package.

This module sets up a reusable logger instance (`logger`) that can be imported
across the project to ensure consistent, centralized logging configuration.

Supports:
- Shared UTC timestamp for file/folder naming
- Dynamic logger name via environment variable
- Dynamic log level via environment variable
"""

import os
import logging

from .logger import setup_logger

# Use env variable for logger name, default to "networksecurity"
LOGGER_NAME = os.getenv("LOGGER_NAME", "networksecurity")

# Use env variable for log level, default to "DEBUG"
LOG_LEVEL = os.getenv("LOG_LEVEL", "DEBUG").upper()

# Initialize and configure the logger
logger = setup_logger(name=LOGGER_NAME)
logger.setLevel(getattr(logging, LOG_LEVEL, logging.DEBUG))

================================================================================
# FILE: src\networksecurity\logging\logger.py
================================================================================

"""
Logging utility module.

Provides `setup_logger()` to configure a logger with both a file and stream handler,
using a UTC timestamp synchronized across the pipeline (logs, artifacts, models, etc.).
"""

import logging
import sys
from pathlib import Path

from src.networksecurity.constants.constants import LOGS_ROOT
from src.networksecurity.utils.timestamp import get_shared_utc_timestamp

def setup_logger(name: str = "app_logger") -> logging.Logger:
    """
    Set up and return a logger instance with a consistent timestamped log directory and file.

    Ensures:
    - One timestamp per pipeline run (shared with ConfigurationManager)
    - No duplicate handlers on repeated setup
    - Clean log formatting to stdout and file

    Args:
        name (str): The name of the logger instance (e.g., 'data_ingestion').

    Returns:
        logging.Logger: Configured logger with file and stream handlers.
    """
    sys.stdout.reconfigure(encoding='utf-8')

    # Get shared timestamp for this run
    timestamp = get_shared_utc_timestamp()

    # Log folder: logs/<timestamp>/
    log_dir = Path(LOGS_ROOT) / timestamp
    log_dir.mkdir(parents=True, exist_ok=True)

    # Log file: logs/<timestamp>/<timestamp>.log
    log_filepath = log_dir / f"{timestamp}.log"

    # Log message format
    log_format = "[%(asctime)s] - %(levelname)s - %(module)s - %(message)s"
    formatter = logging.Formatter(log_format)

    # Get or create logger
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)

    # Add file handler if not already added
    if not any(isinstance(h, logging.FileHandler) and h.baseFilename == str(log_filepath)
               for h in logger.handlers):
        file_handler = logging.FileHandler(log_filepath, mode="a")
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    # Add stdout stream handler if not already added
    if not any(isinstance(h, logging.StreamHandler) and h.stream == sys.stdout
               for h in logger.handlers):
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)

    return logger


# Example usage
if __name__ == "__main__":
    logger = setup_logger()
    logger.info("Logger initialized successfully.")

================================================================================
# FILE: src\networksecurity\pipeline\__init__.py
================================================================================



================================================================================
# FILE: src\networksecurity\pipeline\data_ingestion_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_ingestion import DataIngestion
from src.networksecurity.dbhandler.mongodb_handler import MongoDBHandler
from src.networksecurity.entity.config_entity import DataIngestionConfig, MongoHandlerConfig
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataIngestionPipeline:
    """
    Runs the data ingestion stage:
    - Loads configs
    - Instantiates handler and component
    - Triggers ingestion and returns artifact
    """

    def __init__(self):
        try:
            self.config_manager = ConfigurationManager()
            self.ingestion_config: DataIngestionConfig = self.config_manager.get_data_ingestion_config()
            self.mongo_config: MongoHandlerConfig = self.config_manager.get_mongo_handler_config()
            self.mongo_handler = MongoDBHandler(config=self.mongo_config)
        except Exception as e:
            logger.exception("Failed to initialize DataIngestionPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> DataIngestionArtifact:
        try:
            logger.info("========== Data Ingestion Stage Started ==========")

            ingestion = DataIngestion(
                config=self.ingestion_config,
                db_handler=self.mongo_handler,
            )
            artifact = ingestion.run_ingestion()

            logger.info(f"Data Ingestion Process Completed.\n{artifact}")
            logger.info("========== Data Ingestion Stage Completed ==========")

            return artifact

        except Exception as e:
            logger.exception("Data Ingestion Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\pipeline\data_transformation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_transformation import DataTransformation
from src.networksecurity.entity.artifact_entity import (
    DataValidationArtifact,
    DataTransformationArtifact,
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataTransformationPipeline:
    """
    Orchestrates the Data Transformation stage of the pipeline.

    Responsibilities:
    - Loads transformation configuration
    - Accepts validated artifact
    - Performs feature transformation and returns transformation artifact
    """

    def __init__(self, validation_artifact: DataValidationArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.config = self.config_manager.get_data_transformation_config()
            self.validation_artifact = validation_artifact
        except Exception as e:
            logger.exception("Failed to initialize DataTransformationPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> DataTransformationArtifact:
        try:
            logger.info("========== Data Transformation Stage Started ==========")

            transformer = DataTransformation(
                config=self.config,
                validation_artifact=self.validation_artifact,
            )
            transformation_artifact = transformer.run_transformation()

            logger.info(f"Data Transformation Completed Successfully: {transformation_artifact}")
            logger.info("========== Data Transformation Stage Completed ==========")

            return transformation_artifact

        except Exception as e:
            logger.exception("Data Transformation Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\pipeline\data_validation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_validation import DataValidation
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataValidationPipeline:
    """
    Orchestrates the Data Validation stage of the pipeline.

    Responsibilities:
    - Fetches the configuration and artifacts
    - Loads validated DataFrame from artifact
    - Validates schema and schema hash
    """

    def __init__(self, ingestion_artifact: DataIngestionArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.ingestion_artifact = ingestion_artifact
            self.config = self.config_manager.get_data_validation_config()
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run(self):
        try:
            logger.info("========= Data Validation Stage Started =========")
            validation = DataValidation(config=self.config, ingestion_artifact=self.ingestion_artifact)
            validation_artifact = validation.run_validation()
            logger.info(f"Data Validation Process Completed.\n{validation_artifact}")
            logger.info("========= Data Validation Stage Completed =========")
            return validation_artifact
        except Exception as e:
            logger.error("Data Validation Pipeline Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\pipeline\model_trainer_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.model_trainer import ModelTrainer
from src.networksecurity.entity.artifact_entity import (
    DataTransformationArtifact,
    ModelTrainerArtifact
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class ModelTrainerPipeline:
    """
    Orchestrates the Model Training stage of the pipeline.

    Responsibilities:
    - Loads model‐trainer configuration
    - Accepts transformation artifact
    - Trains (and tunes) candidate models
    - Logs & registers via MLflow
    - Emits a ModelTrainerArtifact
    """

    def __init__(self, transformation_artifact: DataTransformationArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.config = self.config_manager.get_model_trainer_config()
            self.transformation_artifact = transformation_artifact
        except Exception as e:
            logger.exception("Failed to initialize ModelTrainerPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> ModelTrainerArtifact:
        try:
            logger.info("========== Model Training Stage Started ==========")

            trainer = ModelTrainer(
                config=self.config,
                transformation_artifact=self.transformation_artifact
            )
            trainer_artifact = trainer.run_training()

            logger.info(f"Model Training Completed Successfully: {trainer_artifact}")
            logger.info("========== Model Training Stage Completed ==========")

            return trainer_artifact

        except Exception as e:
            logger.exception("Model Training Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\utils\__init__.py
================================================================================



================================================================================
# FILE: src\networksecurity\utils\core.py
================================================================================

import json
import joblib
import pandas as pd
import numpy as np
import yaml
from pathlib import Path
from box import ConfigBox
from box.exceptions import BoxKeyError, BoxTypeError, BoxValueError
from urllib.parse import quote_plus
from ensure import ensure_annotations

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


@ensure_annotations
def read_yaml(path_to_yaml: Path) -> ConfigBox:
    """
    Load a YAML file as a ConfigBox, always using UTF-8.
    """
    if not path_to_yaml.exists():
        msg = f"YAML file not found: '{path_to_yaml}'"
        logger.error(msg)
        raise NetworkSecurityError(FileNotFoundError(msg), logger)

    try:
        # Explicitly open as UTF-8 to avoid Windows cp1252 issues
        with path_to_yaml.open("r", encoding="utf-8") as f:
            content = yaml.safe_load(f)
    except (BoxValueError, BoxTypeError, BoxKeyError, yaml.YAMLError) as e:
        logger.error(f"Failed to load YAML from {path_to_yaml.as_posix()}: {e}")
        raise NetworkSecurityError(e, logger) from e
    except Exception as e:
        logger.error(f"Unexpected error reading YAML file: {e}")
        raise NetworkSecurityError(e, logger) from e

    if content is None:
        msg = "YAML file is empty or improperly formatted."
        logger.error(msg)
        raise NetworkSecurityError(ValueError(msg), logger)

    logger.info(f"YAML loaded successfully from: '{path_to_yaml.as_posix()}'")
    return ConfigBox(content)


@ensure_annotations
def csv_to_json_convertor(source_filepath: Path, destination_filepath: Path):
    """
    Convert a CSV file to a list of JSON records and optionally save it.
    """
    try:
        if not source_filepath.exists():
            raise FileNotFoundError(f"CSV source file not found: '{source_filepath.as_posix()}'")

        df = pd.read_csv(source_filepath).reset_index(drop=True)
        records = df.to_dict(orient="records")

        parent_dir = destination_filepath.parent
        if not parent_dir.exists():
            parent_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Created directory for JSON output: '{parent_dir.as_posix()}'")
        else:
            logger.info(f"Directory already exists for JSON output: '{parent_dir.as_posix()}'")

        with destination_filepath.open("w", encoding="utf-8") as f:
            json.dump(records, f, indent=4)

        logger.info(f"CSV converted and saved to: '{destination_filepath.as_posix()}'")
        return records

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def replace_username_password_in_uri(base_uri: str, username: str, password: str) -> str:
    if not all([base_uri, username, password]):
        raise ValueError("base_uri, username, and password must all be provided.")

    encoded_username = quote_plus(username)
    encoded_password = quote_plus(password)

    return (
        base_uri
        .replace("<username>", encoded_username)
        .replace("<password>", encoded_password)
    )


@ensure_annotations
def save_to_yaml(data: dict, *paths: Path, label: str):
    """
    Write a dict out to YAML, always using UTF-8.
    """
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            # Write UTF-8
            with open(path, "w", encoding="utf-8") as file:
                yaml.dump(data, file, sort_keys=False)

            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def save_to_csv(df: pd.DataFrame, *paths: Path, label: str):
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            df.to_csv(path, index=False)
            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def save_array(array: np.ndarray | pd.Series, *paths: Path, label: str):
    """
    Saves a NumPy array or pandas Series to the specified paths in `.npy` format.

    Args:
        array (Union[np.ndarray, pd.Series]): Data to save.
        *paths (Path): One or more file paths.
        label (str): Label for logging.
    """
    try:
        # Convert Series to ndarray if needed
        array = np.asarray(array)

        for path in paths:
            path = Path(path)

            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            # # Ensure file ends with `.npy`
            # if path.suffix != ".npy":
            #     path = path.with_suffix(".npy")

            np.save(path, array)
            logger.info(f"{label} saved to: '{path.as_posix()}'")

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def save_to_json(data: dict, *paths: Path, label: str):
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            with open(path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=4)

            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def read_csv(path: Path, label: str) -> pd.DataFrame:
    try:
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"{label} not found at path: {path.as_posix()}")

        df = pd.read_csv(path)
        logger.info(f"{label} loaded from: '{path.as_posix()}'")
        return df

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def save_object(obj: object, path: Path, label: str):
    try:
        path = Path(path)
        if not path.parent.exists():
            path.parent.mkdir(parents=True, exist_ok=True)
            logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
        else:
            logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

        joblib.dump(obj, path)
        logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

================================================================================
# FILE: src\networksecurity\utils\timestamp.py
================================================================================

from datetime import datetime, timezone

# Internal module-level cache to ensure the timestamp is consistent across the entire pipeline
_timestamp_cache: str = None

def get_shared_utc_timestamp(fmt: str = "%Y_%m_%dT%H_%M_%SZ") -> str:
    """
    Returns a consistent, cached UTC timestamp for the current pipeline run.

    This function guarantees that:
    - The timestamp is generated only once.
    - All modules (logger, config, etc.) use the exact same value.
    - The format is clean and safe for filenames, directories, and versioning.

    Args:
        fmt (str): Optional datetime format string.
                   Default: 'YYYY_MM_DDTHH_MM_SSZ' (e.g., '2025_04_16T17_45_02Z')

    Returns:
        str: UTC timestamp string formatted per the provided format.
    """
    global _timestamp_cache
    if _timestamp_cache is None:
        _timestamp_cache = datetime.now(timezone.utc).strftime(fmt)
    return _timestamp_cache


--- YAML CONFIG FILES DUMP ---


================================================================================
# YAML FILE: config\config.yaml
================================================================================

# Root directory for all experiment artifacts
project:
  artifacts_root: artifacts

# MongoDB configuration
mongo_handler:
  input_data_path: network_data/input_csv/phisingData.csv
  json_data_filename: input_data.json
  database_name: network_security_db
  collection_name: phishing_records

# Data ingestion configuration
data_ingestion:
  raw_data_filename: raw_data.csv
  ingested_data_filename: ingested_data.csv

# Data validation configuration
data_validation:
  validated_filename: validated_data.csv
  missing_report_filename: missing_values_report.json
  duplicates_report_filename: duplicates_report.yaml
  drift_report_filename: drift_report.yaml
  validation_report_filename: validation_report.yaml
  schema_hash_filename: schema_hash.json


# Data transformation configuration
data_transformation:
  x_train_filename: x_train.npy
  y_train_filename: y_train.npy
  x_val_filename: x_val.npy
  y_val_filename: y_val.npy
  x_test_filename: x_test.npy
  y_test_filename: y_test.npy
  x_preprocessor_filename: x_preprocessor.joblib
  y_preprocessor_filename: y_preprocessor.joblib
  

# Model trainer configuration
model_trainer:
  root_dir: artifacts/model_trainer
  model_dir: saved_models
  trained_model_filename: model.joblib
  # where to dump a short training report
  training_report_filename: training_report.yaml
  

# Model evaluation configuration
model_evaluation:
  evaluation_report_filename: evaluation_report.yaml

# Model prediction configuration
model_prediction:
  prediction_output_filename: prediction_output.csv

# Stable DVC-tracked data paths
data_paths:
  raw_data_dvc_filepath: data/raw/raw_data.csv
  validated_dvc_filepath: data/validated/validated_data.csv
  train_dvc_dir: data/transformed/train
  val_dvc_dir: data/transformed/val
  test_dvc_dir: data/transformed/test

================================================================================
# YAML FILE: config\params.yaml
================================================================================

# Parameters for drift detection during data validation
validation_params:
  drift_detection:
    enabled: true
    method: ks_test
    p_value_threshold: 0.05

  schema_check:
    enabled: true
    method: hash


# Parameters for data transformation
transformation_params:
  # Parameters for data splitting
  data_split:
    train_size: 0.6
    test_size: 0.2
    val_size: 0.2
    random_state: 42
    stratify: true

  steps:
    x:
      imputer: knn
    y:
      label_mapping: label_mapper

  methods:
    x:
      imputer:
        missing_values: null
        n_neighbors: 3
        weights: uniform
    y:
      label_mapping:
        from: -1
        to: 0




# ───────────────────────────────────────────────────────────────────────────────
# Parameters for model training
# ───────────────────────────────────────────────────────────────────────────────
# Parameters for model training
model_trainer:


  # candidate estimators (importable by sklearn convention)
  models:
    - name: sklearn.ensemble.RandomForestClassifier
      # default params (used if optimization.enabled = false)
      params:
        n_estimators: 100
        max_depth: 10
        random_state: 42
      # search-space for Optuna
      search_space:
        n_estimators:
          distribution: int
          low: 50
          high: 300
          step: 10
        max_depth:
          distribution: int
          low: 5
          high: 50
          step: 1

    - name: sklearn.ensemble.GradientBoostingClassifier
      params:
        n_estimators: 100
        learning_rate: 0.1
        max_depth: 3
        random_state: 42
      search_space:
        n_estimators:
          distribution: int
          low: 50
          high: 200
          step: 10
        learning_rate:
          distribution: float
          low: 0.01
          high: 1.0
          log: true
        max_depth:
          distribution: int
          low: 2
          high: 10

  # hyperparameter optimization settings
  optimization:
    enabled: true
    method: optuna
    n_trials: 30
    direction: maximize
    cv_folds: 5
    scoring: accuracy

  # MLflow tracking & registry settings (URI is now picked from ENV)
  tracking:
    mlflow:
      enabled: true
      experiment_name: NetworkSecurityExperiment
      registry_model_name: NetworkSecurityModel
      metrics_to_log:
        - accuracy
        - f1
        - precision
        - recall
      # new switch: whether to log each Optuna trial as its own MLflow run
      log_trials: false

================================================================================
# YAML FILE: config\schema.yaml
================================================================================

columns:
  Abnormal_URL: int64
  DNSRecord: int64
  Domain_registeration_length: int64
  Favicon: int64
  Google_Index: int64
  HTTPS_token: int64
  Iframe: int64
  Links_in_tags: int64
  Links_pointing_to_page: int64
  Page_Rank: int64
  Prefix_Suffix: int64
  Redirect: int64
  Request_URL: int64
  Result: int64
  RightClick: int64
  SFH: int64
  SSLfinal_State: int64
  Shortining_Service: int64
  Statistical_report: int64
  Submitting_to_email: int64
  URL_Length: int64
  URL_of_Anchor: int64
  age_of_domain: int64
  double_slash_redirecting: int64
  having_At_Symbol: int64
  having_IP_Address: int64
  having_Sub_Domain: int64
  on_mouseover: int64
  popUpWidnow: int64
  port: int64
  web_traffic: int64

target_column: Result

target_labels:
  0: phishing
  1: legitimate

================================================================================
# YAML FILE: research\config\schema.yaml
================================================================================

columns:
  Abnormal_URL: int64
  DNSRecord: int64
  Domain_registeration_length: int64
  Favicon: int64
  Google_Index: int64
  HTTPS_token: int64
  Iframe: int64
  Links_in_tags: int64
  Links_pointing_to_page: int64
  Page_Rank: int64
  Prefix_Suffix: int64
  Redirect: int64
  Request_URL: int64
  Result: int64
  RightClick: int64
  SFH: int64
  SSLfinal_State: int64
  Shortining_Service: int64
  Statistical_report: int64
  Submitting_to_email: int64
  URL_Length: int64
  URL_of_Anchor: int64
  age_of_domain: int64
  double_slash_redirecting: int64
  having_At_Symbol: int64
  having_IP_Address: int64
  having_Sub_Domain: int64
  on_mouseover: int64
  popUpWidnow: int64
  port: int64
  web_traffic: int64
