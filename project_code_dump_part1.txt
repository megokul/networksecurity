
📦 Project Structure of: E:\MyProjects\networksecurity

📁 .dvc/
    📄 .gitignore
    📁 cache/
        📁 files/
            📁 md5/
                📁 68/
                    📄 770e0efceb9cd3777502f505ecbec4
                📁 70/
                    📄 3506878baa6bf0360b2b42782e9677
                📁 83/
                    📄 de4e3a0dbe7d93403a8c15753377f7
                📁 86/
                    📄 4e5c8474fcd1b15bc434932036628d
                📁 87/
                    📄 f220430be73d16609bfd637b1644af
                📁 8d/
                    📄 e71e17a20f71bece7659a34bae7027
                📁 bf/
                    📄 fa4350e8aa16caaa3cbb92ec8e048c
                📁 e1/
                    📄 a8ae356f072436dd3acc1bcbafbd89
    📄 config
    📁 tmp/
        📄 btime
        📄 dag.md
        📁 exps/
            📁 cache/
                📁 08/
                    📄 ba3af380fc7743bb7784b0b99b9e84b597659c
                📁 2e/
                    📄 d0f21b70c06ff15b188bb69f6ce389a869af78
                📁 32/
                    📄 9b746e5971f793e269ced861c2d1e401f43d48
                📁 3e/
                    📄 a84c7b28b0242174e45334d5d6a9d3026f546d
                📁 48/
                    📄 faf510268751ce441d9178a563f5e2c4577d88
                📁 4a/
                    📄 640ce33b51ffeb8c1aa85e520ce78dfe105123
                📁 61/
                    📄 a81d71c590a7a662176a82aa5f00a7532a3e16
                📁 8a/
                    📄 ed08439cb37bbee9fee4ad00d82ea30435606b
                📁 a8/
                    📄 2c1cb5bbc048ca32cb78061a4b8aaf3735233f
                📁 b0/
                    📄 63569211a95f11f23c4f61f6b9518bbf3b2cf3
                📁 bb/
                    📄 6504b5b79c287b4b60d2355251d8b30025c769
                📁 c4/
                    📄 9033287ca54d77ad6ab9c7603c06825bbac4e6
                📁 c7/
                    📄 31a093cb23b7a2bef65922d39d9d20ac56e6f3
                📁 e3/
                    📄 5de325a2125360b5ae289c080fc748a0140cdc
                📁 eb/
                    📄 79e8cfe8c4a974d876650c1a48de56761ef2a1
                📁 f9/
                    📄 21cb7aa3662a13f16a67e07a71781cec9b1a91
            📁 celery/
                📁 broker/
                    📁 control/
                    📁 in/
                    📁 processed/
                📁 result/
        📄 lock
        📄 rwlock
        📄 rwlock.lock
📄 .dvcignore
📄 .env
📄 .gitignore
📄 Dockerfile
📄 LICENSE
📁 NetworkSecurity.egg-info/
    📄 PKG-INFO
    📄 SOURCES.txt
    📄 dependency_links.txt
    📄 requires.txt
    📄 top_level.txt
📄 README.md
📄 app.py
📁 artifacts/
    📁 2025_05_01T19_59_20Z/
        📁 data_ingestion/
            📁 featurestore/
                📄 raw_data.csv
            📁 ingested/
                📄 ingested_data.csv
        📁 data_transformation/
            📁 preprocessor/
                📄 x_preprocessor.joblib
                📄 y_preprocessor.joblib
            📁 transformed/
                📁 test/
                    📄 x_test.npy
                    📄 y_test.npy
                📁 train/
                    📄 x_train.npy
                    📄 y_train.npy
                📁 val/
                    📄 x_val.npy
                    📄 y_val.npy
        📁 data_validation/
            📁 reports/
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.json
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
        📁 model_evaluation/
            📄 evaluation_report.yaml
        📁 model_trainer/
            📁 inference_model/
                📄 inference_model.joblib
            📁 reports/
                📄 training_report.yaml
            📁 trained_model/
                📄 model.joblib
    📁 2025_05_01T20_31_11Z/
        📁 data_ingestion/
            📁 featurestore/
                📄 raw_data.csv
            📁 ingested/
                📄 ingested_data.csv
        📁 data_transformation/
            📁 preprocessor/
                📄 x_preprocessor.joblib
                📄 y_preprocessor.joblib
            📁 transformed/
                📁 test/
                    📄 x_test.npy
                    📄 y_test.npy
                📁 train/
                    📄 x_train.npy
                    📄 y_train.npy
                📁 val/
                    📄 x_val.npy
                    📄 y_val.npy
        📁 data_validation/
            📁 reports/
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.json
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
        📁 model_evaluation/
            📄 evaluation_report.yaml
        📁 model_trainer/
            📁 inference_model/
                📄 inference_model.joblib
            📁 reports/
                📄 training_report.yaml
            📁 trained_model/
                📄 model.joblib
📁 config/
    📄 config.yaml
    📄 params.yaml
    📄 schema.yaml
    📄 templates.yaml
📁 data/
    📁 raw/
        📄 raw_data.csv
        📄 raw_data.csv.dvc
    📁 transformed/
        📁 test/
            📄 x_test.npy
            📄 x_test.npy.dvc
            📄 y_test.npy
            📄 y_test.npy.dvc
        📁 train/
            📄 x_train.npy
            📄 x_train.npy.dvc
            📄 y_train.npy
            📄 y_train.npy.dvc
        📁 val/
            📄 x_val.npy
            📄 x_val.npy.dvc
            📄 y_val.npy
            📄 y_val.npy.dvc
    📁 validated/
        📄 validated_data.csv
        📄 validated_data.csv.dvc
📄 debug.py
📄 debug_pipeline.py
📁 final_model/
    📄 final_inference_model.joblib
📁 logs/
    📁 2025_05_01T19_59_20Z/
        📄 2025_05_01T19_59_20Z.log
    📁 2025_05_01T20_31_11Z/
        📄 2025_05_01T20_31_11Z.log
📄 main.py
📁 network_data/
    📁 input_csv/
        📄 phisingData.csv
📄 notes.txt
📄 print_structure.py
📄 project_dump.py
📄 project_template.py
📄 prompt.txt
📄 requirements.txt
📁 research/
    📁 config/
        📄 schema.yaml
    📄 ingested_data.csv
    📁 logs/
        📁 2025_04_10T16_40_06/
            📄 2025_04_10T16_40_06.log
        📁 2025_04_10T17_30_35/
            📄 2025_04_10T17_30_35.log
        📁 2025_04_10T17_38_55/
            📄 2025_04_10T17_38_55.log
    📄 research.ipynb
📄 setup.py
📁 src/
    📁 networksecurity/
        📄 __init__.py
        📁 cloud/
            📄 __init__.py
            📄 s3_syncer.py
        📁 components/
            📄 __init__.py
            📄 data_ingestion.py
            📄 data_transformation.py
            📄 data_validation.py
            📄 model_evaluation.py
            📄 model_pusher.py
            📄 model_trainer.py
        📁 config/
            📄 __init__.py
            📄 configuration.py
        📁 constants/
            📄 __init__.py
            📄 constants.py
        📁 data_processors/
            📄 encoder_factory.py
            📄 imputer_factory.py
            📄 label_mapper.py
            📄 preprocessor_builder.py
            📄 scaler_factory.py
        📁 dbhandler/
            📄 __init__.py
            📄 base_handler.py
            📄 mongodb_handler.py
        📁 entity/
            📄 __init__.py
            📄 artifact_entity.py
            📄 config_entity.py
        📁 exception/
            📄 __init__.py
            📄 exception.py
        📁 inference/
            📄 estimator.py
        📁 logging/
            📄 __init__.py
            📄 logger.py
        📁 pipeline/
            📄 __init__.py
            📄 data_ingestion_pipeline.py
            📄 data_transformation_pipeline.py
            📄 data_validation_pipeline.py
            📄 model_evaluation_pipeline.py
            📄 model_pusher_pipeline.py
            📄 model_trainer_pipeline.py
            📄 training_pipeline.py
        📁 utils/
            📄 __init__.py
            📄 core.py
            📄 timestamp.py
        📁 worker/
            📄 celery_worker.py
📁 templates/
    📄 table.html

--- CODE DUMP | PART 1 of 3 ---


================================================================================
# PY FILE: app.py
================================================================================

import sys
import os

import certifi
ca = certifi.where()

from dotenv import load_dotenv
load_dotenv()
mongo_db_url = os.getenv("MONGODB_URL_KEY")
print(mongo_db_url)
import pymongo
from networksecurity.exception.exception import NetworkSecurityException
from networksecurity.logging.logger import logging
from networksecurity.pipeline.training_pipeline import TrainingPipeline

from fastapi.middleware.cors import CORSMiddleware
from fastapi import FastAPI, File, UploadFile,Request
from uvicorn import run as app_run
from fastapi.responses import Response
from starlette.responses import RedirectResponse
import pandas as pd

from networksecurity.utils.main_utils.utils import load_object

from networksecurity.utils.ml_utils.model.estimator import NetworkModel


client = pymongo.MongoClient(mongo_db_url, tlsCAFile=ca)

from networksecurity.constant.training_pipeline import DATA_INGESTION_COLLECTION_NAME
from networksecurity.constant.training_pipeline import DATA_INGESTION_DATABASE_NAME

database = client[DATA_INGESTION_DATABASE_NAME]
collection = database[DATA_INGESTION_COLLECTION_NAME]

app = FastAPI()
origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

from fastapi.templating import Jinja2Templates
templates = Jinja2Templates(directory="./templates")

@app.get("/", tags=["authentication"])
async def index():
    return RedirectResponse(url="/docs")

@app.get("/train")
async def train_route():
    try:
        train_pipeline=TrainingPipeline()
        train_pipeline.run_pipeline()
        return Response("Training is successful")
    except Exception as e:
        raise NetworkSecurityException(e,sys)
    
@app.post("/predict")
async def predict_route(request: Request,file: UploadFile = File(...)):
    try:
        df=pd.read_csv(file.file)
        #print(df)
        preprocesor=load_object("final_model/preprocessor.pkl")
        final_model=load_object("final_model/model.pkl")
        network_model = NetworkModel(preprocessor=preprocesor,model=final_model)
        print(df.iloc[0])
        y_pred = network_model.predict(df)
        print(y_pred)
        df['predicted_column'] = y_pred
        print(df['predicted_column'])
        #df['predicted_column'].replace(-1, 0)
        #return df.to_json()
        df.to_csv('prediction_output/output.csv')
        table_html = df.to_html(classes='table table-striped')
        #print(table_html)
        return templates.TemplateResponse("table.html", {"request": request, "table": table_html})
        
    except Exception as e:
            raise NetworkSecurityException(e,sys)

    
if __name__=="__main__":
    app_run(app,host="0.0.0.0",port=8000)

================================================================================
# PY FILE: debug.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.dbhandler.mongodb_handler import MongoDBHandler
from src.networksecurity.components.data_ingestion import DataIngestion
from dotenv import load_dotenv
load_dotenv()


configmanager = ConfigurationManager()

mongohandler_config = configmanager.get_mongo_handler_config()

mongohandler = MongoDBHandler(mongohandler_config)

with mongohandler:
    mongohandler.insert_csv_to_collection(mongohandler_config.input_data_path)

================================================================================
# PY FILE: debug_pipeline.py
================================================================================

# FILE: debug_pipeline.py

import sys
from src.networksecurity.pipeline.training_pipeline import TrainingPipeline
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


def run_debug_pipeline() -> None:
    """
    Trigger the full training pipeline in debug mode.
    Logs structured output and final artifact summary.
    """
    try:
        logger.info("\n DEBUG MODE: Starting the Training Pipeline...\n")

        pipeline = TrainingPipeline()
        final_artifact = pipeline.run_pipeline()

        logger.info("\n Final ModelPusherArtifact:")
        logger.info(final_artifact)

    except NetworkSecurityError as nse:
        logger.error("Training pipeline failed with a NetworkSecurityError.")
        logger.error(nse)

    except Exception as e:
        logger.exception("Unexpected error occurred during pipeline execution.")
        sys.exit(1)


if __name__ == "__main__":
    run_debug_pipeline()

================================================================================
# PY FILE: main.py
================================================================================



================================================================================
# PY FILE: print_structure.py
================================================================================

import os


def print_directory_tree(start_path: str, indent: str = "", exclude_dirs=None) -> None:
    """
    Recursively prints the directory structure starting from `start_path`,
    excluding directories listed in `exclude_dirs`.

    Args:
        start_path (str): The root folder path to start from.
        indent (str): Used for indentation in recursive calls.
        exclude_dirs (set): Directory names to ignore.
    """
    if exclude_dirs is None:
        exclude_dirs = {'.venv', 'venv', '__pycache__', '.github', '.git'}

    try:
        items = sorted(os.listdir(start_path))
    except PermissionError:
        return  # Skip directories/files we can't access

    for item in items:
        item_path = os.path.join(start_path, item)

        if os.path.isdir(item_path):
            if item in exclude_dirs:
                continue
            print(f"{indent}📁 {item}/")
            print_directory_tree(item_path, indent + "    ", exclude_dirs)
        else:
            print(f"{indent}📄 {item}")


if __name__ == "__main__":
    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
    print(f"\n📦 Project Structure of: {ROOT_DIR}\n")
    print_directory_tree(ROOT_DIR)

================================================================================
# PY FILE: project_dump.py
================================================================================

import os
import math

EXCLUDE_DIRS = {'.venv', 'venv', '__pycache__', '.github', '.git', '.idea', '.vscode', 'build', 'dist', '.mypy_cache'}
INCLUDE_YAML_FILES = {'config.yaml', 'params.yaml', 'schema.yaml'}
BASE_OUTPUT_FILE = "project_code_dump_part"
SUMMARY_FILE = "project_code_dump_index.txt"


def is_valid_directory(dirname):
    return not any(part in EXCLUDE_DIRS for part in dirname.split(os.sep))


def print_directory_tree(start_path: str, indent: str = "", exclude_dirs=None, out_lines=None) -> list:
    if exclude_dirs is None:
        exclude_dirs = EXCLUDE_DIRS
    if out_lines is None:
        out_lines = []

    try:
        items = sorted(os.listdir(start_path))
    except PermissionError:
        return out_lines

    for item in items:
        item_path = os.path.join(start_path, item)
        if os.path.isdir(item_path):
            if item in exclude_dirs:
                continue
            out_lines.append(f"{indent}📁 {item}/")
            print_directory_tree(item_path, indent + "    ", exclude_dirs, out_lines)
        else:
            out_lines.append(f"{indent}📄 {item}")
    return out_lines


def list_target_files(root_dir):
    py_files = []
    yaml_files = []

    for dirpath, dirnames, filenames in os.walk(root_dir):
        dirnames[:] = [d for d in dirnames if is_valid_directory(os.path.join(dirpath, d))]
        for filename in filenames:
            full_path = os.path.join(dirpath, filename)
            rel_path = os.path.relpath(full_path, root_dir)

            if filename.endswith('.py'):
                py_files.append((rel_path, full_path))
            elif filename in INCLUDE_YAML_FILES:
                yaml_files.append((rel_path, full_path))

    return sorted(py_files), sorted(yaml_files)


def chunk_list(data, num_chunks):
    chunk_size = math.ceil(len(data) / num_chunks)
    return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]


def dump_project_code_in_parts(root_dir='.', num_parts=1):
    py_files, yaml_files = list_target_files(root_dir)
    tree_lines = print_directory_tree(root_dir, out_lines=[])
    total_files = py_files + yaml_files

    if not total_files:
        print("❌ No .py or relevant .yaml files found.")
        return

    file_chunks = chunk_list(total_files, num_parts)

    summary_lines = []
    for i, chunk in enumerate(file_chunks, start=1):
        part_filename = f"{BASE_OUTPUT_FILE}{i}.txt"
        with open(part_filename, 'w', encoding='utf-8') as out_file:
            out_file.write(f"\n📦 Project Structure of: {os.path.abspath(root_dir)}\n\n")
            out_file.write("\n".join(tree_lines))
            out_file.write(f"\n\n--- CODE DUMP | PART {i} of {num_parts} ---\n\n")

            for rel_path, full_path in chunk:
                summary_lines.append(f"{part_filename}: {rel_path}")
                out_file.write(f"\n{'=' * 80}\n")
                file_type = "PY FILE" if rel_path.endswith('.py') else "YAML FILE"
                out_file.write(f"# {file_type}: {rel_path}\n")
                out_file.write(f"{'=' * 80}\n\n")
                try:
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        out_file.write(content.strip() + "\n")
                except Exception as e:
                    out_file.write(f"Error reading {rel_path}: {e}\n")

        print(f"✅ Dumped part {i} to: {os.path.abspath(part_filename)}")

    # Write summary file
    with open(SUMMARY_FILE, 'w', encoding='utf-8') as f:
        f.write("📄 File-to-Part Mapping\n\n")
        for line in summary_lines:
            f.write(line + "\n")

    print(f"\n📝 Summary index saved to: {os.path.abspath(SUMMARY_FILE)}")


if __name__ == "__main__":
    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
    try:
        num_parts = int(input("Enter number of parts to split the dump into: ").strip())
        if num_parts < 1:
            raise ValueError
    except ValueError:
        print("❌ Invalid input. Please enter a positive integer.")
    else:
        dump_project_code_in_parts(ROOT_DIR, num_parts)

================================================================================
# PY FILE: project_template.py
================================================================================

import os
from pathlib import Path
import logging


# ==============================
# 🔹 LOGGING SETUP
# ==============================

# ✅ Define the directory where logs will be stored
log_dir = "logs"

# ✅ Define the log file name and full path
log_filepath = os.path.join(log_dir, 'directorygen_logs.log')

# ✅ Define the format for log messages
log_format = '[%(asctime)s] - %(levelname)s - %(module)s - %(message)s'


def setup_logging():
    """
    Sets up a custom logger:
    - Creates the `logs/` directory if it doesn't exist.
    - Configures log messages to be written to both a file and the console.
    - Uses append mode (`"a"`) so logs persist across multiple runs.
    - Ensures handlers are not added multiple times.
    - Logger name: `directory_builder` (used for all logging in this script).

    Returns:
        logging.Logger: Custom logger instance.
    """

    # ✅ Ensure the log directory exists before creating the log file
    os.makedirs(log_dir, exist_ok=True)

    # ✅ Create a custom logger (separate from the root logger)
    logger = logging.getLogger('directory_builder')

    # ✅ Set the logger level to DEBUG (captures all log levels)
    logger.setLevel(logging.DEBUG)

    # ✅ Prevent adding duplicate handlers
    if not logger.hasHandlers():
        formatter = logging.Formatter(log_format)  # ✅ Define the log message format

        # ✅ Create a File Handler (logs INFO and above)
        file_handler = logging.FileHandler(log_filepath, mode='a')  # Append mode ("a")
        file_handler.setFormatter(formatter)  # Apply the log format

        # ✅ Create a Stream Handler (logs DEBUG and above to console)
        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(formatter)  # Apply the log format

        # ✅ Add handlers to the logger
        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)

    return logger  # ✅ Return the configured logger


# ==============================
# 🔹 PROJECT SETUP
# ==============================

# ✅ Define the project name (used in file paths)
project_name = input("Please enter the project name: ")

# ✅ List of files and directories to be created in the project structure
list_of_files = [
    # 🔹 GitHub workflows (for CI/CD setup)
    ".github/workflows/.gitkeep",
    
    # 🔹 Source Code Structure
    f"src/{project_name}/__init__.py",  # Main package initializer
    f"src/{project_name}/components/__init__.py",  # Components submodule initializer
    f"src/{project_name}/utils/__init__.py",  # Utilities submodule initializer
    f"src/{project_name}/utils/common.py",  # Common utility functions
    f"src/{project_name}/config/__init__.py",  # Configuration submodule
    f"src/{project_name}/config/configuration.py",  # Configuration handling script
    f"src/{project_name}/pipeline/__init__.py",  # Pipeline processing module
    f"src/{project_name}/entity/__init__.py",  # Entity-related module
    f"src/{project_name}/entity/config_entity.py",  # Configuration entity class
    f"src/{project_name}/constants/__init__.py",  # Constants module

    # 🔹 Configuration and Parameter Files
    "config/config.yaml",  # YAML file for configuration settings
    "params.yaml",  # YAML file for parameter tuning
    "schema.yaml",  # YAML file for data schema definition

    # 🔹 Project Execution and Deployment
    "main.py",  # Main entry point of the project
    "Dockerfile",  # Dockerfile for containerization
    "setup.py",  # Setup script for packaging
    "requirements.txt",  # Requirements file for Python dependencies

    # 🔹 Research and Web Components
    "research/research.ipynb",  # Jupyter notebook for exploratory research
    "templates/index.html",  # HTML template file (for a web component)

    # 🔹 Backend API
    "app.py"  # Flask or FastAPI backend application script
]


# ==============================
# 🔹 DIRECTORY & FILE CREATION
# ==============================

def create_file_structure(file_list):
    """
    Creates directories and files based on the given list.
    
    - If a directory does not exist, it is created.
    - If a file does not exist or is empty, it is created.
    - Logs every operation to track what is being created.

    Parameters:
        file_list (list): List of file paths to be created.
    """

    for filepath in file_list:
        filepath = Path(filepath)  # ✅ Convert string path to a `Path` object
        filedir, filename = os.path.split(filepath)  # ✅ Extract directory and filename separately

        # ✅ Ensure the parent directory exists before creating the file
        if filedir:
            os.makedirs(filedir, exist_ok=True)  # ✅ Create directory if it does not exist
            logger.info(f"Creating the directory '{filedir}' for file: '{filename}'")

        # ✅ Check if the file does not exist or is empty, then create it
        if not filepath.exists() or filepath.stat().st_size == 0:
            with open(filepath, 'w'):  # ✅ Create an empty file
                pass  # No content is added, just initializing the file
            logger.info(f"Creating empty file: '{filepath}'")  # ✅ Log file creation
        else:
            logger.info(f"'{filepath}' already exists")  # ✅ Log if the file already exists


# ✅ Initialize the logger
logger = setup_logging()

# ✅ Run the file creation function
create_file_structure(list_of_files)

================================================================================
# PY FILE: setup.py
================================================================================

from setuptools import find_packages, setup
from typing import List


def get_requirements() -> List[str]:
    requirement_lst: List[str] = []
    try:
        with open("requirements.txt", "r") as file:
            lines = file.readlines()
            for line in lines:
                requirement = line.strip()
                if requirement and requirement != "-e .":
                    requirement_lst.append(requirement)
    except FileExistsError:
        print("'requirements.txt' file not found")

    return requirement_lst


setup(
    name="NetworkSecurity",
    version="0.0.1",
    author="Gokul Krishna",
    author_email="iamgokul93@gmail.com",
    packages=find_packages(),
    install_requires=get_requirements(),
)

================================================================================
# PY FILE: src\networksecurity\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\cloud\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\cloud\s3_syncer.py
================================================================================

from pathlib import Path
import os
import boto3
from botocore.exceptions import ClientError

from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class S3Syncer:
    """
    Handles syncing of files and directories to AWS S3 with logging and error handling.
    """

    def __init__(self, bucket_name: str, region: str = "us-east-1") -> None:
        try:
            self.bucket_name = bucket_name
            self.s3_client = boto3.client("s3", region_name=region)
            logger.info(f"S3Syncer initialized for bucket '{bucket_name}' in region '{region}'")
        except Exception as e:
            logger.exception("S3 client initialization failed")
            raise NetworkSecurityError(e, logger) from e

    def upload_file(self, local_path: Path, s3_key: str) -> None:
        """
        Upload a single file to S3.
        """
        try:
            local_path = Path(local_path)
            if not local_path.is_file():
                raise FileNotFoundError(f"Local file not found: {local_path.as_posix()}")

            self.s3_client.upload_file(
                Filename=str(local_path),
                Bucket=self.bucket_name,
                Key=s3_key
            )
            logger.info(f"File uploaded to S3: {local_path.as_posix()} -> s3://{self.bucket_name}/{s3_key}")

        except ClientError as e:
            logger.error(f"AWS ClientError while uploading to S3: {e}")
            raise NetworkSecurityError(e, logger) from e
        except Exception as e:
            logger.error(f"Unexpected error while uploading file to S3: {e}")
            raise NetworkSecurityError(e, logger) from e

    def sync_directory(self, local_dir: Path, s3_prefix: str = "") -> None:
        """
        Recursively upload a local directory to the specified S3 prefix.
        """
        try:
            local_dir = Path(local_dir)
            if not local_dir.is_dir():
                raise NotADirectoryError(f"Local directory not found: {local_dir.as_posix()}")

            logger.info(f"Walking through directory: {local_dir.resolve().as_posix()}")
            logger.info(f"Target S3 prefix: {s3_prefix}")

            for root, _, files in os.walk(local_dir):
                for file in files:
                    local_file_path = Path(root) / file
                    relative_path = local_file_path.relative_to(local_dir)
                    s3_key = (Path(s3_prefix) / relative_path).as_posix()
                    logger.info(f"Uploading file: {local_file_path.as_posix()} -> s3://{self.bucket_name}/{s3_key}")
                    self.upload_file(local_file_path, s3_key)

            logger.info(f"Directory synced to S3: {local_dir.as_posix()} -> s3://{self.bucket_name}/{s3_prefix}")

        except Exception as e:
            logger.error("Failed to sync directory to S3")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\components\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\components\data_ingestion.py
================================================================================

import numpy as np
import pandas as pd

from src.networksecurity.entity.config_entity import DataIngestionConfig
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact
from src.networksecurity.dbhandler.base_handler import DBHandler
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from src.networksecurity.utils.core import save_to_csv, read_csv


class DataIngestion:
    def __init__(
        self,
        config: DataIngestionConfig,
        db_handler: DBHandler,
    ):
        try:
            self.config = config
            self.db_handler = db_handler
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def __fetch_raw_data(self) -> pd.DataFrame:
        try:
            with self.db_handler as handler:
                df = handler.load_from_source()
            logger.info(f"Fetched {len(df)} raw rows from data source.")
            return df
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def __clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        try:
            df_cleaned = df.drop(columns=["_id"], errors="ignore").copy()
            df_cleaned.replace({"na": np.nan}, inplace=True)
            logger.info("Raw DataFrame cleaned successfully.")
            return df_cleaned
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run_ingestion(self) -> DataIngestionArtifact:
        try:
            logger.info("========== Starting Data Ingestion ==========")

            # Step 1: Fetch raw data
            raw_df = self.__fetch_raw_data()

            # Step 2: Save raw data
            save_to_csv(
                raw_df,
                self.config.raw_data_filepath,
                self.config.raw_dvc_path,
                label="Raw Data"
            )

            # Step 3: Clean raw data
            cleaned_df = self.__clean_dataframe(raw_df)

            # Step 4: Save cleaned (ingested) data
            save_to_csv(
                cleaned_df,
                self.config.ingested_data_filepath,
                label="Cleaned (Ingested) Data"
            )

            logger.info("========== Data Ingestion Completed ==========")

            return DataIngestionArtifact(
                raw_artifact_path=self.config.raw_data_filepath,
                raw_dvc_path=self.config.raw_dvc_path,
                ingested_data_filepath=self.config.ingested_data_filepath,
            )

        except Exception as e:
            logger.error("Data ingestion failed.")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\components\data_transformation.py
================================================================================

# FILE: src/networksecurity/components/data_transformation.py

from pathlib import Path
from typing import Tuple
import pandas as pd
from sklearn.model_selection import train_test_split

from src.networksecurity.entity.config_entity import DataTransformationConfig
from src.networksecurity.entity.artifact_entity import DataValidationArtifact, DataTransformationArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.utils.core import read_csv, save_object, save_array
from src.networksecurity.data_processors.preprocessor_builder import PreprocessorBuilder
from src.networksecurity.constants.constants import (
    X_TRAIN_LABEL, Y_TRAIN_LABEL,
    X_VAL_LABEL, Y_VAL_LABEL,
    X_TEST_LABEL, Y_TEST_LABEL
)


class DataTransformation:
    def __init__(self, config: DataTransformationConfig, validation_artifact: DataValidationArtifact):
        try:
            self.config = config
            self.validation_artifact = validation_artifact
            self.df = read_csv(validation_artifact.validated_filepath, label="Validated Data")
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _split_features_and_target(self) -> Tuple[pd.DataFrame, pd.Series]:
        try:
            df = self.df.copy()
            X = df.drop(columns=[self.config.target_column])
            y = df[self.config.target_column]
            return X, y
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _split_data(self, X: pd.DataFrame, y: pd.Series) -> Tuple:
        try:
            split_params = self.config.transformation_params.data_split
            stratify = y if split_params.stratify else None

            X_train, X_temp, y_train, y_temp = train_test_split(
                X, y,
                train_size=split_params.train_size,
                stratify=stratify,
                random_state=split_params.random_state
            )

            relative_test_size = split_params.test_size / (split_params.test_size + split_params.val_size)
            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp,
                test_size=relative_test_size,
                stratify=y_temp if split_params.stratify else None,
                random_state=split_params.random_state
            )

            return X_train, X_val, X_test, y_train, y_val, y_test
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _save_datasets(self, X_train, X_val, X_test, y_train, y_val, y_test):
        try:
            save_array(X_train, self.config.x_train_filepath, self.config.x_train_dvc_filepath, label=X_TRAIN_LABEL)

            save_array(y_train, self.config.y_train_filepath, self.config.y_train_dvc_filepath, label=Y_TRAIN_LABEL)
            save_array(X_val, self.config.x_val_filepath, self.config.x_val_dvc_filepath, label=X_VAL_LABEL)
            save_array(y_val, self.config.y_val_filepath, self.config.y_val_dvc_filepath, label=Y_VAL_LABEL)
            save_array(X_test, self.config.x_test_filepath, self.config.x_test_dvc_filepath, label=X_TEST_LABEL)
            save_array(y_test, self.config.y_test_filepath, self.config.y_test_dvc_filepath, label=Y_TEST_LABEL)
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run_transformation(self) -> DataTransformationArtifact:
        try:
            logger.info("========== Starting Data Transformation ==========")

            # Step 1: Separate features and target
            X, y = self._split_features_and_target()

            # Step 2: Split data into train/val/test
            X_train, X_val, X_test, y_train, y_val, y_test = self._split_data(X, y)

            # Step 3: Build preprocessor pipelines for X and Y
            builder = PreprocessorBuilder(
                steps=self.config.transformation_params.steps,
                methods=self.config.transformation_params.methods,
            )
            x_processor, y_processor = builder.build()

            # Step 4: Fit and transform data
            X_train = x_processor.fit_transform(X_train)
            X_val = x_processor.transform(X_val)
            X_test = x_processor.transform(X_test)

            y_train = y_processor.fit_transform(y_train)
            y_val = y_processor.transform(y_val)
            y_test = y_processor.transform(y_test)

            # Step 5: Save X and Y processors
            save_object(x_processor, self.config.x_preprocessor_filepath, label="X Preprocessor Pipeline")
            save_object(y_processor, self.config.y_preprocessor_filepath, label="Y Preprocessor Pipeline")

            # Step 6: Save transformed datasets
            self._save_datasets(X_train, X_val, X_test, y_train, y_val, y_test)

            logger.info("========== Data Transformation Completed ==========")

            return DataTransformationArtifact(
                x_train_filepath=self.config.x_train_filepath,
                y_train_filepath=self.config.y_train_filepath,
                x_val_filepath=self.config.x_val_filepath,
                y_val_filepath=self.config.y_val_filepath,
                x_test_filepath=self.config.x_test_filepath,
                y_test_filepath=self.config.y_test_filepath,
                x_preprocessor_filepath=self.config.x_preprocessor_filepath,
                y_preprocessor_filepath=self.config.y_preprocessor_filepath ,
            )

        except Exception as e:
            logger.error("Data transformation failed.")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\components\data_validation.py
================================================================================

import hashlib
import pandas as pd
from pathlib import Path
from box import ConfigBox
from datetime import timezone
from scipy.stats import ks_2samp

from src.networksecurity.entity.config_entity import DataValidationConfig
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact, DataValidationArtifact
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from src.networksecurity.utils.core import save_to_yaml, save_to_csv, save_to_json, read_csv
from src.networksecurity.utils.timestamp import get_shared_utc_timestamp


class DataValidation:
    def __init__(self, config: DataValidationConfig, ingestion_artifact: DataIngestionArtifact):
        try:
            self.config = config
            self.schema = config.schema
            self.params = config.validation_params
            self.df = read_csv(ingestion_artifact.ingested_data_filepath, "Ingested Data")

            self.base_df = None
            self.drift_check_performed = False
            if self.params.drift_detection.enabled and config.validated_dvc_path.exists():
                self.base_df = read_csv(config.validated_dvc_path, "Validated Base Data")
                self.drift_check_performed = True

            self.validated_filepath = None
            self.timestamp = get_shared_utc_timestamp()

            self.report = ConfigBox(config.val_report_template.copy())
            self.critical_checks = ConfigBox({k: False for k in self.report.check_results.critical_checks.keys()})
            self.non_critical_checks = ConfigBox({k: False for k in self.report.check_results.non_critical_checks.keys()})
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _check_schema_hash(self):
        try:
            expected_schema = self.schema.columns
            expected_str = "|".join(f"{col}:{dtype}" for col, dtype in sorted(expected_schema.items()))
            expected_hash = hashlib.md5(expected_str.encode()).hexdigest()

            current_str = "|".join(f"{col}:{self.df[col].dtype}" for col in sorted(self.df.columns))
            current_hash = hashlib.md5(current_str.encode()).hexdigest()

            self.critical_checks.schema_is_match = (current_hash == expected_hash)
            logger.info("Schema hash check passed." if self.critical_checks.schema_is_match else "Schema hash mismatch.")
        except Exception as e:
            self.critical_checks.schema_is_match = False
            raise NetworkSecurityError(e, logger) from e

    def _check_structure_schema(self):
        try:
            expected = set(self.schema.columns.keys()) | {self.schema.target_column}
            actual = set(self.df.columns)
            self.critical_checks.schema_is_match = (expected == actual)
            if not self.critical_checks.schema_is_match:
                logger.error(f"Schema structure mismatch: expected={expected}, actual={actual}")
        except Exception as e:
            self.critical_checks.schema_is_match = False
            raise NetworkSecurityError(e, logger) from e

    def _check_missing_values(self):
        try:
            missing = self.df.isnull().sum().to_dict()
            missing["timestamp"] = self.timestamp
            save_to_yaml(missing, self.config.missing_report_filepath, label="Missing Value Report")
            self.non_critical_checks.no_missing_values = not any(v > 0 for v in missing.values() if isinstance(v, (int, float)))
        except Exception as e:
            self.non_critical_checks.no_missing_values = False
            raise NetworkSecurityError(e, logger) from e

    def _check_duplicates(self):
        try:
            before = len(self.df)
            self.df = self.df.drop_duplicates()
            after = len(self.df)
            duplicates_removed = before - after

            result = {
                "duplicate_rows_removed": duplicates_removed,
                "timestamp": self.timestamp
            }

            save_to_json(result, self.config.duplicates_report_filepath, label="Duplicates Report")
            self.non_critical_checks.no_duplicate_rows = (duplicates_removed == 0)
        except Exception as e:
            self.non_critical_checks.no_duplicate_rows = False
            raise NetworkSecurityError(e, logger) from e

    def _check_drift(self):
        try:
            if self.base_df is None:
                logger.info("Base dataset not found. Skipping drift check.")
                return

            drift_results = {}
            drift_detected = False

            for col in self.df.columns:
                if col not in self.base_df.columns:
                    continue
                _, p = ks_2samp(self.base_df[col], self.df[col])
                drift = bool(p < self.params.drift_detection.p_value_threshold)
                drift_results[col] = {"p_value": float(p), "drift": drift}
                if drift:
                    drift_detected = True

            drift_results["drift_detected"] = drift_detected
            drift_results["timestamp"] = self.timestamp

            save_to_yaml(drift_results, self.config.drift_report_filepath, label="Drift Result")
            self.critical_checks.no_data_drift = not drift_detected
        except Exception as e:
            self.critical_checks.no_data_drift = False
            raise NetworkSecurityError(e, logger) from e

    def _generate_report(self) -> dict:
        try:
            validation_status = all(self.critical_checks.values())
            non_critical_passed = all(self.non_critical_checks.values())

            self.report.timestamp = self.timestamp
            self.report.validation_status = validation_status
            self.report.critical_passed = validation_status
            self.report.non_critical_passed = non_critical_passed
            self.report.schema_check_type = self.params.schema_check.method

            if self.drift_check_performed:
                self.report.drift_check_method = self.params.drift_detection.method
            else:
                del self.report["drift_check_method"]

            self.report.check_results.critical_checks = self.critical_checks.to_dict()
            self.report.check_results.non_critical_checks = self.non_critical_checks.to_dict()

            return self.report.to_dict()
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run_validation(self) -> DataValidationArtifact:
        try:
            logger.info("Running data validation...")

            if self.params.schema_check.method == "hash":
                self._check_schema_hash()
            else:
                self._check_structure_schema()

            self._check_missing_values()
            self._check_duplicates()

            if self.params.drift_detection.enabled:
                self._check_drift()

            report_dict = self._generate_report()
            save_to_yaml(report_dict, self.config.validation_report_filepath, label="Validation Report")

            is_valid = all(self.critical_checks.values())
            validated_filepath = self.config.validated_filepath if is_valid else None

            if is_valid:
                save_to_csv(self.df, validated_filepath, self.config.validated_dvc_path, label="Validated Data")
            else:
                logger.warning("Validation failed. Validated data not saved.")

            return DataValidationArtifact(
                validated_filepath=validated_filepath,
                validation_status=is_valid
            )

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\components\model_evaluation.py
================================================================================

from pathlib import Path
from typing import Dict
import numpy as np

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from src.networksecurity.entity.config_entity import ModelEvaluationConfig
from src.networksecurity.entity.artifact_entity import ModelEvaluationArtifact, ModelTrainerArtifact
from src.networksecurity.utils.core import load_object, load_array, save_to_yaml
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from src.networksecurity.inference.estimator import NetworkModel


class ModelEvaluation:
    def __init__(self, config: ModelEvaluationConfig, trainer_artifact: ModelTrainerArtifact):
        try:
            self.config = config
            self.trainer_artifact = trainer_artifact

            logger.info(f"Initializing ModelEvaluator with root_dir={config.root_dir}")
            self.config.root_dir.mkdir(parents=True, exist_ok=True)

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _load_data(self):
        try:
            self.X_train = load_array(self.trainer_artifact.x_train_filepath, label="X Train")
            self.y_train = load_array(self.trainer_artifact.y_train_filepath, label="Y Train")
            self.X_val = load_array(self.trainer_artifact.x_val_filepath, label="X Val")
            self.y_val = load_array(self.trainer_artifact.y_val_filepath, label="Y Val")
            self.X_test = load_array(self.trainer_artifact.x_test_filepath, label="X Test")
            self.y_test = load_array(self.trainer_artifact.y_test_filepath, label="Y Test")

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _load_model(self):
        try:
            self.model: NetworkModel = load_object(self.trainer_artifact.trained_model_filepath)
            logger.info(f"Inference Model loaded successfully for evaluation.")
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _evaluate_split(self, X: np.ndarray, y_true: np.ndarray) -> Dict[str, float]:
        try:
            y_pred = self.model.predict(X)

            metrics = {
                "accuracy": accuracy_score(y_true, y_pred),
                "f1": f1_score(y_true, y_pred),
                "precision": precision_score(y_true, y_pred),
                "recall": recall_score(y_true, y_pred),
            }
            return metrics

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run_evaluation(self) -> ModelEvaluationArtifact:
        try:
            logger.info("========== Starting Model Evaluation ==========")

            self._load_data()
            self._load_model()

            evaluation_report = {
                "train_metrics": self._evaluate_split(self.X_train, self.y_train),
                "val_metrics": self._evaluate_split(self.X_val, self.y_val),
                "test_metrics": self._evaluate_split(self.X_test, self.y_test),
            }

            save_to_yaml(
                evaluation_report,
                self.config.evaluation_report_filepath,
                label="Evaluation Report"
            )

            logger.info("========== Model Evaluation Completed ==========")

            return ModelEvaluationArtifact(
                evaluation_report_filepath=self.config.evaluation_report_filepath
            )

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\components\model_pusher.py
================================================================================

from src.networksecurity.entity.config_entity import ModelPusherConfig
from src.networksecurity.entity.artifact_entity import ModelTrainerArtifact, ModelPusherArtifact
from src.networksecurity.cloud.s3_syncer import S3Syncer
from src.networksecurity.utils.core import save_object, load_object
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from pathlib import Path


class ModelPusher:
    """
    Pushes the final trained model to local storage and optionally to S3.
    """

    def __init__(self, model_pusher_config: ModelPusherConfig, model_trainer_artifact: ModelTrainerArtifact) -> None:
        try:
            self.config = model_pusher_config
            self.trainer_artifact = model_trainer_artifact
        except Exception as e:
            logger.exception("Failed to initialize ModelPusher")
            raise NetworkSecurityError(e, logger) from e

    def push_model(self) -> ModelPusherArtifact:
        try:
            logger.info("Starting model push process...")

            final_model = load_object(self.trainer_artifact.trained_model_filepath)

            # Save model locally
            save_object(
                obj=final_model,
                path=self.config.pushed_model_filepath,
                label="Final Model"
            )

            s3_syncer = S3Syncer(
                bucket_name=self.config.final_model_s3_bucket,
                region=self.config.aws_region
            )

            if self.config.upload_to_s3:
                # Upload only the final model file
                s3_syncer.upload_file(
                    local_path=self.config.pushed_model_filepath,
                    s3_key=self.config.s3_key_final_model,
                )

                # Upload entire artifacts folder
                s3_syncer.sync_directory(
                    local_dir=Path("artifacts"),
                    s3_prefix=f"{self.config.s3_artifacts_folder}/artifacts"
                )

                # Upload entire logs folder
                s3_syncer.sync_directory(
                    local_dir=Path("logs"),
                    s3_prefix=f"{self.config.s3_artifacts_folder}/logs"
                )

            logger.info("Model push process completed successfully.")
            return ModelPusherArtifact(
                pushed_model_local_path=self.config.pushed_model_filepath,
                pushed_model_s3_path=f"s3://{self.config.final_model_s3_bucket}/{self.config.s3_key_final_model}",
            )

        except Exception as e:
            logger.exception("Model push failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\components\model_trainer.py
================================================================================

from pathlib import Path
from datetime import datetime, timezone
import os
import importlib
import numpy as np
import optuna
import mlflow
import dagshub
import joblib
from sklearn.model_selection import cross_val_score
from sklearn.metrics import get_scorer
from mlflow import sklearn as mlflow_sklearn

from src.networksecurity.entity.config_entity import ModelTrainerConfig
from src.networksecurity.entity.artifact_entity import DataTransformationArtifact, ModelTrainerArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.utils.core import save_to_yaml, save_object, load_array
from src.networksecurity.inference.estimator import NetworkModel


class ModelTrainer:
    def __init__(self, config: ModelTrainerConfig, transformation_artifact: DataTransformationArtifact):
        try:
            self.config = config
            self.transformation_artifact = transformation_artifact
            logger.info(f"Initializing ModelTrainer with root_dir={config.root_dir}")
            config.root_dir.mkdir(parents=True, exist_ok=True)

            if config.tracking.mlflow.enabled:
                dagshub.init(
                    repo_owner=os.getenv("DAGSHUB_REPO_OWNER"),
                    repo_name=os.getenv("DAGSHUB_REPO_NAME"),
                    mlflow=True,
                )
                mlflow.set_tracking_uri(config.tracking.tracking_uri)
                mlflow.set_experiment(config.tracking.mlflow.experiment_name)
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _load_data(self):
        try:
            self.X_train = load_array(self.transformation_artifact.x_train_filepath, "X train")
            self.y_train = load_array(self.transformation_artifact.y_train_filepath, "Y train")
            self.X_val = load_array(self.transformation_artifact.x_val_filepath, "X val")
            self.y_val = load_array(self.transformation_artifact.y_val_filepath, "Y val")
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def _instantiate(self, full_class_string: str, params: dict):
        module_path, class_name = full_class_string.rsplit(".", 1)
        module = importlib.import_module(module_path)
        return getattr(module, class_name)(**(params or {}))

    def _optimize_one(self, model_spec: dict):
        model_name = model_spec["name"]
        search_space = model_spec.get("search_space", {})

        def objective(trial):
            sampled = {}
            for name, space in search_space.items():
                if "choices" in space:
                    sampled[name] = trial.suggest_categorical(name, space["choices"])
                else:
                    low, high = space["low"], space["high"]
                    step = space.get("step", 1)
                    log = space.get("log", False)
                    if isinstance(low, int) and isinstance(high, int):
                        sampled[name] = trial.suggest_int(name, low, high, step=step)
                    else:
                        sampled[name] = trial.suggest_float(name, float(low), float(high), log=log)
            clf = self._instantiate(model_name, sampled)
            scores = cross_val_score(clf, self.X_train, self.y_train, cv=self.config.optimization.cv_folds,
                                     scoring=self.config.optimization.scoring, n_jobs=-1)
            return scores.mean()

        study = optuna.create_study(direction=self.config.optimization.direction)
        study.optimize(objective, n_trials=self.config.optimization.n_trials)
        return study.best_trial, study

    def _select_and_tune(self):
        best_result = {"score": -np.inf, "spec": None, "trial": None, "study": None}

        for model_spec in self.config.models:
            if self.config.optimization.enabled:
                trial, study = self._optimize_one(model_spec)
                score = trial.value
            else:
                model = self._instantiate(model_spec["name"], model_spec.get("params", {}))
                score = cross_val_score(model, self.X_train, self.y_train, cv=self.config.optimization.cv_folds,
                                        scoring=self.config.optimization.scoring).mean()
                trial, study = None, None

            if score > best_result["score"]:
                best_result.update(score=score, spec=model_spec, trial=trial, study=study)

        return best_result

    def _train_and_eval(self, model_spec: dict, params: dict):
        clf = self._instantiate(model_spec["name"], params)
        clf.fit(self.X_train, self.y_train)

        train_metrics = {m: get_scorer(m)(clf, self.X_train, self.y_train)
                         for m in self.config.tracking.mlflow.metrics_to_log}
        val_metrics = {m: get_scorer(m)(clf, self.X_val, self.y_val)
                       for m in self.config.tracking.mlflow.metrics_to_log}
        return clf, train_metrics, val_metrics

    def _generate_report(self, best, train_metrics, val_metrics):
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "best_model": best["spec"]["name"].split(".")[-1],
            "best_model_params": best["trial"].params if best["trial"] else best["spec"].get("params", {}),
            "train_metrics": train_metrics,
            "val_metrics": val_metrics,
            "optimization": {
                "enabled": self.config.optimization.enabled,
                "best_trial": best["trial"].number if best["trial"] else None,
                "cv_folds": self.config.optimization.cv_folds,
                "direction": self.config.optimization.direction,
                "mean_score": best["score"]
            }
        }

    def run_training(self) -> ModelTrainerArtifact:
        try:
            logger.info("========== Starting Model Training ==========")
            self._load_data()

            with mlflow.start_run():
                best = self._select_and_tune()
                params = best["trial"].params if best["trial"] else best["spec"].get("params", {})
                model, train_m, val_m = self._train_and_eval(best["spec"], params)

                mlflow.log_params(params)
                for k, v in train_m.items():
                    mlflow.log_metric(f"train_{k}", v)
                for k, v in val_m.items():
                    mlflow.log_metric(f"val_{k}", v)

                # Save raw trained model
                trained_model_dir = self.config.root_dir / "trained_model"
                trained_model_path = trained_model_dir / "model.joblib"
                save_object(model, trained_model_path, "Trained Model")

                # Save inference-ready model (NetworkModel)
                network_model = NetworkModel.from_objects(
                    model=model,
                    x_preprocessor=joblib.load(self.transformation_artifact.x_preprocessor_filepath),
                    y_preprocessor=joblib.load(self.transformation_artifact.y_preprocessor_filepath),
                )
                inference_model_dir = self.config.root_dir / "inference_model"
                inference_model_path = inference_model_dir / "inference_model.joblib"
                save_object(network_model, inference_model_path, "Inference Model")

                # Save training report
                report_dir = self.config.root_dir / "reports"
                report_path = report_dir / "training_report.yaml"
                save_to_yaml(self._generate_report(best, train_m, val_m), report_path, label="Training Report")

            logger.info("========== Model Training Completed ==========")
            return ModelTrainerArtifact(
                trained_model_filepath=inference_model_path,
                training_report_filepath=report_path,
                x_train_filepath=self.transformation_artifact.x_train_filepath,
                y_train_filepath=self.transformation_artifact.y_train_filepath,
                x_val_filepath=self.transformation_artifact.x_val_filepath,
                y_val_filepath=self.transformation_artifact.y_val_filepath,
                x_test_filepath=self.transformation_artifact.x_test_filepath,
                y_test_filepath=self.transformation_artifact.y_test_filepath,
            )

        except Exception as e:
            raise NetworkSecurityError(e, logger) from e
