
📦 Project Structure of: E:\MyProjects\networksecurity

📁 .dvc/
    📄 .gitignore
    📁 cache/
        📁 files/
            📁 md5/
                📁 68/
                    📄 770e0efceb9cd3777502f505ecbec4
                📁 70/
                    📄 3506878baa6bf0360b2b42782e9677
                📁 83/
                    📄 de4e3a0dbe7d93403a8c15753377f7
                📁 86/
                    📄 4e5c8474fcd1b15bc434932036628d
                📁 87/
                    📄 f220430be73d16609bfd637b1644af
                📁 8d/
                    📄 e71e17a20f71bece7659a34bae7027
                📁 bf/
                    📄 fa4350e8aa16caaa3cbb92ec8e048c
                📁 e1/
                    📄 a8ae356f072436dd3acc1bcbafbd89
    📄 config
    📁 tmp/
        📄 btime
        📄 dag.md
        📁 exps/
            📁 cache/
                📁 08/
                    📄 ba3af380fc7743bb7784b0b99b9e84b597659c
                📁 0c/
                    📄 7b66bd77f8e71f84c28f8868f79d1c391b243e
                📁 17/
                    📄 a15f92afbc3a237675719f1bb52d4e45b2855f
                📁 20/
                    📄 418b499ec78701cefc3c979011ef77fa37084d
                📁 28/
                    📄 1710634e34c59aaca7c823b82ba97e8c3ae6c8
                    📄 e888b2e08a4b7dc34604b66abd2af2aaed6b86
                📁 29/
                    📄 28b4f351c2d8f210468c693d53455c7b09a1eb
                    📄 dfeb7151d1e0fc143507ecdecb1bedd2b5cc4d
                📁 2e/
                    📄 d0f21b70c06ff15b188bb69f6ce389a869af78
                📁 32/
                    📄 9b746e5971f793e269ced861c2d1e401f43d48
                📁 3e/
                    📄 a84c7b28b0242174e45334d5d6a9d3026f546d
                📁 48/
                    📄 faf510268751ce441d9178a563f5e2c4577d88
                📁 4a/
                    📄 640ce33b51ffeb8c1aa85e520ce78dfe105123
                📁 4c/
                    📄 0bc699b6997f940ba6b39f9c3f002d74b0cd52
                📁 51/
                    📄 a289cbadb16786fe46c2b4bab48853efc637a9
                📁 52/
                    📄 0a8ade47b7e748a46de7cd94c1f1db7a925527
                    📄 b49095c9d6e6a69ba58f77d1979c4626e30907
                📁 61/
                    📄 a81d71c590a7a662176a82aa5f00a7532a3e16
                📁 63/
                    📄 02ec043a14cb6881a1196ca1b58f7df093a586
                    📄 b517b2eff95eb91aba4bfef87abcd7955ae01c
                📁 64/
                    📄 3172fd8ac68c1fe35ae5f9a8b4734633a30778
                📁 67/
                    📄 fee62ed0bb527f9980ea136f0c312784242b38
                📁 68/
                    📄 81874523aeb9b0b77eb7bae250d0287e341558
                📁 6c/
                    📄 4d54822a8eec14b0c0ea4846ba14c9541e31dd
                📁 7f/
                    📄 e668c86a64d0171bc4dc8708460ac48ec77a34
                📁 88/
                    📄 a8f98863afa828023b2e3616348335b6d4628e
                📁 89/
                    📄 daed7059804441e9041b6d073c071a1cfcbe2c
                📁 8a/
                    📄 a79a390fba4858401f3e79d94bee5dbf354341
                    📄 ed08439cb37bbee9fee4ad00d82ea30435606b
                📁 a5/
                    📄 168ff7e9460e593e744f60ef1bad04413e1ca7
                📁 a8/
                    📄 2c1cb5bbc048ca32cb78061a4b8aaf3735233f
                📁 ad/
                    📄 329b2130c11421d465202bdf74f5170e661c23
                📁 af/
                    📄 ea20d43951b4448cd0a4eb4a7a134958cfb5c5
                📁 b0/
                    📄 63569211a95f11f23c4f61f6b9518bbf3b2cf3
                📁 b2/
                    📄 a55eaf06589f37339db03046e528a4eb70f795
                📁 bb/
                    📄 6504b5b79c287b4b60d2355251d8b30025c769
                    📄 e1aa1f0748e2a41f3ece907bfaaae1cafb5c48
                📁 c4/
                    📄 9033287ca54d77ad6ab9c7603c06825bbac4e6
                📁 c7/
                    📄 31a093cb23b7a2bef65922d39d9d20ac56e6f3
                📁 c8/
                    📄 dbdb66a143e984e4fe8b43c7d755441cdec3f5
                📁 ce/
                    📄 1c26eef879f92adf044d9d0da37c028fd7d494
                    📄 4e08484f42af33f1129dda5084bf3fed271a22
                📁 d2/
                    📄 3e801f5cac5d3a86a0e23a6fdbe1cbe281f789
                📁 d6/
                    📄 4f71f9b0de7fb7fca7d5f21249de0a489b4ca8
                📁 dc/
                    📄 fff6f4ba5cf77e9a0eaf75c0830d9795bb58cf
                📁 dd/
                    📄 ee659d7114460f19e1804a98117314724c9b04
                📁 e3/
                    📄 5de325a2125360b5ae289c080fc748a0140cdc
                    📄 8f209c48f47e7a864c8c9c61048ce16f1a2896
                📁 eb/
                    📄 79e8cfe8c4a974d876650c1a48de56761ef2a1
                📁 f3/
                    📄 8d3509705265cc0399a9e13b577074bfb9aedf
                📁 f6/
                    📄 f9d61771a2404a0d798efa45ff7ff15c8f639a
                📁 f9/
                    📄 21cb7aa3662a13f16a67e07a71781cec9b1a91
            📁 celery/
                📁 broker/
                    📁 control/
                    📁 in/
                    📁 processed/
                📁 result/
        📄 lock
        📄 rwlock
        📄 rwlock.lock
📄 .dvcignore
📄 .env
📄 .gitignore
📄 Dockerfile
📄 LICENSE
📁 NetworkSecurity.egg-info/
    📄 PKG-INFO
    📄 SOURCES.txt
    📄 dependency_links.txt
    📄 requires.txt
    📄 top_level.txt
📄 README.md
📄 app.py
📁 artifacts/
    📁 2025_05_24T09_51_21Z/
        📁 data_ingestion/
            📁 featurestore/
                📄 raw_data.csv
            📁 ingested/
                📄 ingested_data.csv
        📁 data_transformation/
            📁 preprocessor/
                📄 x_preprocessor.joblib
                📄 y_preprocessor.joblib
            📁 transformed/
                📁 test/
                    📄 x_test.npy
                    📄 y_test.npy
                📁 train/
                    📄 x_train.npy
                    📄 y_train.npy
                📁 val/
                    📄 x_val.npy
                    📄 y_val.npy
        📁 data_validation/
            📁 reports/
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.json
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
        📁 model_evaluation/
            📄 evaluation_report.yaml
        📁 model_trainer/
            📁 inference_model/
                📄 inference_model.joblib
            📁 reports/
                📄 training_report.yaml
            📁 trained_model/
                📄 model.joblib
📁 config/
    📄 config.yaml
    📄 params.yaml
    📄 schema.yaml
    📄 templates.yaml
📁 data/
    📁 raw/
        📄 raw_data.csv
        📄 raw_data.csv.dvc
    📁 transformed/
        📁 test/
            📄 x_test.npy
            📄 x_test.npy.dvc
            📄 y_test.npy
            📄 y_test.npy.dvc
        📁 train/
            📄 x_train.npy
            📄 x_train.npy.dvc
            📄 y_train.npy
            📄 y_train.npy.dvc
        📁 val/
            📄 x_val.npy
            📄 x_val.npy.dvc
            📄 y_val.npy
            📄 y_val.npy.dvc
    📁 validated/
        📄 validated_data.csv
        📄 validated_data.csv.dvc
📄 debug.py
📄 debug_pipeline.py
📄 docker-compose.yaml
📁 final_model/
    📄 final_inference_model.joblib
📁 logs/
    📁 2025_05_24T09_51_21Z/
        📄 2025_05_24T09_51_21Z.log
📄 main.py
📁 network_data/
    📁 input_csv/
        📄 phisingData.csv
        📄 phisingData_check.csv
📁 prediction_output/
📄 print_structure.py
📄 project_dump.py
📄 project_template.py
📄 requirements.txt
📁 research/
    📁 config/
        📄 schema.yaml
    📄 ingested_data.csv
    📄 research.ipynb
📄 setup.py
📁 src/
    📁 networksecurity/
        📄 __init__.py
        📁 components/
            📄 __init__.py
            📄 data_ingestion.py
            📄 data_transformation.py
            📄 data_validation.py
            📄 model_evaluation.py
            📄 model_pusher.py
            📄 model_trainer.py
        📁 config/
            📄 __init__.py
            📄 configuration.py
        📁 constants/
            📄 __init__.py
            📄 constants.py
        📁 data_processors/
            📄 encoder_factory.py
            📄 imputer_factory.py
            📄 label_mapper.py
            📄 preprocessor_builder.py
            📄 scaler_factory.py
        📁 dbhandler/
            📄 __init__.py
            📄 base_handler.py
            📄 mongodb_handler.py
            📄 s3_handler.py
        📁 entity/
            📄 __init__.py
            📄 artifact_entity.py
            📄 config_entity.py
        📁 exception/
            📄 __init__.py
            📄 exception.py
        📁 inference/
            📄 estimator.py
        📁 logging/
            📄 __init__.py
            📄 logger.py
        📁 pipeline/
            📄 __init__.py
            📄 data_ingestion_pipeline.py
            📄 data_transformation_pipeline.py
            📄 data_validation_pipeline.py
            📄 model_evaluation_pipeline.py
            📄 model_pusher_pipeline.py
            📄 model_trainer_pipeline.py
            📄 training_pipeline.py
        📁 utils/
            📄 __init__.py
            📄 core.py
            📄 timestamp.py
        📁 worker/
            📄 celery_worker.py
📁 templates/
    📄 table.html

--- CODE DUMP | PART 3 of 4 ---


================================================================================
# PY FILE: src\networksecurity\dbhandler\s3_handler.py
================================================================================

from pathlib import Path
import os
import boto3
from botocore.exceptions import ClientError

import pandas as pd

from src.networksecurity.entity.config_entity import S3HandlerConfig
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger
from src.networksecurity.dbhandler.base_handler import DBHandler


class S3Handler(DBHandler):
    """
    AWS S3 Handler implementing DBHandler interface.
    Supports file upload and directory sync.
    """

    def __init__(self, config: S3HandlerConfig):
        try:
            self.config = config
            self._client = boto3.client("s3", region_name=self.config.aws_region)
            logger.info(f"S3Handler initialized for bucket '{self.config.bucket_name}' in region '{self.config.aws_region}'")
        except Exception as e:
            logger.exception("Failed to initialize S3 client")
            raise NetworkSecurityError(e, logger) from e

    def close(self) -> None:
        """
        No persistent connection in S3 client.
        Included for interface compatibility.
        """
        logger.info("S3Handler close() called. No persistent connection to close.")

    def load_from_source(self) -> pd.DataFrame:
        """
        Not supported: Loading data as DataFrame from S3 is not implemented.
        """
        raise NotImplementedError("S3Handler does not support DataFrame loading from S3.")

    def upload_file(self, local_path: Path, s3_key: str) -> None:
        """
        Upload a single file to S3.

        Args:
            local_path (Path): Local file path.
            s3_key (str): Destination S3 key (folder/filename).
        """
        try:
            local_path = Path(local_path)
            if not local_path.is_file():
                raise FileNotFoundError(f"Local file not found: {local_path.as_posix()}")

            self._client.upload_file(
                Filename=str(local_path),
                Bucket=self.config.bucket_name,
                Key=s3_key
            )
            logger.info(f"Uploaded: {local_path.as_posix()} -> s3://{self.config.bucket_name}/{s3_key}")

        except ClientError as e:
            logger.error(f"AWS ClientError while uploading to S3: {e}")
            raise NetworkSecurityError(e, logger) from e
        except Exception as e:
            logger.error(f"Unexpected error uploading to S3: {e}")
            raise NetworkSecurityError(e, logger) from e

    def sync_directory(self, local_dir: Path, s3_prefix: str) -> None:
        """
        Uploads a directory to S3 recursively.

        Args:
            local_dir (Path): Local directory to sync.
            s3_prefix (str): S3 prefix (target folder path).
        """
        try:
            local_dir = Path(local_dir)
            if not local_dir.is_dir():
                raise NotADirectoryError(f"Local directory not found: {local_dir.as_posix()}")

            logger.info(f"Starting S3 sync: {local_dir.as_posix()} -> s3://{self.config.bucket_name}/{s3_prefix}")

            for root, _, files in os.walk(local_dir):
                for file in files:
                    local_file_path = Path(root) / file
                    relative_path = local_file_path.relative_to(local_dir)
                    remote_key = (Path(s3_prefix) / relative_path).as_posix()
                    self.upload_file(local_file_path, remote_key)

            logger.info(f"Directory synced to S3: {local_dir.as_posix()} -> s3://{self.config.bucket_name}/{s3_prefix}")

        except Exception as e:
            logger.error("Directory sync to S3 failed.")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\entity\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\entity\artifact_entity.py
================================================================================

from dataclasses import dataclass
from pathlib import Path
from typing import Optional


@dataclass(frozen=True)
class DataIngestionArtifact:
    raw_artifact_path: Path
    ingested_data_filepath: Path
    raw_dvc_path: Path

    def __repr__(self) -> str:
        raw_artifact_str = self.raw_artifact_path.as_posix() if self.raw_artifact_path else "None"
        raw_dvc_str = self.raw_dvc_path.as_posix() if self.raw_dvc_path else "None"
        ingested_data_str = self.ingested_data_filepath.as_posix() if self.ingested_data_filepath else "None"

        return (
            "\nData Ingestion Artifact:\n"
            f"  - Raw Artifact:         '{raw_artifact_str}'\n"
            f"  - Raw DVC Path:         '{raw_dvc_str}'\n"
            f"  - Ingested Data Path:   '{ingested_data_str}'\n"
        )


@dataclass(frozen=True)
class DataValidationArtifact:
    validated_filepath: Path
    validation_status: bool

    def __repr__(self) -> str:
        validated_str = self.validated_filepath.as_posix() if self.validated_filepath else "None"

        return (
            "\nData Validation Artifact:\n"
            f"  - Validated Data Path: '{validated_str}'\n"
            f"  - Validation Status:   '{self.validation_status}'\n"
        )

@dataclass(frozen=True)
class DataTransformationArtifact:
    x_train_filepath: Path
    y_train_filepath: Path
    x_val_filepath: Path
    y_val_filepath: Path
    x_test_filepath: Path
    y_test_filepath: Path
    x_preprocessor_filepath: Path
    y_preprocessor_filepath: Path

    def __repr__(self) -> str:
        x_train_str = self.x_train_filepath.as_posix() if self.x_train_filepath else "None"
        y_train_str = self.y_train_filepath.as_posix() if self.y_train_filepath else "None"
        x_val_str = self.x_val_filepath.as_posix() if self.x_val_filepath else "None"
        y_val_str = self.y_val_filepath.as_posix() if self.y_val_filepath else "None"
        x_test_str = self.x_test_filepath.as_posix() if self.x_test_filepath else "None"
        y_test_str = self.y_test_filepath.as_posix() if self.y_test_filepath else "None"
        x_preprocessor_str = self.x_preprocessor_filepath.as_posix() if self.x_preprocessor_filepath else "None"
        y_preprocessor_str = self.y_preprocessor_filepath.as_posix() if self.y_preprocessor_filepath else "None"

        return (
            "\nData Transformation Artifact:\n"
            f"  - X-Train Data Path:    '{x_train_str}'\n"
            f"  - Y-Train Data Path:    '{y_train_str}'\n"
            f"  - X-Val Data Path:      '{x_val_str}'\n"
            f"  - Y-Val Data Path:      '{y_val_str}'\n"
            f"  - X-Test Data Path:     '{x_test_str}'\n"
            f"  - Y-Test Data Path:     '{y_test_str}'\n"
            f"  - X-Processor Path:     '{x_preprocessor_str}'\n"
            f"  - Y-Processor Path:     '{y_preprocessor_str}'\n"
        )


@dataclass(frozen=True)
class ModelTrainerArtifact:
    trained_model_filepath: Path
    training_report_filepath: Path
    x_train_filepath: Path
    y_train_filepath: Path
    x_val_filepath: Path
    y_val_filepath: Path
    x_test_filepath: Path
    y_test_filepath: Path

    def __repr__(self) -> str:
        return (
            "\nModel Trainer Artifact:\n"
            f"  - Trained Model Path:   '{self.trained_model_filepath.as_posix()}'\n"
            f"  - Training Report Path: '{self.training_report_filepath.as_posix()}'\n"
            f"  - X Train Path: '{self.x_train_filepath.as_posix()}'\n"
            f"  - Y Train Path: '{self.y_train_filepath.as_posix()}'\n"
            f"  - X Val Path:   '{self.x_val_filepath.as_posix()}'\n"
            f"  - Y Val Path:   '{self.y_val_filepath.as_posix()}'\n"
            f"  - X Test Path:  '{self.x_test_filepath.as_posix()}'\n"
            f"  - Y Test Path:  '{self.y_test_filepath.as_posix()}'"
        )


@dataclass(frozen=True)
class ModelEvaluationArtifact:
    evaluation_report_filepath: Path

    def __repr__(self) -> str:
        report_str = self.evaluation_report_filepath.as_posix() if self.evaluation_report_filepath else "None"

        return (
            "\nModel Evaluation Artifact:\n"
            f"  - Evaluation Report Path: '{report_str}'\n"
        )

@dataclass(frozen=True)
class ModelPusherArtifact:
    pushed_model_local_path: Path
    pushed_model_s3_path: str

    def __repr__(self) -> str:
        local_str = self.pushed_model_local_path.as_posix() if self.pushed_model_local_path else "None"
        s3_str = self.pushed_model_s3_path if self.pushed_model_s3_path else "None"
        return (
            "\nModel Pusher Artifact:\n"
            f"  - Local Path: '{local_str}'\n"
            f"  - S3 Path:    '{s3_str}'\n"
        )


@dataclass(frozen=True)
class ModelPusherArtifact:
    pushed_model_local_path: Path
    pushed_model_s3_path: str | None = None  # Optional if S3 upload is disabled

    def __repr__(self) -> str:
        return (
            "\nModel Pusher Artifact:\n"
            f"  - Local Model Path: {self.pushed_model_local_path.as_posix()}\n"
            f"  - S3 Model Path:    {self.pushed_model_s3_path or 'Not uploaded'}\n"
        )

================================================================================
# PY FILE: src\networksecurity\entity\config_entity.py
================================================================================

from pathlib import Path
from dataclasses import dataclass
from box import ConfigBox
from typing import List


@dataclass
class MongoHandlerConfig:
    root_dir: Path
    input_data_path: Path
    json_data_filename: str
    json_data_dir: Path
    mongodb_uri: str
    database_name: str
    collection_name: str

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.input_data_path = Path(self.input_data_path)
        self.json_data_dir = Path(self.json_data_dir)

    @property
    def json_data_filepath(self) -> Path:
        return self.json_data_dir / self.json_data_filename


@dataclass
class DataIngestionConfig:
    root_dir: Path
    featurestore_dir: Path
    raw_data_filename: str
    ingested_data_dir: Path
    ingested_data_filename: str
    raw_dvc_path: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.featurestore_dir = Path(self.featurestore_dir)
        self.ingested_data_dir = Path(self.ingested_data_dir)
        self.raw_dvc_path = Path(self.raw_dvc_path)

    @property
    def raw_data_filepath(self) -> Path:
        return self.featurestore_dir / self.raw_data_filename

    @property
    def ingested_data_filepath(self) -> Path:
        return self.ingested_data_dir / self.ingested_data_filename


@dataclass
class DataValidationConfig:
    root_dir: Path
    validated_dir: Path
    validated_filename: str
    report_dir: Path
    missing_report_filename: str
    duplicates_report_filename: str
    drift_report_filename: str
    validation_report_filename: str
    schema: dict
    validation_params: dict
    validated_dvc_path: Path
    val_report_template: dict

    def __post_init__(self) -> Path:
        self.root_dir = Path(self.root_dir)
        self.validated_dir = Path(self.validated_dir)
        self.report_dir = Path(self.report_dir)
        self.validated_dvc_path = Path(self.validated_dvc_path)

    @property
    def validated_filepath(self) -> Path:
        return self.validated_dir / self.validated_filename

    @property
    def missing_report_filepath(self) -> Path:
        return self.report_dir / self.missing_report_filename

    @property
    def duplicates_report_filepath(self) -> Path:
        return self.report_dir / self.duplicates_report_filename

    @property
    def drift_report_filepath(self) -> Path:
        return self.report_dir / self.drift_report_filename

    @property
    def validation_report_filepath(self) -> Path:
        return self.report_dir / self.validation_report_filename


@dataclass
class DataTransformationConfig:
    root_dir: Path
    transformation_params: dict
    train_dir: Path
    val_dir: Path
    test_dir: Path
    x_train_filename: str
    y_train_filename: str
    x_val_filename: str
    y_val_filename: str
    x_test_filename: str
    y_test_filename: str
    preprocessor_dir: Path
    x_preprocessor_filename: str
    y_preprocessor_filename: str
    target_column: str
    train_dvc_dir: Path
    val_dvc_dir: Path
    test_dvc_dir: Path

    def __post_init__(self) -> Path:
        self.root_dir = Path(self.root_dir)
        self.train_dir = Path(self.train_dir)
        self.val_dir = Path(self.val_dir)
        self.test_dir = Path(self.test_dir)
        self.preprocessor_dir = Path(self.preprocessor_dir)

    @property
    def x_train_filepath(self) -> Path:
        return self.train_dir / self.x_train_filename

    @property
    def y_train_filepath(self) -> Path:
        return self.train_dir / self.y_train_filename

    @property
    def x_val_filepath(self) -> Path:
        return self.val_dir / self.x_val_filename

    @property
    def y_val_filepath(self) -> Path:
        return self.val_dir / self.y_val_filename

    @property
    def x_test_filepath(self) -> Path:
        return self.test_dir / self.x_test_filename

    @property
    def y_test_filepath(self) -> Path:
        return self.test_dir / self.y_test_filename

    @property
    def x_preprocessor_filepath(self) -> Path:
        return self.preprocessor_dir / self.x_preprocessor_filename

    @property
    def y_preprocessor_filepath(self) -> Path:
        return self.preprocessor_dir / self.y_preprocessor_filename

    @property
    def x_train_dvc_filepath(self) -> Path:
        return self.train_dvc_dir / self.x_train_filename

    @property
    def y_train_dvc_filepath(self) -> Path:
        return self.train_dvc_dir / self.y_train_filename

    @property
    def x_val_dvc_filepath(self) -> Path:
        return self.val_dvc_dir / self.x_val_filename

    @property
    def y_val_dvc_filepath(self) -> Path:
        return self.val_dvc_dir / self.y_val_filename

    @property
    def x_test_dvc_filepath(self) -> Path:
        return self.test_dvc_dir / self.x_test_filename

    @property
    def y_test_dvc_filepath(self) -> Path:
        return self.test_dvc_dir / self.y_test_filename



@dataclass
class ModelTrainerConfig:
    root_dir: Path
    trained_model_filename: str
    training_report_filename: str
    models: list[dict]
    optimization: dict
    tracking: dict
    models: List[dict]
    optimization: ConfigBox
    tracking: ConfigBox
    # where to load transformed arrays from (DVC-tracked)
    train_dir: Path
    val_dir:   Path
    test_dir:  Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)

    @property
    def trained_model_filepath(self) -> Path:
        return self.root_dir / self.trained_model_filename

    @property
    def training_report_filepath(self) -> Path:
        return self.root_dir / self.training_report_filename


@dataclass
class ModelEvaluationConfig:
    root_dir: Path
    evaluation_report_filename: str
    train_dir: Path
    val_dir: Path
    test_dir: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)

    @property
    def evaluation_report_filepath(self) -> Path:
        return self.root_dir / self.evaluation_report_filename


@dataclass
class ModelPusherConfig:
    pushed_model_dir: Path
    pushed_model_filename: str
    upload_to_s3: bool

    def __post_init__(self):
        self.pushed_model_dir = Path(self.pushed_model_dir)

    @property
    def pushed_model_filepath(self) -> Path:
        return self.pushed_model_dir / self.pushed_model_filename


@dataclass
class S3HandlerConfig:
    root_dir: Path
    bucket_name: str
    aws_region: str
    local_dir_to_sync: Path
    s3_artifacts_prefix: str
    s3_final_model_prefix: str

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.local_dir_to_sync = Path(self.local_dir_to_sync)

================================================================================
# PY FILE: src\networksecurity\exception\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\exception\exception.py
================================================================================

"""Custom exception and logger interface for the phishing detection web app.

This module defines:
- LoggerInterface: a structural protocol for logging
- NetworkSecurityError: a traceback-aware exception with logging support
"""


import sys
from typing import Protocol


class LoggerInterface(Protocol):
    """A structural interface that defines the logger's required methods.

    Any logger passed to NetworkSecurityException must implement this interface.
    """

    def error(self, message: str) -> None:
        """Log an error-level message."""
        ...


class NetworkSecurityError(Exception):
    """Custom exception class for the phishing detection web application.

    Captures traceback details and logs the error using the provided logger.
    """

    def __init__(self, error_message: Exception, logger: LoggerInterface) -> None:
        """Initialize the exception and log it using the injected logger.

        Args:
            error_message (Exception): The original caught exception.
            logger (LoggerInterface): A logger that supports an `error(str)` method.

        """
        super().__init__(str(error_message))
        self.error_message: str = str(error_message)

        # Get traceback info from sys
        _, _, exc_tb = sys.exc_info()
        self.lineno: int | None = exc_tb.tb_lineno if exc_tb else None
        self.file_name: str | None = (
            exc_tb.tb_frame.f_code.co_filename if exc_tb else "Unknown"
        )

        # Log the formatted error
        logger.error(str(self))

    def __str__(self) -> str:
        """Return a formatted error message with file name and line number."""
        return (
            f"Error occurred in file [{self.file_name}], "
            f"line [{self.lineno}], "
            f"message: [{self.error_message}]"
        )

================================================================================
# PY FILE: src\networksecurity\inference\estimator.py
================================================================================

import joblib
from dataclasses import dataclass
from typing import Any
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


@dataclass
class NetworkModel:
    model: Any
    x_preprocessor: Any = None
    y_preprocessor: Any = None

    def predict(self, X):
        try:
            if self.x_preprocessor:
                X = self.x_preprocessor.transform(X)
            return self.model.predict(X)
        except Exception as e:
            raise NetworkSecurityError(e, logger)

    @classmethod
    def from_artifacts(cls, model_path, x_preprocessor_path=None, y_preprocessor_path=None):
        try:
            model = joblib.load(model_path)

            x_preprocessor = joblib.load(x_preprocessor_path) if x_preprocessor_path else None
            y_preprocessor = joblib.load(y_preprocessor_path) if y_preprocessor_path else None

            return cls(model=model, x_preprocessor=x_preprocessor, y_preprocessor=y_preprocessor)

        except Exception as e:
            raise NetworkSecurityError(e, logger)

    @classmethod
    def from_objects(cls, model, x_preprocessor=None, y_preprocessor=None):
        try:
            return cls(model=model, x_preprocessor=x_preprocessor, y_preprocessor=y_preprocessor)
        except Exception as e:
            raise NetworkSecurityError(e, logger)

================================================================================
# PY FILE: src\networksecurity\logging\__init__.py
================================================================================

"""
Initialize centralized logger for the `networksecurity.logging` package.

This module sets up a reusable logger instance (`logger`) that can be imported
across the project to ensure consistent, centralized logging configuration.

Supports:
- Shared UTC timestamp for file/folder naming
- Dynamic logger name via environment variable
- Dynamic log level via environment variable
"""

import os
import logging

from .logger import setup_logger

# Use env variable for logger name, default to "networksecurity"
LOGGER_NAME = os.getenv("LOGGER_NAME", "networksecurity")

# Use env variable for log level, default to "DEBUG"
LOG_LEVEL = os.getenv("LOG_LEVEL", "DEBUG").upper()

# Initialize and configure the logger
logger = setup_logger(name=LOGGER_NAME)
logger.setLevel(getattr(logging, LOG_LEVEL, logging.DEBUG))

================================================================================
# PY FILE: src\networksecurity\logging\logger.py
================================================================================

"""
Logging utility module.

Provides `setup_logger()` to configure a logger with both a file and stream handler,
using a UTC timestamp synchronized across the pipeline (logs, artifacts, models, etc.).
"""

import logging
import sys
from pathlib import Path

from src.networksecurity.constants.constants import LOGS_ROOT
from src.networksecurity.utils.timestamp import get_shared_utc_timestamp

def setup_logger(name: str = "app_logger") -> logging.Logger:
    """
    Set up and return a logger instance with a consistent timestamped log directory and file.

    Ensures:
    - One timestamp per pipeline run (shared with ConfigurationManager)
    - No duplicate handlers on repeated setup
    - Clean log formatting to stdout and file

    Args:
        name (str): The name of the logger instance (e.g., 'data_ingestion').

    Returns:
        logging.Logger: Configured logger with file and stream handlers.
    """
    sys.stdout.reconfigure(encoding='utf-8')

    # Get shared timestamp for this run
    timestamp = get_shared_utc_timestamp()

    # Log folder: logs/<timestamp>/
    log_dir = Path(LOGS_ROOT) / timestamp
    log_dir.mkdir(parents=True, exist_ok=True)

    # Log file: logs/<timestamp>/<timestamp>.log
    log_filepath = log_dir / f"{timestamp}.log"

    # Log message format
    log_format = "[%(asctime)s] - %(levelname)s - %(module)s - %(message)s"
    formatter = logging.Formatter(log_format)

    # Get or create logger
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)

    # Add file handler if not already added
    if not any(isinstance(h, logging.FileHandler) and h.baseFilename == str(log_filepath)
               for h in logger.handlers):
        file_handler = logging.FileHandler(log_filepath, mode="a")
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    # Add stdout stream handler if not already added
    if not any(isinstance(h, logging.StreamHandler) and h.stream == sys.stdout
               for h in logger.handlers):
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)

    return logger


# Example usage
if __name__ == "__main__":
    logger = setup_logger()
    logger.info("Logger initialized successfully.")

================================================================================
# PY FILE: src\networksecurity\pipeline\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\pipeline\data_ingestion_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_ingestion import DataIngestion
from src.networksecurity.dbhandler.mongodb_handler import MongoDBHandler
from src.networksecurity.entity.config_entity import DataIngestionConfig, MongoHandlerConfig
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataIngestionPipeline:
    """
    Runs the data ingestion stage:
    - Loads configs
    - Instantiates handler and component
    - Triggers ingestion and returns artifact
    """

    def __init__(self):
        try:
            self.config_manager = ConfigurationManager()
            self.ingestion_config: DataIngestionConfig = self.config_manager.get_data_ingestion_config()
            self.mongo_config: MongoHandlerConfig = self.config_manager.get_mongo_handler_config()
            self.mongo_handler = MongoDBHandler(config=self.mongo_config)
        except Exception as e:
            logger.exception("Failed to initialize DataIngestionPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> DataIngestionArtifact:
        try:
            logger.info("========== Data Ingestion Stage Started ==========")

            ingestion = DataIngestion(
                config=self.ingestion_config,
                db_handler=self.mongo_handler,
            )
            artifact = ingestion.run_ingestion()

            logger.info(f"Data Ingestion Process Completed.\n{artifact}")
            logger.info("========== Data Ingestion Stage Completed ==========")

            return artifact

        except Exception as e:
            logger.exception("Data Ingestion Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\data_transformation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_transformation import DataTransformation
from src.networksecurity.entity.artifact_entity import (
    DataValidationArtifact,
    DataTransformationArtifact,
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataTransformationPipeline:
    """
    Orchestrates the Data Transformation stage of the pipeline.

    Responsibilities:
    - Loads transformation configuration
    - Accepts validated artifact
    - Performs feature transformation and returns transformation artifact
    """

    def __init__(self, validation_artifact: DataValidationArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.config = self.config_manager.get_data_transformation_config()
            self.validation_artifact = validation_artifact
        except Exception as e:
            logger.exception("Failed to initialize DataTransformationPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> DataTransformationArtifact:
        try:
            logger.info("========== Data Transformation Stage Started ==========")

            transformer = DataTransformation(
                config=self.config,
                validation_artifact=self.validation_artifact,
            )
            transformation_artifact = transformer.run_transformation()

            logger.info(f"Data Transformation Completed Successfully: {transformation_artifact}")
            logger.info("========== Data Transformation Stage Completed ==========")

            return transformation_artifact

        except Exception as e:
            logger.exception("Data Transformation Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\data_validation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.data_validation import DataValidation
from src.networksecurity.entity.artifact_entity import DataIngestionArtifact
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class DataValidationPipeline:
    """
    Orchestrates the Data Validation stage of the pipeline.

    Responsibilities:
    - Fetches the configuration and artifacts
    - Loads validated DataFrame from artifact
    - Validates schema and schema hash
    """

    def __init__(self, ingestion_artifact: DataIngestionArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.ingestion_artifact = ingestion_artifact
            self.config = self.config_manager.get_data_validation_config()
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run(self):
        try:
            logger.info("========= Data Validation Stage Started =========")
            validation = DataValidation(config=self.config, ingestion_artifact=self.ingestion_artifact)
            validation_artifact = validation.run_validation()
            logger.info(f"Data Validation Process Completed.\n{validation_artifact}")
            logger.info("========= Data Validation Stage Completed =========")
            return validation_artifact
        except Exception as e:
            logger.error("Data Validation Pipeline Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\model_evaluation_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.model_evaluation import ModelEvaluation
from src.networksecurity.entity.artifact_entity import (
    ModelTrainerArtifact,
    ModelEvaluationArtifact,
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError

class ModelEvaluationPipeline:
    """
    Orchestrates the Model Evaluation stage of the pipeline.

    Responsibilities:
    - Loads model evaluation configuration
    - Accepts only the trainer artifact (no transformation artifact needed)
    - Evaluates trained model on train/val/test datasets
    - Emits a ModelEvaluationArtifact
    """

    def __init__(self, trainer_artifact: ModelTrainerArtifact):
        try:
            logger.info("Initializing ModelEvaluationPipeline...")
            self.config = ConfigurationManager().get_model_evaluation_config()
            self.trainer_artifact = trainer_artifact
        except Exception as e:
            logger.exception("Failed to initialize ModelEvaluationPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> ModelEvaluationArtifact:
        try:
            logger.info("========== Model Evaluation Stage Started ==========")

            evaluator = ModelEvaluation(
                config=self.config,
                trainer_artifact=self.trainer_artifact
            )
            evaluation_artifact = evaluator.run_evaluation()

            logger.info(f"Model Evaluation Completed Successfully: {evaluation_artifact}")
            logger.info("========== Model Evaluation Stage Completed ==========")

            return evaluation_artifact

        except Exception as e:
            logger.exception("Model Evaluation Stage Failed")
            raise NetworkSecurityError(e, logger) from e
