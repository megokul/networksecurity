
📦 Project Structure of: E:\MyProjects\networksecurity

📁 .dvc/
    📄 .gitignore
    📁 cache/
        📁 files/
            📁 md5/
                📁 68/
                    📄 770e0efceb9cd3777502f505ecbec4
                📁 70/
                    📄 3506878baa6bf0360b2b42782e9677
                📁 83/
                    📄 de4e3a0dbe7d93403a8c15753377f7
                📁 86/
                    📄 4e5c8474fcd1b15bc434932036628d
                📁 87/
                    📄 f220430be73d16609bfd637b1644af
                📁 8d/
                    📄 e71e17a20f71bece7659a34bae7027
                📁 bf/
                    📄 fa4350e8aa16caaa3cbb92ec8e048c
                📁 e1/
                    📄 a8ae356f072436dd3acc1bcbafbd89
    📄 config
    📁 tmp/
        📄 btime
        📄 dag.md
        📁 exps/
            📁 cache/
                📁 08/
                    📄 ba3af380fc7743bb7784b0b99b9e84b597659c
                📁 0c/
                    📄 7b66bd77f8e71f84c28f8868f79d1c391b243e
                📁 17/
                    📄 a15f92afbc3a237675719f1bb52d4e45b2855f
                📁 20/
                    📄 418b499ec78701cefc3c979011ef77fa37084d
                📁 28/
                    📄 1710634e34c59aaca7c823b82ba97e8c3ae6c8
                    📄 e888b2e08a4b7dc34604b66abd2af2aaed6b86
                📁 29/
                    📄 28b4f351c2d8f210468c693d53455c7b09a1eb
                    📄 dfeb7151d1e0fc143507ecdecb1bedd2b5cc4d
                📁 2e/
                    📄 d0f21b70c06ff15b188bb69f6ce389a869af78
                📁 32/
                    📄 9b746e5971f793e269ced861c2d1e401f43d48
                📁 3e/
                    📄 a84c7b28b0242174e45334d5d6a9d3026f546d
                📁 48/
                    📄 faf510268751ce441d9178a563f5e2c4577d88
                📁 4a/
                    📄 640ce33b51ffeb8c1aa85e520ce78dfe105123
                📁 4c/
                    📄 0bc699b6997f940ba6b39f9c3f002d74b0cd52
                📁 51/
                    📄 a289cbadb16786fe46c2b4bab48853efc637a9
                📁 52/
                    📄 0a8ade47b7e748a46de7cd94c1f1db7a925527
                    📄 b49095c9d6e6a69ba58f77d1979c4626e30907
                📁 61/
                    📄 a81d71c590a7a662176a82aa5f00a7532a3e16
                📁 63/
                    📄 02ec043a14cb6881a1196ca1b58f7df093a586
                    📄 b517b2eff95eb91aba4bfef87abcd7955ae01c
                📁 64/
                    📄 3172fd8ac68c1fe35ae5f9a8b4734633a30778
                📁 67/
                    📄 fee62ed0bb527f9980ea136f0c312784242b38
                📁 68/
                    📄 81874523aeb9b0b77eb7bae250d0287e341558
                📁 6c/
                    📄 4d54822a8eec14b0c0ea4846ba14c9541e31dd
                📁 7f/
                    📄 e668c86a64d0171bc4dc8708460ac48ec77a34
                📁 88/
                    📄 a8f98863afa828023b2e3616348335b6d4628e
                📁 89/
                    📄 daed7059804441e9041b6d073c071a1cfcbe2c
                📁 8a/
                    📄 a79a390fba4858401f3e79d94bee5dbf354341
                    📄 ed08439cb37bbee9fee4ad00d82ea30435606b
                📁 a5/
                    📄 168ff7e9460e593e744f60ef1bad04413e1ca7
                📁 a8/
                    📄 2c1cb5bbc048ca32cb78061a4b8aaf3735233f
                📁 ad/
                    📄 329b2130c11421d465202bdf74f5170e661c23
                📁 af/
                    📄 ea20d43951b4448cd0a4eb4a7a134958cfb5c5
                📁 b0/
                    📄 63569211a95f11f23c4f61f6b9518bbf3b2cf3
                📁 b2/
                    📄 a55eaf06589f37339db03046e528a4eb70f795
                📁 bb/
                    📄 6504b5b79c287b4b60d2355251d8b30025c769
                    📄 e1aa1f0748e2a41f3ece907bfaaae1cafb5c48
                📁 c4/
                    📄 9033287ca54d77ad6ab9c7603c06825bbac4e6
                📁 c7/
                    📄 31a093cb23b7a2bef65922d39d9d20ac56e6f3
                📁 c8/
                    📄 dbdb66a143e984e4fe8b43c7d755441cdec3f5
                📁 ce/
                    📄 1c26eef879f92adf044d9d0da37c028fd7d494
                    📄 4e08484f42af33f1129dda5084bf3fed271a22
                📁 d2/
                    📄 3e801f5cac5d3a86a0e23a6fdbe1cbe281f789
                📁 d6/
                    📄 4f71f9b0de7fb7fca7d5f21249de0a489b4ca8
                📁 dc/
                    📄 fff6f4ba5cf77e9a0eaf75c0830d9795bb58cf
                📁 dd/
                    📄 ee659d7114460f19e1804a98117314724c9b04
                📁 e3/
                    📄 5de325a2125360b5ae289c080fc748a0140cdc
                    📄 8f209c48f47e7a864c8c9c61048ce16f1a2896
                📁 eb/
                    📄 79e8cfe8c4a974d876650c1a48de56761ef2a1
                📁 f3/
                    📄 8d3509705265cc0399a9e13b577074bfb9aedf
                📁 f6/
                    📄 f9d61771a2404a0d798efa45ff7ff15c8f639a
                📁 f9/
                    📄 21cb7aa3662a13f16a67e07a71781cec9b1a91
            📁 celery/
                📁 broker/
                    📁 control/
                    📁 in/
                    📁 processed/
                📁 result/
        📄 lock
        📄 rwlock
        📄 rwlock.lock
📄 .dvcignore
📄 .env
📄 .gitignore
📄 Dockerfile
📄 LICENSE
📁 NetworkSecurity.egg-info/
    📄 PKG-INFO
    📄 SOURCES.txt
    📄 dependency_links.txt
    📄 requires.txt
    📄 top_level.txt
📄 README.md
📄 app.py
📁 artifacts/
    📁 2025_05_24T09_51_21Z/
        📁 data_ingestion/
            📁 featurestore/
                📄 raw_data.csv
            📁 ingested/
                📄 ingested_data.csv
        📁 data_transformation/
            📁 preprocessor/
                📄 x_preprocessor.joblib
                📄 y_preprocessor.joblib
            📁 transformed/
                📁 test/
                    📄 x_test.npy
                    📄 y_test.npy
                📁 train/
                    📄 x_train.npy
                    📄 y_train.npy
                📁 val/
                    📄 x_val.npy
                    📄 y_val.npy
        📁 data_validation/
            📁 reports/
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.json
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
        📁 model_evaluation/
            📄 evaluation_report.yaml
        📁 model_trainer/
            📁 inference_model/
                📄 inference_model.joblib
            📁 reports/
                📄 training_report.yaml
            📁 trained_model/
                📄 model.joblib
📁 config/
    📄 config.yaml
    📄 params.yaml
    📄 schema.yaml
    📄 templates.yaml
📁 data/
    📁 raw/
        📄 raw_data.csv
        📄 raw_data.csv.dvc
    📁 transformed/
        📁 test/
            📄 x_test.npy
            📄 x_test.npy.dvc
            📄 y_test.npy
            📄 y_test.npy.dvc
        📁 train/
            📄 x_train.npy
            📄 x_train.npy.dvc
            📄 y_train.npy
            📄 y_train.npy.dvc
        📁 val/
            📄 x_val.npy
            📄 x_val.npy.dvc
            📄 y_val.npy
            📄 y_val.npy.dvc
    📁 validated/
        📄 validated_data.csv
        📄 validated_data.csv.dvc
📄 debug.py
📄 debug_pipeline.py
📄 docker-compose.yaml
📁 final_model/
    📄 final_inference_model.joblib
📁 logs/
    📁 2025_05_24T09_51_21Z/
        📄 2025_05_24T09_51_21Z.log
📄 main.py
📁 network_data/
    📁 input_csv/
        📄 phisingData.csv
        📄 phisingData_check.csv
📁 prediction_output/
📄 print_structure.py
📄 project_dump.py
📄 project_template.py
📄 requirements.txt
📁 research/
    📁 config/
        📄 schema.yaml
    📄 ingested_data.csv
    📄 research.ipynb
📄 setup.py
📁 src/
    📁 networksecurity/
        📄 __init__.py
        📁 components/
            📄 __init__.py
            📄 data_ingestion.py
            📄 data_transformation.py
            📄 data_validation.py
            📄 model_evaluation.py
            📄 model_pusher.py
            📄 model_trainer.py
        📁 config/
            📄 __init__.py
            📄 configuration.py
        📁 constants/
            📄 __init__.py
            📄 constants.py
        📁 data_processors/
            📄 encoder_factory.py
            📄 imputer_factory.py
            📄 label_mapper.py
            📄 preprocessor_builder.py
            📄 scaler_factory.py
        📁 dbhandler/
            📄 __init__.py
            📄 base_handler.py
            📄 mongodb_handler.py
            📄 s3_handler.py
        📁 entity/
            📄 __init__.py
            📄 artifact_entity.py
            📄 config_entity.py
        📁 exception/
            📄 __init__.py
            📄 exception.py
        📁 inference/
            📄 estimator.py
        📁 logging/
            📄 __init__.py
            📄 logger.py
        📁 pipeline/
            📄 __init__.py
            📄 data_ingestion_pipeline.py
            📄 data_transformation_pipeline.py
            📄 data_validation_pipeline.py
            📄 model_evaluation_pipeline.py
            📄 model_pusher_pipeline.py
            📄 model_trainer_pipeline.py
            📄 training_pipeline.py
        📁 utils/
            📄 __init__.py
            📄 core.py
            📄 timestamp.py
        📁 worker/
            📄 celery_worker.py
📁 templates/
    📄 table.html

--- CODE DUMP | PART 4 of 4 ---


================================================================================
# PY FILE: src\networksecurity\pipeline\model_pusher_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.model_pusher import ModelPusher
from src.networksecurity.entity.artifact_entity import ModelTrainerArtifact, ModelPusherArtifact
from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


class ModelPusherPipeline:
    """
    Orchestrates the model pushing stage.

    Responsibilities:
    - Loads model pusher configuration
    - Accepts the ModelTrainerArtifact
    - Saves final model to local path
    - Pushes model to S3 via S3Syncer
    - Emits a ModelPusherArtifact
    """

    def __init__(self, model_trainer_artifact: ModelTrainerArtifact) -> None:
        try:
            logger.info("Initializing ModelPusherPipeline...")

            self.config = ConfigurationManager().get_model_pusher_config()
            self.model_trainer_artifact = model_trainer_artifact

        except Exception as e:
            logger.exception("Failed to initialize ModelPusherPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> ModelPusherArtifact:
        try:
            logger.info("========== Model Pusher Stage Started ==========")

            model_pusher = ModelPusher(
                model_pusher_config=self.config,
                model_trainer_artifact=self.model_trainer_artifact
            )
            pusher_artifact = model_pusher.push_model()

            logger.info(f"Model Pusher Stage Completed Successfully: {pusher_artifact}")
            logger.info("========== Model Pusher Stage Completed ==========")

            return pusher_artifact

        except Exception as e:
            logger.exception("Model Pusher Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\model_trainer_pipeline.py
================================================================================

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.components.model_trainer import ModelTrainer
from src.networksecurity.entity.artifact_entity import (
    DataTransformationArtifact,
    ModelTrainerArtifact
)
from src.networksecurity.logging import logger
from src.networksecurity.exception.exception import NetworkSecurityError


class ModelTrainerPipeline:
    """
    Orchestrates the Model Training stage of the pipeline.

    Responsibilities:
    - Loads model‐trainer configuration
    - Accepts transformation artifact
    - Trains (and tunes) candidate models
    - Logs & registers via MLflow
    - Emits a ModelTrainerArtifact
    """

    def __init__(self, transformation_artifact: DataTransformationArtifact):
        try:
            self.config_manager = ConfigurationManager()
            self.config = self.config_manager.get_model_trainer_config()
            self.transformation_artifact = transformation_artifact
        except Exception as e:
            logger.exception("Failed to initialize ModelTrainerPipeline")
            raise NetworkSecurityError(e, logger) from e

    def run(self) -> ModelTrainerArtifact:
        try:
            logger.info("========== Model Training Stage Started ==========")

            trainer = ModelTrainer(
                config=self.config,
                transformation_artifact=self.transformation_artifact
            )
            trainer_artifact = trainer.run_training()

            logger.info(f"Model Training Completed Successfully: {trainer_artifact}")
            logger.info("========== Model Training Stage Completed ==========")

            return trainer_artifact

        except Exception as e:
            logger.exception("Model Training Stage Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\pipeline\training_pipeline.py
================================================================================

import sys

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger

from src.networksecurity.components.data_ingestion import DataIngestion
from src.networksecurity.components.data_validation import DataValidation
from src.networksecurity.components.data_transformation import DataTransformation
from src.networksecurity.components.model_trainer import ModelTrainer
from src.networksecurity.components.model_evaluation import ModelEvaluation
from src.networksecurity.components.model_pusher import ModelPusher

from src.networksecurity.config.configuration import ConfigurationManager
from src.networksecurity.dbhandler.mongodb_handler import MongoDBHandler
from src.networksecurity.dbhandler.s3_handler import S3Handler

from src.networksecurity.entity.artifact_entity import (
    DataIngestionArtifact,
    DataValidationArtifact,
    DataTransformationArtifact,
    ModelTrainerArtifact,
    ModelEvaluationArtifact,
    ModelPusherArtifact,
)


class TrainingPipeline:
    """
    Orchestrates the full end-to-end training pipeline:
    - MongoDB ingestion
    - Validation
    - Transformation
    - Training
    - Evaluation
    - Model push (local + optional S3)
    """

    def __init__(self) -> None:
        try:
            self.config_manager = ConfigurationManager()
        except Exception as e:
            raise NetworkSecurityError(e, logger) from e

    def run_pipeline(self) -> ModelPusherArtifact:
        try:
            logger.info("========== Training Pipeline Started ==========")

            # ───────────────
            # MongoDB Handler
            # ───────────────
            mongo_config = self.config_manager.get_mongo_handler_config()
            mongo_handler = MongoDBHandler(config=mongo_config)

            # ───────────────
            # Data Ingestion
            # ───────────────
            ingestion_config = self.config_manager.get_data_ingestion_config()
            ingestion = DataIngestion(config=ingestion_config, db_handler=mongo_handler)
            ingestion_artifact: DataIngestionArtifact = ingestion.run_ingestion()

            # ───────────────
            # Data Validation
            # ───────────────
            validation_config = self.config_manager.get_data_validation_config()
            validation = DataValidation(config=validation_config, ingestion_artifact=ingestion_artifact)
            validation_artifact: DataValidationArtifact = validation.run_validation()

            if not validation_artifact.validation_status:
                logger.error("Validation failed. Aborting pipeline.")
                raise RuntimeError("Data validation failed. Pipeline terminated.")

            # ──────────────────
            # Data Transformation
            # ──────────────────
            transformation_config = self.config_manager.get_data_transformation_config()
            transformation = DataTransformation(config=transformation_config, validation_artifact=validation_artifact)
            transformation_artifact: DataTransformationArtifact = transformation.run_transformation()

            # ───────────────
            # Model Training
            # ───────────────
            trainer_config = self.config_manager.get_model_trainer_config()
            trainer = ModelTrainer(config=trainer_config, transformation_artifact=transformation_artifact)
            trainer_artifact: ModelTrainerArtifact = trainer.run_training()

            # ────────────────
            # Model Evaluation
            # ────────────────
            evaluation_config = self.config_manager.get_model_evaluation_config()
            evaluator = ModelEvaluation(config=evaluation_config, trainer_artifact=trainer_artifact)
            evaluation_artifact: ModelEvaluationArtifact = evaluator.run_evaluation()

            # ───────────────
            # Model Pusher
            # ───────────────
            pusher_config = self.config_manager.get_model_pusher_config()
            s3_handler_config = self.config_manager.get_s3_handler_config()
            s3_handler = S3Handler(config=s3_handler_config)

            pusher = ModelPusher(
                model_pusher_config=pusher_config,
                s3_handler=s3_handler,
                model_trainer_artifact=trainer_artifact
            )
            pusher_artifact: ModelPusherArtifact = pusher.push_model()

            logger.info("========== Training Pipeline Completed Successfully ==========")
            return pusher_artifact

        except Exception as e:
            logger.exception("Training Pipeline Failed")
            raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\utils\__init__.py
================================================================================



================================================================================
# PY FILE: src\networksecurity\utils\core.py
================================================================================

import json
import joblib
import pandas as pd
import numpy as np
import yaml
from pathlib import Path
from box import ConfigBox
from box.exceptions import BoxKeyError, BoxTypeError, BoxValueError
from urllib.parse import quote_plus
from ensure import ensure_annotations

from src.networksecurity.exception.exception import NetworkSecurityError
from src.networksecurity.logging import logger


@ensure_annotations
def read_yaml(path_to_yaml: Path) -> ConfigBox:
    """
    Load a YAML file as a ConfigBox, always using UTF-8.
    """
    if not path_to_yaml.exists():
        msg = f"YAML file not found: '{path_to_yaml}'"
        logger.error(msg)
        raise NetworkSecurityError(FileNotFoundError(msg), logger)

    try:
        # Explicitly open as UTF-8 to avoid Windows cp1252 issues
        with path_to_yaml.open("r", encoding="utf-8") as f:
            content = yaml.safe_load(f)
    except (BoxValueError, BoxTypeError, BoxKeyError, yaml.YAMLError) as e:
        logger.error(f"Failed to load YAML from {path_to_yaml.as_posix()}: {e}")
        raise NetworkSecurityError(e, logger) from e
    except Exception as e:
        logger.error(f"Unexpected error reading YAML file: {e}")
        raise NetworkSecurityError(e, logger) from e

    if content is None:
        msg = "YAML file is empty or improperly formatted."
        logger.error(msg)
        raise NetworkSecurityError(ValueError(msg), logger)

    logger.info(f"YAML loaded successfully from: '{path_to_yaml.as_posix()}'")
    return ConfigBox(content)


@ensure_annotations
def csv_to_json_convertor(source_filepath: Path, destination_filepath: Path):
    """
    Convert a CSV file to a list of JSON records and optionally save it.
    """
    try:
        if not source_filepath.exists():
            raise FileNotFoundError(f"CSV source file not found: '{source_filepath.as_posix()}'")

        df = pd.read_csv(source_filepath).reset_index(drop=True)
        records = df.to_dict(orient="records")

        parent_dir = destination_filepath.parent
        if not parent_dir.exists():
            parent_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Created directory for JSON output: '{parent_dir.as_posix()}'")
        else:
            logger.info(f"Directory already exists for JSON output: '{parent_dir.as_posix()}'")

        with destination_filepath.open("w", encoding="utf-8") as f:
            json.dump(records, f, indent=4)

        logger.info(f"CSV converted and saved to: '{destination_filepath.as_posix()}'")
        return records

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def replace_username_password_in_uri(base_uri: str, username: str, password: str) -> str:
    if not all([base_uri, username, password]):
        raise ValueError("base_uri, username, and password must all be provided.")

    encoded_username = quote_plus(username)
    encoded_password = quote_plus(password)

    return (
        base_uri
        .replace("<username>", encoded_username)
        .replace("<password>", encoded_password)
    )


@ensure_annotations
def save_to_yaml(data: dict, *paths: Path, label: str):
    """
    Write a dict out to YAML, always using UTF-8.
    """
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            # Write UTF-8
            with open(path, "w", encoding="utf-8") as file:
                yaml.dump(data, file, sort_keys=False)

            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def save_to_csv(df: pd.DataFrame, *paths: Path, label: str):
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            df.to_csv(path, index=False)
            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def save_array(array: np.ndarray | pd.Series, *paths: Path, label: str):
    """
    Saves a NumPy array or pandas Series to the specified paths in `.npy` format.

    Args:
        array (Union[np.ndarray, pd.Series]): Data to save.
        *paths (Path): One or more file paths.
        label (str): Label for logging.
    """
    try:
        # Convert Series to ndarray if needed
        array = np.asarray(array)

        for path in paths:
            path = Path(path)

            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            # # Ensure file ends with `.npy`
            # if path.suffix != ".npy":
            #     path = path.with_suffix(".npy")

            np.save(path, array)
            logger.info(f"{label} saved to: '{path.as_posix()}'")

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e
    
@ensure_annotations
def load_array(path: Path, label: str) -> np.ndarray:
    """
    Loads a NumPy array from the specified `.npy` file path.

    Args:
        path (Path): Path to the `.npy` file.
        label (str): Label for logging.

    Returns:
        np.ndarray: Loaded NumPy array.
    """
    try:
        path = Path(path)

        if not path.exists():
            raise FileNotFoundError(f"{label} file not found at path: '{path.as_posix()}'")

        array = np.load(path)
        logger.info(f"{label} loaded successfully from: '{path.as_posix()}'")
        return array

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def save_to_json(data: dict, *paths: Path, label: str):
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            with open(path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=4)

            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def read_csv(path: Path, label: str) -> pd.DataFrame:
    try:
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"{label} not found at path: {path.as_posix()}")

        df = pd.read_csv(path)
        logger.info(f"{label} loaded from: '{path.as_posix()}'")
        return df

    except Exception as e:
        raise NetworkSecurityError(e, logger) from e


@ensure_annotations
def save_object(obj: object, path: Path, label: str):
    try:
        path = Path(path)
        if not path.parent.exists():
            path.parent.mkdir(parents=True, exist_ok=True)
            logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
        else:
            logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

        joblib.dump(obj, path)
        logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

@ensure_annotations
def load_object(path: Path):
    """
    Load a serialized Python object from the given path using joblib.

    Args:
        path (Path): Path to the saved object file.

    Returns:
        object: The deserialized Python object.

    Raises:
        NetworkSecurityError: If loading fails.
    """
    try:
        path = Path(path)
        logger.info(f"Loading object from: '{path.as_posix()}'")
        return joblib.load(path)
    except Exception as e:
        raise NetworkSecurityError(e, logger) from e

================================================================================
# PY FILE: src\networksecurity\utils\timestamp.py
================================================================================

from datetime import datetime, timezone

# Internal module-level cache to ensure the timestamp is consistent across the entire pipeline
_timestamp_cache: str = None

def get_shared_utc_timestamp(fmt: str = "%Y_%m_%dT%H_%M_%SZ") -> str:
    """
    Returns a consistent, cached UTC timestamp for the current pipeline run.

    This function guarantees that:
    - The timestamp is generated only once.
    - All modules (logger, config, etc.) use the exact same value.
    - The format is clean and safe for filenames, directories, and versioning.

    Args:
        fmt (str): Optional datetime format string.
                   Default: 'YYYY_MM_DDTHH_MM_SSZ' (e.g., '2025_04_16T17_45_02Z')

    Returns:
        str: UTC timestamp string formatted per the provided format.
    """
    global _timestamp_cache
    if _timestamp_cache is None:
        _timestamp_cache = datetime.now(timezone.utc).strftime(fmt)
    return _timestamp_cache

================================================================================
# PY FILE: src\networksecurity\worker\celery_worker.py
================================================================================

from celery import Celery
from src.networksecurity.pipeline.training_pipeline import TrainingPipeline

celery_app = Celery(
    "networksecurity_tasks",
    broker="redis://localhost:6379/0",
    backend="redis://localhost:6379/0",
)

@celery_app.task(name="trigger_training_task")
def trigger_training_task():
    pipeline = TrainingPipeline()
    pipeline.run_pipeline()

================================================================================
# YAML FILE: config\config.yaml
================================================================================

# Root directory for all experiment artifacts
project:
  artifacts_root: artifacts

# MongoDB configuration
mongo_handler:
  input_data_path: network_data/input_csv/phisingData.csv
  json_data_filename: input_data.json
  database_name: network_security_db
  collection_name: phishing_records

# Data ingestion configuration
data_ingestion:
  raw_data_filename: raw_data.csv
  ingested_data_filename: ingested_data.csv

# Data validation configuration
data_validation:
  validated_filename: validated_data.csv
  missing_report_filename: missing_values_report.json
  duplicates_report_filename: duplicates_report.yaml
  drift_report_filename: drift_report.yaml
  validation_report_filename: validation_report.yaml
  schema_hash_filename: schema_hash.json


# Data transformation configuration
data_transformation:
  x_train_filename: x_train.npy
  y_train_filename: y_train.npy
  x_val_filename: x_val.npy
  y_val_filename: y_val.npy
  x_test_filename: x_test.npy
  y_test_filename: y_test.npy
  x_preprocessor_filename: x_preprocessor.joblib
  y_preprocessor_filename: y_preprocessor.joblib
  

# Model trainer configuration
model_trainer:
  model_dir: saved_models
  trained_model_filename: model.joblib
  # where to dump a short training report
  training_report_filename: training_report.yaml

# Model prediction configuration
model_prediction:
  prediction_output_filename: prediction_output.csv

# Stable DVC-tracked data paths
data_paths:
  raw_data_dvc_filepath: data/raw/raw_data.csv
  validated_dvc_filepath: data/validated/validated_data.csv
  train_dvc_dir: data/transformed/train
  val_dvc_dir: data/transformed/val
  test_dvc_dir: data/transformed/test

# Model evaluation configuration
model_evaluation:
  evaluation_report_filename: evaluation_report.yaml
  evaluated_model_filename: evaluated_model.joblib

model_pusher:
  final_model_filename: final_inference_model.joblib

s3_handler:
  final_model_s3_bucket: networksecurity-dev-artifacts
  s3_final_model_prefix: final_model
  s3_artifacts_prefix: artifacts

================================================================================
# YAML FILE: config\params.yaml
================================================================================

# Parameters for drift detection during data validation
validation_params:
  drift_detection:
    enabled: true
    method: ks_test
    p_value_threshold: 0.05

  schema_check:
    enabled: true
    method: hash


# Parameters for data transformation
transformation_params:
  # Parameters for data splitting
  data_split:
    train_size: 0.6
    test_size: 0.2
    val_size: 0.2
    random_state: 42
    stratify: true

  steps:
    x:
      imputer: knn
    y:
      label_mapping: label_mapper

  methods:
    x:
      imputer:
        missing_values: null
        n_neighbors: 3
        weights: uniform
    y:
      label_mapping:
        from: -1
        to: 0

# ───────────────────────────────────────────────────────────────────────────────
# Parameters for model training
# ───────────────────────────────────────────────────────────────────────────────
# Parameters for model training
model_trainer:
  # candidate estimators (importable by sklearn convention)
  models:
    - name: sklearn.ensemble.RandomForestClassifier
      # default params (used if optimization.enabled = false)
      params:
        n_estimators: 100
        max_depth: 10
        random_state: 42
      # search-space for Optuna
      search_space:
        n_estimators:
          distribution: int
          low: 50
          high: 300
          step: 10
        max_depth:
          distribution: int
          low: 5
          high: 50
          step: 1

    - name: sklearn.ensemble.GradientBoostingClassifier
      params:
        n_estimators: 100
        learning_rate: 0.1
        max_depth: 3
        random_state: 42
      search_space:
        n_estimators:
          distribution: int
          low: 50
          high: 200
          step: 10
        learning_rate:
          distribution: float
          low: 0.01
          high: 1.0
          log: true
        max_depth:
          distribution: int
          low: 2
          high: 10

  # hyperparameter optimization settings
  optimization:
    enabled: true
    method: optuna
    n_trials: 30
    direction: maximize
    cv_folds: 5
    scoring: accuracy

  # MLflow tracking & registry settings (URI is now picked from ENV)
  tracking:
    mlflow:
      enabled: true
      experiment_name: NetworkSecurityExperiment
      registry_model_name: NetworkSecurityModel
      metrics_to_log:
        - accuracy
        - f1
        - precision
        - recall
      # new switch: whether to log each Optuna trial as its own MLflow run
      log_trials: false

model_pusher:
   upload_to_s3: true

================================================================================
# YAML FILE: config\schema.yaml
================================================================================

columns:
  Abnormal_URL: int64
  DNSRecord: int64
  Domain_registeration_length: int64
  Favicon: int64
  Google_Index: int64
  HTTPS_token: int64
  Iframe: int64
  Links_in_tags: int64
  Links_pointing_to_page: int64
  Page_Rank: int64
  Prefix_Suffix: int64
  Redirect: int64
  Request_URL: int64
  Result: int64
  RightClick: int64
  SFH: int64
  SSLfinal_State: int64
  Shortining_Service: int64
  Statistical_report: int64
  Submitting_to_email: int64
  URL_Length: int64
  URL_of_Anchor: int64
  age_of_domain: int64
  double_slash_redirecting: int64
  having_At_Symbol: int64
  having_IP_Address: int64
  having_Sub_Domain: int64
  on_mouseover: int64
  popUpWidnow: int64
  port: int64
  web_traffic: int64

target_column: Result

target_labels:
  0: phishing
  1: legitimate

================================================================================
# YAML FILE: config\templates.yaml
================================================================================

validation_report:
  # UTC timestamp of the validation run
  timestamp: ""
  
  # Overall validation pass/fail
  validation_status: null
  critical_passed: null
  non_critical_passed: null

  # Which schema‐check was used (“hash” or “structure”)
  schema_check_type: ""
  # Which drift‐detection method (if any)
  drift_check_method: ""

  check_results:
    critical_checks:
      # True if the schema hash/structure matched
      schema_is_match: null
      # True if no data drift was detected
      no_data_drift: null

    non_critical_checks:
      # True if no missing values were found
      no_missing_values: null
      # True if no duplicate rows were found
      no_duplicate_rows: null


# templates.yaml

# … your existing templates …

training_report:
  timestamp: ""
  best_model: ""
  best_model_params: {}
  train_metrics: {}
  val_metrics: {}
  optimization:
    method: ""
    best_trial: null
    n_trials: null
    direction: ""
    cv:
      folds: null
      mean_score: null
      std_score: null

================================================================================
# YAML FILE: research\config\schema.yaml
================================================================================

columns:
  Abnormal_URL: int64
  DNSRecord: int64
  Domain_registeration_length: int64
  Favicon: int64
  Google_Index: int64
  HTTPS_token: int64
  Iframe: int64
  Links_in_tags: int64
  Links_pointing_to_page: int64
  Page_Rank: int64
  Prefix_Suffix: int64
  Redirect: int64
  Request_URL: int64
  Result: int64
  RightClick: int64
  SFH: int64
  SSLfinal_State: int64
  Shortining_Service: int64
  Statistical_report: int64
  Submitting_to_email: int64
  URL_Length: int64
  URL_of_Anchor: int64
  age_of_domain: int64
  double_slash_redirecting: int64
  having_At_Symbol: int64
  having_IP_Address: int64
  having_Sub_Domain: int64
  on_mouseover: int64
  popUpWidnow: int64
  port: int64
  web_traffic: int64
